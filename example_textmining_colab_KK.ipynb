{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61316,"status":"ok","timestamp":1697294156348,"user":{"displayName":"Kyungwon Kim","userId":"12494571024836189016"},"user_tz":-540},"id":"a1RK-GkfNjpT","outputId":"e3e1b077-2e5f-46d3-e9fd-c7863c64683e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'nsmc'...\n","remote: Enumerating objects: 14763, done.\u001b[K\n","remote: Counting objects: 100% (14762/14762), done.\u001b[K\n","remote: Compressing objects: 100% (13012/13012), done.\u001b[K\n","remote: Total 14763 (delta 1748), reused 14762 (delta 1748), pack-reused 1\u001b[K\n","Receiving objects: 100% (14763/14763), 56.19 MiB | 17.50 MiB/s, done.\n","Resolving deltas: 100% (1748/1748), done.\n","Updating files: 100% (14737/14737), done.\n","Collecting tensorflow_addons\n","  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n","Collecting typeguard\u003c3.0.0,\u003e=2.7 (from tensorflow_addons)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Installing collected packages: typeguard, tensorflow_addons\n","Successfully installed tensorflow_addons-0.21.0 typeguard-2.13.3\n","Collecting transformers\n","  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers\u003c0.15,\u003e=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors\u003e=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.16.4-\u003etransformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.16.4-\u003etransformers) (4.5.0)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.3.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.6)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"]}],"source":["!git clone https://github.com/e9t/nsmc.git\n","!pip install tensorflow_addons\n","!pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":22824,"status":"ok","timestamp":1697294995832,"user":{"displayName":"Kyungwon Kim","userId":"12494571024836189016"},"user_tz":-540},"id":"VYJ3D4yiNFrg"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import tensorflow as tf\n","from transformers import pipeline, AutoTokenizer, BertTokenizer, BertTokenizerFast\n","from transformers import AutoModel, TFBertModel, TFBertForSequenceClassification"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33375,"status":"ok","timestamp":1697295029205,"user":{"displayName":"Kyungwon Kim","userId":"12494571024836189016"},"user_tz":-540},"id":"IY4bBA6dNvuw","outputId":"f3200fb7-3fd8-4423-dc3c-ee7cb95ab257"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","TPU = True\n","if TPU:\n","  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","  tf.config.experimental_connect_to_cluster(resolver)\n","  tf.tpu.experimental.initialize_tpu_system(resolver)\n","else:\n","  pass"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"20421f35"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n","  0%|          | 0/5000 [00:00\u003c?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 5000/5000 [00:03\u003c00:00, 1615.69it/s]\n","100%|██████████| 5000/5000 [00:02\u003c00:00, 1761.60it/s]\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","50/50 [==============================] - 323s 6s/step - loss: 0.7157 - accuracy: 0.5114 - val_loss: 0.6977 - val_accuracy: 0.4874\n","Epoch 2/10\n","50/50 [==============================] - 293s 6s/step - loss: 0.7056 - accuracy: 0.5054 - val_loss: 0.7149 - val_accuracy: 0.5126\n","Epoch 3/10\n","50/50 [==============================] - 294s 6s/step - loss: 0.6976 - accuracy: 0.5128 - val_loss: 0.6957 - val_accuracy: 0.5126\n","Epoch 4/10\n","50/50 [==============================] - 294s 6s/step - loss: 0.6981 - accuracy: 0.5008 - val_loss: 0.6933 - val_accuracy: 0.4874\n","Epoch 5/10\n","50/50 [==============================] - 294s 6s/step - loss: 0.7017 - accuracy: 0.4920 - val_loss: 0.6941 - val_accuracy: 0.5126\n","Epoch 6/10\n","50/50 [==============================] - 295s 6s/step - loss: 0.7003 - accuracy: 0.4952 - val_loss: 0.6928 - val_accuracy: 0.5126\n","Epoch 7/10\n","50/50 [==============================] - 294s 6s/step - loss: 0.6954 - accuracy: 0.5056 - val_loss: 0.6954 - val_accuracy: 0.5126\n","Epoch 8/10\n","50/50 [==============================] - 295s 6s/step - loss: 0.6975 - accuracy: 0.5040 - val_loss: 0.6929 - val_accuracy: 0.5126\n","Epoch 9/10\n","50/50 [==============================] - 294s 6s/step - loss: 0.6971 - accuracy: 0.5116 - val_loss: 0.7010 - val_accuracy: 0.4874\n","Epoch 10/10\n","50/50 [==============================] - 295s 6s/step - loss: 0.6998 - accuracy: 0.4884 - val_loss: 0.6994 - val_accuracy: 0.5126\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7b5ef435f190\u003e"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# !git clone https://github.com/e9t/nsmc.git\n","train = pd.read_table(\"nsmc/\"+\"ratings_train.txt\")\n","train = train.dropna().sample(5000).reset_index().iloc[:,1:]\n","test = pd.read_table(\"nsmc/\"+\"ratings_test.txt\")\n","test = test.dropna().sample(5000).reset_index().iloc[:,1:]\n","\n","def preprocessing_sentence_to_BERTinput(df, tokenizer, colname_data, colname_target=None, seq_len=128,\n","                                        return_type='tensor'):\n","    tokens, masks, segments, targets = [], [], [], []\n","    for i in tqdm(range(len(df))):\n","        # 변환\n","        token = tokenizer.encode_plus(df[colname_data][i], max_length=seq_len,\n","                                      pad_to_max_length=True, truncation=True,\n","                                      return_attention_mask=True,\n","                                      add_special_tokens=True)\n","\n","        # 정리\n","        tokens.append(token['input_ids'])\n","        masks.append(token['attention_mask'])\n","        segments.append(token['token_type_ids'])\n","        if colname_target != None:\n","            targets.append(df[colname_target][i])\n","\n","    # array 변환\n","    tokens = np.array(tokens)\n","    masks = np.array(masks)\n","    segments = np.array(segments)\n","    if colname_target != None:\n","        targets = np.array(targets)\n","\n","    # tensor 변환\n","    if return_type == 'tensor':\n","        tokens = tf.convert_to_tensor(tokens, dtype=tf.int32)\n","        masks = tf.convert_to_tensor(masks, dtype=tf.int32)\n","        segments = tf.convert_to_tensor(segments, dtype=tf.int32)\n","\n","    return [tokens, masks, segments], targets\n","\n","import tensorflow_addons as tfa\n","from transformers import pipeline, AutoTokenizer, BertTokenizer, BertTokenizerFast\n","from transformers import AutoModel, AutoModelForTokenClassification, TFBertModel, TFBertForSequenceClassification\n","\n","MODEL_NAME = 'monologg/kobert'    # 'bert-base-multilingual-cased'\n","# OPTIMIZER = tfa.optimizers.RectifiedAdam(lr=1.0e-5, weight_decay=0.0025, warmup_proportion=0.05)\n","OPTIMIZER = tf.keras.optimizers.Adam(lr=1.0e-5)\n","NUM_LABELS = 2\n","SEQ_LEN = 64\n","\n","tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n","X_train, Y_train = preprocessing_sentence_to_BERTinput(train, tokenizer=tokenizer,\n","                                                       colname_target='label', colname_data='document', seq_len=SEQ_LEN)\n","X_test, Y_test = preprocessing_sentence_to_BERTinput(test, tokenizer=tokenizer,\n","                                                       colname_target='label', colname_data='document', seq_len=SEQ_LEN)\n","\n","def modeling_BERTsentiment(model_name, optimizer, num_labels=2, seq_len=128):\n","    # 모델 로딩\n","    model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","    model.compile(optimizer=optimizer, loss=loss, metrics=metric)\n","\n","    return model\n","\n","# def modeling_BERTsentiment(model_name, optimizer, num_labels=2, seq_len=128):\n","#     # 모델 로딩\n","#     model = TFBertModel.from_pretrained(model_name, num_labels=num_labels, output_hidden_states=True)\n","#     outputs = model([tokens, masks, segments])[1]\n","\n","#     # 모델 구성\n","#     layer = tf.keras.layers.Dense(1, activation='sigmoid',\n","#                                   kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(outputs)\n","#     model_sentiment = tf.keras.Model([tokens, masks, segments], layer)\n","#     model_sentiment.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])\n","\n","#     return model_sentiment\n","\n","\n","model = modeling_BERTsentiment(model_name=MODEL_NAME, optimizer=OPTIMIZER, num_labels=NUM_LABELS, seq_len=SEQ_LEN)\n","model.fit(X_train, Y_train, epochs=10, shuffle=True, batch_size=100, validation_data=(X_test, Y_test))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCMFpXiYO5zC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJFTUj4iO6xQ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":5}