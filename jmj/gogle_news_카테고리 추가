import requests
from bs4 import BeautifulSoup
import pandas as pd
from tqdm import tqdm
import os


def get_category_from_link(link, headers):
    """
    Visit the given link and extract category information from the detailed news page.
    """
    response = requests.get(link, headers=headers)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.content, "lxml")
    
    categories_found = []
    for meta_property in ["article:section", "article:section1", "Classification"]:
        meta_element = soup.find("meta", {"property": meta_property}) or soup.find("meta", {"name": meta_property})
        if meta_element and meta_element.get("content"):
            categories_found.append(meta_element["content"])

   
    if not categories_found:
        categories_found = ['없음']

    return ", ".join(categories_found)

def combined_google_news_data(query, save_path, num_pages=30):
    from datetime import datetime, timedelta
    
    google_news = []
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36'
    }

    
    now_date = datetime.now()
    start_date = datetime(2018, 1, 1)
    cd_min = start_date.strftime('%m/%d/%Y')
    cd_max = now_date.strftime('%m/%d/%Y')
    
    for page in range(num_pages):
        
        start_param = page * 10
        
        
        url = f"https://www.google.com/search?q={query}&tbm=nws&tbs=cdr:1,cd_min:{cd_min},cd_max:{cd_max}&start={start_param}"
        
        response = requests.get(url, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        for el in soup.select('div.SoaBEf'):
            link = el.find('a')['href']
            category_info = extract_category(link, headers)
            
            news_dict = {
                'links': link,
                'title': el.select_one('div.MBeuO').get_text(),
                'contents': el.select_one('.GI74Re').get_text(),
                'date': el.select_one('.LfVVr').get_text(),
                'press': el.select_one('.NUnG9d span').get_text(),
                'category': category_info
            }
            
            
            for i in range(24):
                if f'{i}시간 전' in news_dict['date']:
                    news_dict['date'] = now_date.strftime('%Y. %m. %d.')                                      
                elif f'{i}일 전' in news_dict['date']:
                    news_dict['date'] = (now_date - timedelta(days=i)).strftime('%Y. %m. %d.')                                          
                elif f'{i}주 전' in news_dict['date']:
                    news_dict['date'] = (now_date - timedelta(weeks=i)).strftime('%Y. %m. %d.')                                          
                elif '1개월 전' in news_dict['date']:
                    news_dict['date'] = (now_date - timedelta(weeks=4)).strftime('%Y. %m. %d.')
            
            google_news.append(news_dict)

    df = pd.DataFrame(google_news)
    save_file_path = os.path.join(save_path, "crawling_google_search_combined_v4.csv")
    df.to_csv(save_file_path, index=False, encoding='utf-8-sig')
    print(f"Crawling is complete. The data is saved in {save_file_path}")

    return df


# 검색어 입력받기
query = input("검색어")

# 파일 저장 경로 지정 
save_path = "C:\\장민재"

# 크롤링 및 DataFrame 출력
df = combined_google_news_data(query, save_path, num_pages=30)  
# print(df.head())
print(df.head())
