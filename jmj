import requests
from bs4 import BeautifulSoup
import pandas as pd
from tqdm import tqdm
import os

# Google News에서 뉴스 데이터 가져오기
def getGoogleNewsData(query, save_path):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36"
    }
    response = requests.get(f"https://www.google.com/search?q={query}&num=100&gl=us&tbm=nws", headers=headers)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.content, "lxml")
    
    news_results = []
    
    for el in tqdm(soup.select("div.SoaBEf"), desc="Crawling Google News", ncols=100):
        news_dict = {
            "link": el.find("a")["href"],
            "title": el.select_one("div.MBeuO").get_text(),
            "snippet": el.select_one(".GI74Re").get_text(),
            "date": el.select_one(".LfVVr").get_text(),
            "source": el.select_one(".NUnG9d span").get_text()
        }
        news_results.append(news_dict)
        
    df = pd.DataFrame(news_results)
    df.to_csv(os.path.join(save_path, "google_news_results.csv"), index=False, encoding='utf-8-sig')
    print(f"Crawling is complete. The data is saved in {os.path.join(save_path, 'google_news_results.csv')}")
    return df

# 검색어 입력받기
query = input("Enter the search term: ")

# 파일 저장 경로 지정 (예: 'C:\\장민재')
save_path = "C:\\장민재"

# 크롤링 및 DataFrame 출력
df = getGoogleNewsData(query, save_path)
print(df.head())
