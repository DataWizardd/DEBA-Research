{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d256f1d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:07:24.341184Z",
     "start_time": "2023-11-08T05:07:24.014003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:  3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]\n",
      "\n",
      "Tensorflow Version:  2.10.1\n",
      "Num of Physical GPUs Available:  0\n",
      "Cuda is ready?  True \n",
      "\n",
      "\n",
      "Torch Version:  2.1.0+cpu\n",
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "# warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# System related and data input controls\n",
    "import os\n",
    "\n",
    "# Auto reload of library\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python path\n",
    "import sys\n",
    "base_folder = 'DataScience'\n",
    "location_base = os.path.join(os.getcwd().split(base_folder)[0], base_folder)\n",
    "location_module = [os.path.join(location_base, 'Module')] \n",
    "for each in location_module:\n",
    "    if each not in sys.path:\n",
    "        sys.path.append(each)\n",
    "        \n",
    "from import_KK import *\n",
    "DeviceStrategy_GPU()\n",
    "from preprocessing_KK import *\n",
    "from preprocessing_text_KK import * ##\n",
    "from visualization_KK import * ##\n",
    "from algorithm_textmining_KK import *\n",
    "from algorithm_machinelearning_KK import *\n",
    "from algorithm_deeplearning_KK import *\n",
    "from evaluation_KK import *\n",
    "\n",
    "# 하이퍼파라미터\n",
    "DELETE_KEYWORD = ['100세', '거주환경']\n",
    "CATEGORY_BK = ['경제', '사회', '문화', '국제']\n",
    "CATEGORY_BK_Sub = ['경제>경제일반', '경제>국제경제', '경제>취업_창업',\n",
    "                   '사회>노동_복지', '사회>사건_사고', '사회>사회일반', '사회>여성', '사회>장애인', '사회>의료_건강',\n",
    "                   '문화>미술_건축', '문화>요리_여행', '문화>출판',\n",
    "                   '국제>중국', '국제>유럽_EU', '국제>일본', '국제>미국_북미', '국제>중동_아프리카',\n",
    "                   '국제>아시아', '국제>중남미', '국제>국제일반', '국제>러시아']\n",
    "CATEGORY_CR = ['세계', '경제', '생활/문화', '오피니언', '사회', 'IT/과학']\n",
    "COLNAME_CATEGORY = '일자'\n",
    "COLNAME_MINING = '제목'\n",
    "# SAVE_LOCATION = r'C:\\Users\\user\\Desktop\\Data'    # inu\n",
    "SAVE_LOCATION = r'C:\\Users\\KK\\Desktop\\Data'    # home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af50673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:36:05.093610Z",
     "start_time": "2023-11-08T05:36:04.805291Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_sentence_to_BERTinput(X_series, Y_series, tokenizer, \n",
    "                                        seq_len=128, batch_size=32, sampler='random'):\n",
    "    # BERT 입력 형식에 맞게 변환\n",
    "    sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in X_series]\n",
    "    \n",
    "    # 전처리\n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    for i in tqdm(range(len(X_series))):\n",
    "        ## 변환\n",
    "        token = tokenizer.encode_plus(X_series[i], max_length=seq_len,\n",
    "                                      pad_to_max_length=True, truncation=True,\n",
    "                                      return_attention_mask=True,\n",
    "                                      add_special_tokens=True)\n",
    "        ## 정리\n",
    "        tokens.append(token['input_ids'])\n",
    "        masks.append(token['attention_mask'])\n",
    "        segments.append(token['token_type_ids'])\n",
    "        targets.append(Y_series[i])\n",
    "    \n",
    "    # array 변환\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # tensor 변환\n",
    "    tokens = torch.tensor(tokens)\n",
    "    masks = torch.tensor(masks)\n",
    "    segments = torch.tensor(segments)\n",
    "    targets = torch.tensor(targets)\n",
    "    \n",
    "    # pytorch dataloader 연결\n",
    "    data = TensorDataset(tokens, masks, targets)\n",
    "    if sampler == 'random':\n",
    "        dataloader = DataLoader(data, sampler=RandomSampler(data), batch_size=batch_size)\n",
    "    elif sampler == 'sequential':\n",
    "        dataloader = DataLoader(data, sampler=SequentialSampler(data), batch_size=batch_size)\n",
    "        \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40baf6cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:36:11.634427Z",
     "start_time": "2023-11-08T05:36:11.346703Z"
    }
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e72cc0",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49534367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:58:40.519962Z",
     "start_time": "2023-11-08T05:58:39.998895Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(os.getcwd(), 'Data', 'MovieReview_NSMC', 'ratings_train.txt'), sep='\\t')\n",
    "test = pd.read_csv(os.path.join(os.getcwd(), 'Data', 'MovieReview_NSMC', 'ratings_test.txt'), sep='\\t')\n",
    "train, test = train[~train.document.isnull()].reset_index().iloc[:,1:] , test[~test.document.isnull()].reset_index().iloc[:,1:]\n",
    "X_train, Y_train = train['document'], train['label']\n",
    "X_test, Y_test = test['document'], test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "145b0017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:59:08.755112Z",
     "start_time": "2023-11-08T05:58:41.508041Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 149995/149995 [00:18<00:00, 8150.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 49997/49997 [00:06<00:00, 8047.74it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)    # 'bert-base-multilingual-cased', 'klue/roberta-base'\n",
    "train_dataloader = preprocessing_sentence_to_BERTinput(X_train, Y_train, tokenizer,\n",
    "                                                       seq_len=SEQ_LEN, batch_size=BATCH_SIZE)\n",
    "test_dataloader = preprocessing_sentence_to_BERTinput(X_test, Y_test, tokenizer,\n",
    "                                                       seq_len=SEQ_LEN, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2804f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68a7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2eb691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_kk",
   "language": "python",
   "name": "gpu_kk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
