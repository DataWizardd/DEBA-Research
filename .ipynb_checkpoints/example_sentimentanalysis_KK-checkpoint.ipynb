{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d256f1d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T20:27:30.927179Z",
     "start_time": "2023-11-09T20:27:30.628240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensorflow Version:  2.10.0\n",
      "Keras Version:  2.10.0\n",
      "Num of Physical GPUs Available:  1\n",
      "Cuda is ready?  True\n",
      "Cuda Version:  64_112\n",
      "Cudnn Version:  64_8 \n",
      "\n",
      "Torch Version:  2.1.0\n",
      "Torch Cuda Version: 11.8\n",
      "Torch Cudnn Version:8700\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "# warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# System related and data input controls\n",
    "import os\n",
    "\n",
    "# Auto reload of library\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python path\n",
    "import sys\n",
    "base_folder = 'DataScience'\n",
    "location_base = os.path.join(os.getcwd().split(base_folder)[0], base_folder)\n",
    "location_module = [os.path.join(location_base, 'Module')] \n",
    "for each in location_module:\n",
    "    if each not in sys.path:\n",
    "        sys.path.append(each)\n",
    "        \n",
    "from import_KK import *\n",
    "DeviceStrategy_GPU()\n",
    "from preprocessing_KK import *\n",
    "from preprocessing_text_KK import * ##\n",
    "from visualization_KK import * ##\n",
    "from algorithm_textmining_KK import *\n",
    "from algorithm_machinelearning_KK import *\n",
    "from algorithm_deeplearning_KK import *\n",
    "from evaluation_KK import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40baf6cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T20:27:39.580731Z",
     "start_time": "2023-11-09T20:27:39.301310Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed\n",
    "seed_val = 123\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 2.0e-5\n",
    "MODEL_NAME = 'bert-base-multilingual-cased'    # 'bert-base-multilingual-cased', 'klue/roberta-base', 'monologg/kobert'\n",
    "MODELSAVE_LOCATION = os.path.join(os.getcwd(), 'Model', \n",
    "                                  'modeling_BERTsentiment_'+datetime.datetime.today().strftime(\"%Y%m%d\")+'.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e72cc0",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_sentence_to_BERTinput(X_series, Y_series, tokenizer, \n",
    "                                        seq_len=128, batch_size=32, sampler='random'):\n",
    "    # BERT 입력 형식에 맞게 변환\n",
    "    sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in X_series]\n",
    "    \n",
    "    # 전처리\n",
    "    token_list = [tokenizer.encode_plus(sentence, max_length=seq_len,\n",
    "                                        pad_to_max_length=True, truncation=True,\n",
    "                                        return_attention_mask=True,\n",
    "                                        add_special_tokens=True) for sentence in sentences]\n",
    "    tokens = [token['input_ids'] for token in token_list]\n",
    "    masks = [token['attention_mask'] for token in token_list]\n",
    "    segments = [token['token_type_ids'] for token in token_list]\n",
    "    targets = Y_series.values\n",
    "    \n",
    "    # array 변환\n",
    "    tokens = np.array(tokens)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # tensor 변환\n",
    "    tokens = torch.tensor(tokens)\n",
    "    masks = torch.tensor(masks)\n",
    "    segments = torch.tensor(segments)\n",
    "    targets = torch.tensor(targets)\n",
    "    \n",
    "    # pytorch dataloader 연결\n",
    "    data = TensorDataset(tokens, masks, targets)\n",
    "    if sampler == 'random':\n",
    "        dataloader = DataLoader(data, sampler=RandomSampler(data), batch_size=batch_size)\n",
    "    elif sampler == 'sequential':\n",
    "        dataloader = DataLoader(data, sampler=SequentialSampler(data), batch_size=batch_size)\n",
    "        \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49534367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T20:28:09.283017Z",
     "start_time": "2023-11-09T20:27:41.162730Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(os.getcwd(), 'Data', 'MovieReview_NSMC', 'ratings_train.txt'), sep='\\t')\n",
    "test = pd.read_csv(os.path.join(os.getcwd(), 'Data', 'MovieReview_NSMC', 'ratings_test.txt'), sep='\\t')\n",
    "\n",
    "# 데이터 분리\n",
    "X_train, Y_train = train['document'], train['label']\n",
    "X_test, Y_test = test['document'], test['label']\n",
    "\n",
    "# 전처리\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)  \n",
    "train_dataloader = preprocessing_sentence_to_BERTinput(X_train, Y_train, tokenizer,\n",
    "                                                       seq_len=SEQ_LEN, batch_size=BATCH_SIZE)\n",
    "test_dataloader = preprocessing_sentence_to_BERTinput(X_test, Y_test, tokenizer,\n",
    "                                                       seq_len=SEQ_LEN, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6b45b",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f9c7d20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T20:28:56.031146Z",
     "start_time": "2023-11-09T20:28:09.284030Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 =======\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                          | 247/4688 [00:45<13:29,  5.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 125\u001b[0m\n\u001b[0;32m    121\u001b[0m                 torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_location)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m--> 125\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodeling_BERTsentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msave_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODELSAVE_LOCATION\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 60\u001b[0m, in \u001b[0;36mmodeling_BERTsentiment\u001b[1;34m(device, model_name, train_dataloader, validation_dataloader, num_labels, epochs, learning_rate, early_stopping_patience, save_location)\u001b[0m\n\u001b[0;32m     58\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     59\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)    \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# Update model's parameters using the gradients\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()    \u001b[38;5;66;03m# Initialize gradients\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Calculate the average training loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39_dnn895\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39_dnn895\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39_dnn895\\lib\\site-packages\\transformers\\optimization.py:466\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    462\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[0;32m    467\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    468\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def modeling_BERTsentiment(device, model_name, train_dataloader, validation_dataloader,\n",
    "                           num_labels=2, epochs=10, learning_rate=1.0e-5, early_stopping_patience=10,\n",
    "                           save_location=None):\n",
    "    # 하위 함수\n",
    "    ## 정확도 계산 함수\n",
    "    def flat_accuracy(preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=0).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "    ## 시간 표시 함수\n",
    "    def format_time(elapsed):\n",
    "        # 반올림\n",
    "        elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "        # hh:mm:ss으로 형태 변경\n",
    "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "    \n",
    "    # 모델 로딩\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    model.cuda()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, # 학습률\n",
    "                      eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "    \n",
    "    # 학습\n",
    "    training_losses, validation_losses = [], []\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    best_validation_loss = float('inf')\n",
    "    best_accuracy = 0.0\n",
    "    patience = early_stopping_patience\n",
    "    model.zero_grad()\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print('\\n======== Epoch {:} / {:} ======='.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Set start time\n",
    "        t0 = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Switch to training mode\n",
    "        model.train()\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            ## Put the batch on the GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            ## Extract data from the batch\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            ## Perform Forward\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "            ## Get the loss\n",
    "            loss = outputs[0]\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute gradients by performing a backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    # Gradient clipping\n",
    "            optimizer.step()   # Update model's parameters using the gradients\n",
    "            model.zero_grad()    # Initialize gradients\n",
    "\n",
    "        # Calculate the average training loss\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        print(\"Average Loss: {0:.2f}\".format(avg_train_loss), \" Epoch Took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "        # 검증\n",
    "        print(\"Validation...\")\n",
    "        t0 = time.time()    # Set start time\n",
    "        model.eval()    # Switch to evaluation mode\n",
    "        for batch in validation_dataloader:\n",
    "            ## Put the batch on the GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            ## Extract data from the batch\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # Do not calculate gradients during validation\n",
    "            with torch.no_grad():\n",
    "                # Perform Forward\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Ensure that loss is a scalar (0-dimensional tensor) and accumulate the loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            # Move data to the CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate accuracy by comparing output logits and labels\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        # Calculate the average validation loss\n",
    "        avg_validation_loss = eval_loss / nb_eval_steps\n",
    "        validation_losses.append(avg_validation_loss)\n",
    "\n",
    "        # Calculate average validation accuracy\n",
    "        avg_validation_accuracy = eval_accuracy / nb_eval_steps\n",
    "        print(\"Validation Loss: {0:.2f}\".format(avg_validation_loss), \" Validation Accuracy: {0:.2f}\".format(avg_validation_accuracy))\n",
    "        \n",
    "        # 조기종료\n",
    "        # Check for early stopping based on validation loss\n",
    "        if avg_validation_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_validation_loss\n",
    "            patience = early_stopping_patience  # Reset patience\n",
    "        else:\n",
    "            patience -= 1  # Decrease patience\n",
    "\n",
    "        # Early stop when patience becomes 0\n",
    "        if patience == 0:\n",
    "            print(\"Early stop: validation loss does not improve for {} epochs\".format(early_stopping_patience))\n",
    "            break\n",
    "\n",
    "        # Check if the current accuracy is better than the best accuracy\n",
    "        if avg_validation_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_validation_accuracy\n",
    "            if save_location != None:\n",
    "                torch.save(model.state_dict(), save_location)\n",
    "            \n",
    "    return model\n",
    "        \n",
    "model = modeling_BERTsentiment(DEVICE, MODEL_NAME, train_dataloader, test_dataloader, epochs=EPOCHS, learning_rate=LEARNING_RATE,\n",
    "                               save_location=MODELSAVE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cecd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728698f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b693e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008bedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe7268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695483b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c10d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b2c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905fa157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc23cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b38cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2eb691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_dnn895",
   "language": "python",
   "name": "py39_dnn895"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
