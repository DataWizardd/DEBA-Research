{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d256f1d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:07:24.341184Z",
     "start_time": "2023-11-08T05:07:24.014003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:  3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]\n",
      "\n",
      "Tensorflow Version:  2.10.1\n",
      "Num of Physical GPUs Available:  0\n",
      "Cuda is ready?  True \n",
      "\n",
      "\n",
      "Torch Version:  2.1.0+cpu\n",
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "# warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# System related and data input controls\n",
    "import os\n",
    "\n",
    "# Auto reload of library\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python path\n",
    "import sys\n",
    "base_folder = 'DataScience'\n",
    "location_base = os.path.join(os.getcwd().split(base_folder)[0], base_folder)\n",
    "location_module = [os.path.join(location_base, 'Module')] \n",
    "for each in location_module:\n",
    "    if each not in sys.path:\n",
    "        sys.path.append(each)\n",
    "        \n",
    "from import_KK import *\n",
    "DeviceStrategy_GPU()\n",
    "from preprocessing_KK import *\n",
    "from preprocessing_text_KK import * ##\n",
    "from visualization_KK import * ##\n",
    "from algorithm_textmining_KK import *\n",
    "from algorithm_machinelearning_KK import *\n",
    "from algorithm_deeplearning_KK import *\n",
    "from evaluation_KK import *\n",
    "\n",
    "# 하이퍼파라미터\n",
    "DELETE_KEYWORD = ['100세', '거주환경']\n",
    "CATEGORY_BK = ['경제', '사회', '문화', '국제']\n",
    "CATEGORY_BK_Sub = ['경제>경제일반', '경제>국제경제', '경제>취업_창업',\n",
    "                   '사회>노동_복지', '사회>사건_사고', '사회>사회일반', '사회>여성', '사회>장애인', '사회>의료_건강',\n",
    "                   '문화>미술_건축', '문화>요리_여행', '문화>출판',\n",
    "                   '국제>중국', '국제>유럽_EU', '국제>일본', '국제>미국_북미', '국제>중동_아프리카',\n",
    "                   '국제>아시아', '국제>중남미', '국제>국제일반', '국제>러시아']\n",
    "CATEGORY_CR = ['세계', '경제', '생활/문화', '오피니언', '사회', 'IT/과학']\n",
    "COLNAME_CATEGORY = '일자'\n",
    "COLNAME_MINING = '제목'\n",
    "# SAVE_LOCATION = r'C:\\Users\\user\\Desktop\\Data'    # inu\n",
    "SAVE_LOCATION = r'C:\\Users\\KK\\Desktop\\Data'    # home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af50673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:36:05.093610Z",
     "start_time": "2023-11-08T05:36:04.805291Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_sentence_to_BERTinput(X_series, Y_series, tokenizer, \n",
    "                                        seq_len=128, batch_size=32, sampler='random'):\n",
    "    # BERT 입력 형식에 맞게 변환\n",
    "    sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in X_series]\n",
    "    \n",
    "    # 전처리\n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    for i in tqdm(range(len(X_series))):\n",
    "        ## 변환\n",
    "        token = tokenizer.encode_plus(X_series[i], max_length=seq_len,\n",
    "                                      pad_to_max_length=True, truncation=True,\n",
    "                                      return_attention_mask=True,\n",
    "                                      add_special_tokens=True)\n",
    "        ## 정리\n",
    "        tokens.append(token['input_ids'])\n",
    "        masks.append(token['attention_mask'])\n",
    "        segments.append(token['token_type_ids'])\n",
    "        targets.append(Y_series[i])\n",
    "    \n",
    "    # array 변환\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # tensor 변환\n",
    "    tokens = torch.tensor(tokens)\n",
    "    masks = torch.tensor(masks)\n",
    "    segments = torch.tensor(segments)\n",
    "    targets = torch.tensor(targets)\n",
    "    \n",
    "    # pytorch dataloader 연결\n",
    "    data = TensorDataset(tokens, masks, targets)\n",
    "    if sampler == 'random':\n",
    "        dataloader = DataLoader(data, sampler=RandomSampler(data), batch_size=batch_size)\n",
    "    elif sampler == 'sequential':\n",
    "        dataloader = DataLoader(data, sampler=SequentialSampler(data), batch_size=batch_size)\n",
    "        \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40baf6cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:36:11.634427Z",
     "start_time": "2023-11-08T05:36:11.346703Z"
    }
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e72cc0",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27135be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:57:44.951908Z",
     "start_time": "2023-11-08T05:57:44.422287Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(os.getcwd(), 'Data', 'MovieReview_NSMC', 'ratings_train.txt'), sep='\\t')\n",
    "test = pd.read_csv(os.path.join(os.getcwd(), 'Data', 'MovieReview_NSMC', 'ratings_test.txt'), sep='\\t')\n",
    "train, test = train[~train.document.isnull()], test[~test.document.isnull()]\n",
    "X_train, Y_train = train['document'], train['label']\n",
    "X_test, Y_test = test['document'], test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "145b0017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T14:52:41.616483Z",
     "start_time": "2023-11-07T14:52:30.650929Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████▌                                                            | 25857/150000 [00:04<00:23, 5227.42it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m X_test, Y_test \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m], test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)    \u001b[38;5;66;03m# 'bert-base-multilingual-cased', 'klue/roberta-base'\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing_sentence_to_BERTinput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m preprocessing_sentence_to_BERTinput(X_test, Y_test, tokenizer,\n\u001b[0;32m     10\u001b[0m                                                        seq_len\u001b[38;5;241m=\u001b[39mSEQ_LEN, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n",
      "Cell \u001b[1;32mIn[66], line 10\u001b[0m, in \u001b[0;36mpreprocessing_sentence_to_BERTinput\u001b[1;34m(X_series, Y_series, tokenizer, seq_len, batch_size, sampler)\u001b[0m\n\u001b[0;32m      7\u001b[0m tokens, masks, segments, targets \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_series))):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m## 변환\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_series\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mpad_to_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m## 정리\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mappend(token[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_kk\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2977\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2968\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2969\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2970\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2974\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2975\u001b[0m )\n\u001b[1;32m-> 2977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   2978\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2979\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   2980\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2981\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2982\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2983\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2984\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2985\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2986\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2987\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2988\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2989\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2990\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2991\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2992\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2993\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2994\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2995\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2996\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_kk\\lib\\site-packages\\transformers\\tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    717\u001b[0m     )\n\u001b[1;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    723\u001b[0m     first_ids,\n\u001b[0;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    739\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_kk\\lib\\site-packages\\transformers\\tokenization_utils.py:705\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string or a list/tuple of strings when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `is_split_into_words=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(os.getcwd(), 'Data', 'MovieReview_NSMC', 'ratings_train.txt'), sep='\\t')\n",
    "test = pd.read_csv(os.path.join(os.getcwd(), 'Data', 'MovieReview_NSMC', 'ratings_test.txt'), sep='\\t')\n",
    "X_train, Y_train = train['document'], train['label']\n",
    "X_test, Y_test = test['document'], test['label']\n",
    "   \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)    # 'bert-base-multilingual-cased', 'klue/roberta-base'\n",
    "train_dataloader = preprocessing_sentence_to_BERTinput(X_train, Y_train, tokenizer,\n",
    "                                                       seq_len=SEQ_LEN, batch_size=BATCH_SIZE)\n",
    "test_dataloader = preprocessing_sentence_to_BERTinput(X_test, Y_test, tokenizer,\n",
    "                                                       seq_len=SEQ_LEN, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2804f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68a7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2eb691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_kk",
   "language": "python",
   "name": "gpu_kk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
