{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f30a5c",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cfa0985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:02:10.531796Z",
     "start_time": "2023-11-04T14:02:10.526862Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 01:12:23,376\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "# warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from preprocessing_text_KK import *\n",
    "\n",
    "def get_data_from_path(folder_location, folder_name=False, concat_axis='row'):\n",
    "    # path_folder 하위의 모든 폴더위치와 내부 file 출력\n",
    "    df = pd.DataFrame()\n",
    "    print('Getting data from', len(os.listdir(folder_location)), 'folders...')\n",
    "    for (path, dir, files) in os.walk(folder_location):\n",
    "#         print(path)\n",
    "        for file in tqdm(files):\n",
    "            path_file = os.path.join(path, file)\n",
    "\n",
    "            ## 데이터 로딩\n",
    "            if path_file[-4:] == 'xlsx':\n",
    "                df_sub = pd.read_excel(path_file)\n",
    "            elif path_file[-3:] == 'csv':\n",
    "                df_sub = pd.read_csv(path_file)\n",
    "\n",
    "            ## 키워드 태깅 여부\n",
    "            if folder_name:\n",
    "                df_sub['Folder_Name'] = os.path.basename(path)\n",
    "            \n",
    "            ## 정리\n",
    "            if concat_axis == 'col':\n",
    "                df = pd.concat([df, df_sub], axis=1)\n",
    "            elif concat_axis == 'row':\n",
    "                df = pd.concat([df, df_sub], axis=0)\n",
    "                \n",
    "    return df\n",
    "\n",
    "# 하이퍼파라미터\n",
    "DELETE_KEYWORD = ['100세', '거주환경']\n",
    "CATEGORY_BK = ['경제', '사회', '문화', '국제']\n",
    "CATEGORY_BK_Sub = ['경제>경제일반', '경제>국제경제', '경제>취업_창업',\n",
    "                   '사회>노동_복지', '사회>사건_사고', '사회>사회일반', '사회>여성', '사회>장애인', '사회>의료_건강',\n",
    "                   '문화>미술_건축', '문화>요리_여행', '문화>출판',\n",
    "                   '국제>중국', '국제>유럽_EU', '국제>일본', '국제>미국_북미', '국제>중동_아프리카',\n",
    "                   '국제>아시아', '국제>중남미', '국제>국제일반', '국제>러시아']\n",
    "CATEGORY_CR = ['세계', '경제', '생활/문화', '오피니언', '사회', 'IT/과학']\n",
    "COLNAME_CATEGORY = '일자'\n",
    "COLNAME_MINING = '제목'\n",
    "### github에 업로드 되지 않도록 다른 폴더를 지정\n",
    "# 아래 예시는 내PC 바탕화면 Data 폴더를 지정\n",
    "SAVE_LOCATION = r'C:\\Users\\user\\Desktop\\Data'    # home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86745f91",
   "metadata": {},
   "source": [
    "# BigKinds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333cf75",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3f0483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T16:23:38.004014Z",
     "start_time": "2023-11-03T16:23:38.000242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from 39 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  2.00s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:41<00:00,  3.78s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.02s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.96s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.86s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.96s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.01s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.57s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.89s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.00s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.64s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:51<00:00,  4.71s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:20<00:00,  6.76s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.59s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.54s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.57s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.52s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:24<00:00,  4.99s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 509413/509413 [04:20<00:00, 1955.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from 51 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 51/51 [00:00<00:00, 109.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터로딩\n",
    "df_news = get_data_from_path(os.path.join(os.getcwd(), 'Data', 'BigKinds'), folder_name=True)\n",
    "# 전처리\n",
    "## 중복 처리\n",
    "df_news.drop_duplicates(subset=['뉴스 식별자', '언론사', COLNAME_MINING], inplace=True, ignore_index=True)\n",
    "## 불필요 변수 삭제\n",
    "colname_delete = ['뉴스 식별자', '인물', '위치', '기관', '기고자', '통합 분류2', '통합 분류3', \n",
    "                  '사건/사고 분류1', '사건/사고 분류2', '사건/사고 분류3',\n",
    "                  '키워드', '특성추출(가중치순 상위 50개)', 'URL', '분석제외 여부']\n",
    "df_news = df_news[[col for col in df_news.columns if col not in colname_delete]]\n",
    "## 카테고리 필터\n",
    "category_filter = [each for each in df_news['통합 분류1'].unique() if each.split('>')[0] in CATEGORY_BK]\n",
    "df_news = df_news[df_news['통합 분류1'].apply(lambda x: x in category_filter)].reset_index().iloc[:,1:]\n",
    "df_news['Category'] = df_news['통합 분류1'].apply(lambda x: x.split('>')[0])\n",
    "## 전처리\n",
    "df_news[COLNAME_MINING] = df_news[COLNAME_MINING].progress_apply(lambda x: text_preprocessor(x, del_number=False, del_bracket_content=False))\n",
    "## 결측치 및 빈문자 제거\n",
    "df_news = df_news[~df_news[COLNAME_MINING].isnull()].reset_index().iloc[:,1:].copy()\n",
    "df_news = df_news[df_news[COLNAME_MINING].str.len() != 0].reset_index().iloc[:,1:]\n",
    "\n",
    "# 날짜 변환\n",
    "## 연도 반영\n",
    "df_news[COLNAME_CATEGORY+'_Year'] = pd.to_datetime(df_news[COLNAME_CATEGORY].astype(str)).dt.year\n",
    "## 연도+월 반영\n",
    "df_news[COLNAME_CATEGORY+'_YearMonth'] = pd.to_datetime(df_news[COLNAME_CATEGORY].astype(str)).dt.strftime('%Y-%m')\n",
    "## 연도그룹 반영\n",
    "df_news[COLNAME_CATEGORY+'_Era'] = df_news[COLNAME_CATEGORY].apply(lambda x: '2013 ~ 2017' if str(x)[:4] in ['2013', '2014', '2015', '2016', '2017']\n",
    "                                                                                            else '2018 ~ 2023')\n",
    "\n",
    "# 나이대 변수 추가\n",
    "df_news['Age'] = df_news['제목'].apply(lambda x: 20 if re.search(' 20대', x) != None else\n",
    "                                                 (30 if re.search(' 30대', x) != None else\n",
    "                                                 (40 if re.search(' 40대', x) != None else\n",
    "                                                 (50 if re.search(' 50대', x) != None else\n",
    "                                                 (60 if re.search(' 60대', x) != None else\n",
    "                                                 (70 if re.search(' 70대', x) != None else\n",
    "                                                 (80 if re.search(' 80대', x) != None else\n",
    "                                                 (90 if re.search(' 90대', x) != None else 0))))))))\n",
    "\n",
    "# 긍부정 라벨 추가\n",
    "df_news_sentiment = get_data_from_path(os.path.join(os.getcwd(), 'Data', 'Sentiment'), folder_name=False)\n",
    "df_news_sentiment = df_news_sentiment.sort_values(by='Unnamed: 0').reset_index().iloc[:,2:]\n",
    "df_news_sentiment.columns = ['Sentiment']\n",
    "df_news_sentiment['Sentiment'] = df_news_sentiment.Sentiment.apply(lambda x: 'Positive' if x==2 else 'Negative')\n",
    "df_news_sentiment['Positive'] = df_news_sentiment.Sentiment.apply(lambda x: 1 if x=='Positive' else 0)\n",
    "df_news_sentiment['Negative'] = df_news_sentiment.Sentiment.apply(lambda x: -1 if x=='Negative' else 0)\n",
    "df_news = pd.concat([df_news, df_news_sentiment], axis=1)\n",
    "## 최대 중복 처리\n",
    "df_news.drop_duplicates(subset=['언론사', COLNAME_MINING], inplace=True, ignore_index=True)\n",
    "df_news.drop_duplicates(subset=[COLNAME_MINING], inplace=True, ignore_index=True)\n",
    "\n",
    "# 저장\n",
    "df_news.to_csv(os.path.join(SAVE_LOCATION, 'df_news_bigkinds.csv'), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80eebe40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:02:17.997278Z",
     "start_time": "2023-11-04T14:02:14.421982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  경제    182058\n",
      "사회    175598\n",
      "문화     96737\n",
      "국제     37249\n",
      "Name: Category, dtype: int64\n",
      "Category:  사회    143555\n",
      "경제     46072\n",
      "국제     35535\n",
      "문화     21051\n",
      "Name: Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 필터링한 결과를 최종적으로 사용\n",
    "df_news = pd.read_csv(os.path.join(SAVE_LOCATION, 'df_news_bigkinds.csv'))\n",
    "print('Category: ', df_news.Category.value_counts())\n",
    "keyword_filter = [each for each in df_news['Folder_Name'].unique() if each not in DELETE_KEYWORD]\n",
    "df_news = df_news[df_news['Folder_Name'].apply(lambda x: x in keyword_filter)].reset_index().iloc[:,1:]\n",
    "category_filter = [each for each in df_news['통합 분류1'].unique() if each in CATEGORY_BK_Sub]\n",
    "df_news = df_news[df_news['통합 분류1'].apply(lambda x: x in category_filter)].reset_index().iloc[:,1:]\n",
    "print('Category: ', df_news.Category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a08332",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ea419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.636 Gbse memory 1.689 Gb\n",
      "all cohesion probabilities was computed. # words = 54110\n",
      "all branching entropies was computed # words = 76657\n",
      "all accessor variety was computed # words = 76657\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 62858\n",
      "_noun_scores_ 13505\n",
      "after postprocessing 9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [04:08<04:08, 248.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.941 Gbse memory 1.993 Gb\n",
      "all cohesion probabilities was computed. # words = 67374\n",
      "all branching entropies was computed # words = 89549\n",
      "all accessor variety was computed # words = 89549\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 77383\n",
      "_noun_scores_ 16660\n",
      "after postprocessing 11498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [10:21<00:00, 310.96s/it]\n",
      "  0%|                                                                                            | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.692 Gbse memory 1.797 Gb\n",
      "all cohesion probabilities was computed. # words = 102021\n",
      "all branching entropies was computed # words = 135677\n",
      "all accessor variety was computed # words = 135677\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 116331\n",
      "_noun_scores_ 24614\n",
      "after postprocessing 17244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████████                                                                        | 1/9 [16:03<2:08:24, 963.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training ... (0 in 1088 sents) use memory 2.218 Gb\r",
      "training was done. used memory 2.218 Gb\n",
      "\r",
      " cohesion probabilities ... (1 in 1083)\r",
      "all cohesion probabilities was computed. # words = 697\n",
      "\r",
      "all branching entropies was computed # words = 1898\n",
      "\r",
      "all accessor variety was computed # words = 1898\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 1030\n",
      "_noun_scores_ 224\n",
      "after postprocessing 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▋                                                       | 3/9 [16:03<21:34, 215.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.689 Gby 1.689 Gb\n",
      "all cohesion probabilities was computed. # words = 374\n",
      "all branching entropies was computed # words = 1221\n",
      "all accessor variety was computed # words = 1221\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 599\n",
      "_noun_scores_ 120\n",
      "after postprocessing 69\n",
      "training was done. used memory 1.686 Gby 1.686 Gb\n",
      "all cohesion probabilities was computed. # words = 587\n",
      "all branching entropies was computed # words = 1652\n",
      "all accessor variety was computed # words = 1652\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 886\n",
      "_noun_scores_ 207\n",
      "after postprocessing 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████████████████████▉                                              | 4/9 [16:04<10:53, 130.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training ... (0 in 1039 sents) use memory 1.687 Gb\r",
      "training was done. used memory 1.687 Gb\n",
      "\r",
      " cohesion probabilities ... (1 in 981)\r",
      "all cohesion probabilities was computed. # words = 639\n",
      "\r",
      "all branching entropies was computed # words = 1801\n",
      "\r",
      "all accessor variety was computed # words = 1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████████████████████████▋                                     | 5/9 [16:04<05:35, 83.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 926\n",
      "_noun_scores_ 204\n",
      "after postprocessing 101\n",
      "training was done. used memory 1.683 Gby 1.683 Gb\n",
      "all cohesion probabilities was computed. # words = 489\n",
      "all branching entropies was computed # words = 1489\n",
      "all accessor variety was computed # words = 1489\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 773\n",
      "_noun_scores_ 172\n",
      "after postprocessing 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████████████████████                            | 6/9 [16:04<02:46, 55.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training ... (0 in 966 sents) use memory 1.684 Gb\r",
      "training was done. used memory 1.684 Gb\n",
      "\r",
      " cohesion probabilities ... (1 in 871)\r",
      "all cohesion probabilities was computed. # words = 532\n",
      "\r",
      "all branching entropies was computed # words = 1679\n",
      "\r",
      "all accessor variety was computed # words = 1679\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 841\n",
      "_noun_scores_ 191\n",
      "after postprocessing 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████████▎                  | 7/9 [16:05<01:14, 37.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training ... (0 in 860 sents) use memory 1.684 Gb\r",
      "training was done. used memory 1.684 Gb\n",
      "\r",
      " cohesion probabilities ... (1 in 791)\r",
      "all cohesion probabilities was computed. # words = 478\n",
      "\r",
      "all branching entropies was computed # words = 1488\n",
      "\r",
      "all accessor variety was computed # words = 1488\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [16:05<00:00, 107.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before postprocessing 710\n",
      "_noun_scores_ 156\n",
      "after postprocessing 83\n",
      "\r",
      "training ... (0 in 174 sents) use memory 1.684 Gb\r",
      "training was done. used memory 1.684 Gb\n",
      "\r",
      " cohesion probabilities ... (1 in 212)\r",
      "all cohesion probabilities was computed. # words = 101\n",
      "\r",
      "all branching entropies was computed # words = 373\n",
      "\r",
      "all accessor variety was computed # words = 373\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 138\n",
      "_noun_scores_ 23\n",
      "after postprocessing 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # 연도데이터 기준 전처리\n",
    "# wf_year_soy, waf_year_soy, \\\n",
    "# wf_year_tf, waf_year_tf, \\\n",
    "# wf_year_kb, waf_year_kb = preprocessing_wordfreq(df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year',\n",
    "#                                                  num_showkeyword=10, save_local=True, save_name='wordfreq_year')\n",
    "\n",
    "# # 연도그룹데이터 기준 전처리\n",
    "# wf_era_soy, waf_era_soy, \\\n",
    "# wf_era_tf, waf_era_tf, \\\n",
    "# wf_era_kb, waf_era_kb = preprocessing_wordfreq(df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era',\n",
    "#                                                num_showkeyword=10, save_local=True, save_name='wordfreq_era')\n",
    "# ## 안되면 아래줄 실행\n",
    "# if wf_era_tf.shape[0] == 0:\n",
    "#     wf_era_tf = wf_tf.copy()\n",
    "#     wf_era_tf.category.apply(lambda x: '2013 ~ 2017' if x in ['2013', '2014', '2015', '2016', '2017']\n",
    "#                                                       else '2018 ~ 2023')\n",
    "#     wf_era_tf = wf_era_tf.groupby(list(wf_era_tf.columns[:2])).mean().reset_index()\n",
    "#     waf_era_tf = pd.DataFrame()\n",
    "#     for category in tqdm(sorted(df_news[COLNAME_CATEGORY+'_Era'].unique())):\n",
    "#         df_sub = df_news[df_news[COLNAME_CATEGORY+'_Era'] == category]\n",
    "#         waf_era = preprocessing_adjwordcount(wf_era_tf[['word']], df_sub[COLNAME_MINING], num_showkeyword=5)\n",
    "#         waf_era['category'] = str(category)\n",
    "#         waf_era = waf_era[['category']+list(waf_era.columns[:-1])]\n",
    "#         waf_era_tf = pd.concat([waf_era_tf, waf_era], axis=0, ignore_index=True)\n",
    "#     save_name = os.path.join(os.getcwd(), 'Data', 'word_freq_tfidf_era.csv')\n",
    "#     wf_era_tf.to_csv(save_name, index=False, encoding='utf-8-sig')\n",
    "#     save_name = os.path.join(os.getcwd(), 'Data', 'wordadj_freq_tfidf_era.csv')\n",
    "#     waf_era_tf.to_csv(save_name, index=False, encoding='utf-8-sig')\n",
    "# ######################\n",
    "\n",
    "# # 연도감성데이터 기준 전처리\n",
    "# wf_senti_soy, waf_senti_soy, \\\n",
    "# wf_senti_tf, waf_senti_tf, \\\n",
    "# wf_senti_kb, waf_senti_kb = preprocessing_wordfreq(df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', \n",
    "#                                                    num_showkeyword=10, save_local=True, save_name='wordfreq_senti')\n",
    "\n",
    "# # 나이데이터 기준 전처리\n",
    "# wf_age_soy, waf_age_soy, \\\n",
    "# wf_age_tf, waf_age_tf, \\\n",
    "# wf_age_kb, waf_age_kb = preprocessing_wordfreq(df_news, colname_target=COLNAME_MINING, colname_category='Age',\n",
    "#                                                num_showkeyword=10, save_local=True, save_name='wordfreq_age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ac71f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:02:18.138336Z",
     "start_time": "2023-11-04T14:02:17.998286Z"
    }
   },
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "save_name_list=['wordfreq_year_soynlp.csv', 'wordfreq_year_soynlpadj.csv', \n",
    "                'wordfreq_year_tfidf.csv', 'wordfreq_year_tfidfadj.csv',\n",
    "                'wordfreq_year_keybert.csv', 'wordfreq_year_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_year_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_year_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_year_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_year_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_year_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_year_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordfreq_era_soynlp.csv', 'wordfreq_era_soynlpadj.csv', \n",
    "                'wordfreq_era_tfidf.csv', 'wordfreq_era_tfidfadj.csv',\n",
    "                'wordfreq_era_keybert.csv', 'wordfreq_era_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_era_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_era_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_era_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_era_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordfreq_senti_soynlp.csv', 'wordfreq_senti_soynlpadj.csv', \n",
    "                'wordfreq_senti_tfidf.csv', 'wordfreq_senti_tfidfadj.csv',\n",
    "                'wordfreq_senti_keybert.csv', 'wordfreq_senti_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_senti_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_senti_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_senti_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_senti_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_senti_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_senti_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordfreq_age_soynlp.csv', 'wordfreq_age_soynlpadj.csv', \n",
    "                'wordfreq_age_tfidf.csv', 'wordfreq_age_tfidfadj.csv',\n",
    "                'wordfreq_age_keybert.csv', 'wordfreq_age_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_age_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_age_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_age_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_age_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_age_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_age_keybert = pd.read_csv(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669b8ca",
   "metadata": {},
   "source": [
    "## Word Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0080934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:10:30.756437Z",
     "start_time": "2023-11-04T15:08:21.765552Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [02:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.43 GiB for an array with shape (109294, 4213) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # 관련성 전처리\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# wf_yearcorr_soynlp = preprocessing_wordfreq_to_corr(wf_year_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_soynlp.csv')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# waf_yearcorr_soynlp = preprocessing_wordfreq_to_corr(waf_year_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_soynlpadj.csv')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# wf_eracorr_tfidf = preprocessing_wordfreq_to_corr(wf_era_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_tfidf.csv')\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# waf_eracorr_tfidf = preprocessing_wordfreq_to_corr(waf_era_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_tfidf.csv')\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m wf_eracorr_keybert \u001b[38;5;241m=\u001b[39m preprocessing_wordfreq_to_corr(wf_era_keybert\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m10000\u001b[39m,:], \n\u001b[0;32m     14\u001b[0m                                                     df_news, colname_target\u001b[38;5;241m=\u001b[39mCOLNAME_MINING, colname_category\u001b[38;5;241m=\u001b[39mCOLNAME_CATEGORY\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Era\u001b[39m\u001b[38;5;124m'\u001b[39m, num_showkeyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordcorr_era_keybert.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m waf_eracorr_keybert \u001b[38;5;241m=\u001b[39m preprocessing_wordfreq_to_corr(waf_era_keybert\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m10000\u001b[39m,:], \n\u001b[0;32m     16\u001b[0m                                                      df_news, colname_target\u001b[38;5;241m=\u001b[39mCOLNAME_MINING, colname_category\u001b[38;5;241m=\u001b[39mCOLNAME_CATEGORY\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Era\u001b[39m\u001b[38;5;124m'\u001b[39m, num_showkeyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordcorr_era_keybert.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m wf_senticorr_soynlp \u001b[38;5;241m=\u001b[39m preprocessing_wordfreq_to_corr(wf_senti_soynlp, df_news, colname_target\u001b[38;5;241m=\u001b[39mCOLNAME_MINING, colname_category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, num_showkeyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordcorr_senti_soynlp.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\DataScience\\Lecture\\DEBA-Study\\preprocessing_text_KK.py:473\u001b[0m, in \u001b[0;36mpreprocessing_wordfreq_to_corr\u001b[1;34m(df_wordfreq, df, colname_target, colname_category, num_showkeyword, save_local, save_name)\u001b[0m\n\u001b[0;32m    470\u001b[0m df_sub \u001b[38;5;241m=\u001b[39m df[df[colname_category] \u001b[38;5;241m==\u001b[39m category]\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# 단어 벡터화 및 상관관계\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m _, _, word_corrpair \u001b[38;5;241m=\u001b[39m freq2vectorcorr_preprocessor(wf_sub\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:], \n\u001b[0;32m    474\u001b[0m                                                    df_sub[colname_target], num_showkeyword\u001b[38;5;241m=\u001b[39mnum_showkeyword)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m## 카테고리 추가\u001b[39;00m\n\u001b[0;32m    477\u001b[0m word_corrpair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(category)\n",
      "File \u001b[1;32mC:\\DataScience\\Lecture\\DEBA-Study\\preprocessing_text_KK.py:433\u001b[0m, in \u001b[0;36mfreq2vectorcorr_preprocessor\u001b[1;34m(df_wordfreq, df_series, num_showkeyword)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# series to dataframe vector\u001b[39;00m\n\u001b[0;32m    432\u001b[0m df_wordvec \u001b[38;5;241m=\u001b[39m df_series\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: word2vec_preprocessor(dict_wordfreq, x))\n\u001b[1;32m--> 433\u001b[0m df_wordvec \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df_wordvec\u001b[38;5;241m.\u001b[39mvalues], columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(dict_wordfreq\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m## vector가 0의 비율이 많은 word 제거\u001b[39;00m\n\u001b[0;32m    435\u001b[0m colnames_topweight \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries((df_wordvec \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m df_wordvec\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:806\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    805\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 806\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m    809\u001b[0m         data,\n\u001b[0;32m    810\u001b[0m         columns,\n\u001b[0;32m    811\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    812\u001b[0m         dtype,\n\u001b[0;32m    813\u001b[0m     )\n\u001b[0;32m    814\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    815\u001b[0m         arrays,\n\u001b[0;32m    816\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    820\u001b[0m     )\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:835\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays, columns\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 835\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[0;32m    837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_dict_to_arrays(data, columns)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:856\u001b[0m, in \u001b[0;36m_list_to_arrays\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    853\u001b[0m     content \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mto_object_array_tuples(data)\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# list of lists\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     content \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mto_object_array(data)\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "File \u001b[1;32mlib.pyx:2966\u001b[0m, in \u001b[0;36mpandas._libs.lib.to_object_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.43 GiB for an array with shape (109294, 4213) and data type object"
     ]
    }
   ],
   "source": [
    "# # 관련성 전처리\n",
    "# wf_yearcorr_soynlp = preprocessing_wordfreq_to_corr(wf_year_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_soynlp.csv')\n",
    "# waf_yearcorr_soynlp = preprocessing_wordfreq_to_corr(waf_year_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_soynlpadj.csv')\n",
    "# wf_yearcorr_tfidf = preprocessing_wordfreq_to_corr(wf_year_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_tfidf.csv')\n",
    "# waf_yearcorr_tfidf = preprocessing_wordfreq_to_corr(waf_year_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_tfidfadj.csv')\n",
    "# wf_yearcorr_keybert = preprocessing_wordfreq_to_corr(wf_year_keybert, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_keybert.csv')\n",
    "# waf_yearcorr_keybert = preprocessing_wordfreq_to_corr(waf_year_keybert, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_keybertadj.csv')\n",
    "\n",
    "# wf_eracorr_soynlp = preprocessing_wordfreq_to_corr(wf_era_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_soynlp.csv')\n",
    "# waf_eracorr_soynlp = preprocessing_wordfreq_to_corr(waf_era_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_soynlp.csv')\n",
    "# wf_eracorr_tfidf = preprocessing_wordfreq_to_corr(wf_era_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_tfidf.csv')\n",
    "# waf_eracorr_tfidf = preprocessing_wordfreq_to_corr(waf_era_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_tfidf.csv')\n",
    "wf_eracorr_keybert = preprocessing_wordfreq_to_corr(wf_era_keybert.sort_values(by='score', ascending=False).iloc[:10000,:], \n",
    "                                                    df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_keybert.csv')\n",
    "waf_eracorr_keybert = preprocessing_wordfreq_to_corr(waf_era_keybert.sort_values(by='score', ascending=False).iloc[:10000,:], \n",
    "                                                     df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_keybert.csv')\n",
    "\n",
    "wf_senticorr_soynlp = preprocessing_wordfreq_to_corr(wf_senti_soynlp, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_soynlp.csv')\n",
    "waf_senticorr_soynlp = preprocessing_wordfreq_to_corr(waf_senti_soynlp, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_soynlpadj.csv')\n",
    "wf_senticorr_tfidf = preprocessing_wordfreq_to_corr(wf_senti_tfidf, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_tfidf.csv')\n",
    "waf_senticorr_tfidf = preprocessing_wordfreq_to_corr(waf_senti_tfidf, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_tfidfadj.csv')\n",
    "wf_senticorr_keybert = preprocessing_wordfreq_to_corr(wf_senti_keybert, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_keybert.csv')\n",
    "waf_senticorr_keybert = preprocessing_wordfreq_to_corr(waf_senti_keybert, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_keybertadj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "save_name_list=['wordcorr_year_soynlp.csv', 'wordcorr_year_soynlpadj.csv', \n",
    "                'wordcorr_year_tfidf.csv', 'wordcorr_year_tfidfadj.csv',\n",
    "                'wordcorr_year_keybert.csv', 'wordcorr_year_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[0])\n",
    "wf_yearcorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[1])\n",
    "waf_yearcorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[2])\n",
    "wf_yearcorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[3])\n",
    "waf_yearcorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[4])\n",
    "wf_yearcorr_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[5])\n",
    "waf_yearcorr_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordcorr_era_soynlp.csv', 'wordcorr_era_soynlpadj.csv', \n",
    "                'wordcorr_era_tfidf.csv', 'wordcorr_era_tfidfadj.csv',\n",
    "                'wordcorr_era_keybert.csv', 'wordcorr_era_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[0])\n",
    "wf_eracorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[1])\n",
    "waf_eracorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[2])\n",
    "wf_eracorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[3])\n",
    "waf_eracorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[4])\n",
    "wf_eracorr_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[5])\n",
    "waf_eracorr_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordcorr_senti_soynlp.csv', 'wordcorr_senti_soynlpadj.csv', \n",
    "                'wordcorr_senti_tfidf.csv', 'wordcorr_senti_tfidfadj.csv',\n",
    "                'wordcorr_senti_keybert.csv', 'wordcorr_senti_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[0])\n",
    "wf_senticorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[1])\n",
    "waf_senticorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[2])\n",
    "wf_senticorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[3])\n",
    "waf_senticorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[4])\n",
    "wf_senticorr_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[5])\n",
    "waf_senticorr_keybert = pd.read_csv(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4addd0",
   "metadata": {},
   "source": [
    "# Naver Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968376e",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로딩\n",
    "df_news = get_data_from_path(os.path.join(os.getcwd(), 'Data', 'NaverNews'), folder_name=True)\n",
    "\n",
    "# 필터링\n",
    "## 중복 처리\n",
    "df_news.drop_duplicates(subset=['Press', 'Title'], inplace=True, ignore_index=True)\n",
    "## 불필요 변수 삭제\n",
    "colname_delete = ['Content', 'URL_Origin']\n",
    "df_news = df_news[[col for col in df_news.columns if col not in colname_delete]]\n",
    "## 카테고리 필터\n",
    "category_filter = [each for each in df_news.Category.unique() if each in CATEGORY_CR]\n",
    "df_news = df_news[df_news.Category.apply(lambda x: x in category_filter)].reset_index().iloc[:,1:]\n",
    "## 언론사 중복 필터\n",
    "df_news.Press = df_news.Press.progress_apply(lambda x: str(x).split('언론사 선정')[0])\n",
    "\n",
    "# 날짜 변환\n",
    "df_news.Date = pd.to_datetime(df_news.Date)\n",
    "## 연도 반영\n",
    "df_news['Date_Year'] = pd.to_datetime(df_news.Date.astype(str)).dt.year\n",
    "## 연도+월 반영\n",
    "df_news['Date_YearMonth'] = pd.to_datetime(df_news.Date.astype(str)).dt.strftime('%Y-%m')\n",
    "## 연도그룹 반영\n",
    "df_news['Date_Period'] = df_news.Date_Year.apply(lambda x: '2013 ~ 2017' if str(x)[:4] in ['2013', '2014', '2015', '2016', '2017']\n",
    "                                                                          else '2018 ~ 2023')\n",
    "df_news = df_news[['Folder_Name', 'Date', 'Date_Year', 'Date_YearMonth', 'Date_Period', 'Press', 'Category', 'Title', 'Comment', 'URL_Naver']]\n",
    "\n",
    "# 전처리\n",
    "df_news['Title'] = df_news['Title'].progress_apply(lambda x: text_preprocessor(x, del_number=False, \n",
    "                                                                               del_bracket_content=False))\n",
    "df_news = df_news[~df_news['Title'].isnull()].reset_index().iloc[:,1:].copy()\n",
    "df_news = df_news[df_news['Title'].str.len() != 0].reset_index().iloc[:,1:]\n",
    "## 댓글 길이가 5이상 & 갯수가 5개 이상 필터\n",
    "df_news['Comment'] = df_news['Comment'].progress_apply(lambda x: [i for i in literal_eval(x) if len(i) >= 5])\n",
    "df_news = df_news[df_news['Comment'].progress_apply(lambda x: len(x) >= 5)]\n",
    "\n",
    "# 언론사 필터\n",
    "## 댓글 평균이 5이상 필터 & 발행기사수 Top100 필터\n",
    "df_news['Comment_Len'] = df_news.Comment.apply(lambda x: len(x))\n",
    "df_temp = df_news.groupby('Press')['Comment_Len'].mean()\n",
    "del_press = list(pd.DataFrame(df_temp[df_temp < 5]).index)\n",
    "del_press = del_press + list(dict(df_news.Press.value_counts()).keys())[100:]\n",
    "df_news = df_news[~df_news.Press.isin(del_press)].reset_index().iloc[:,1:]\n",
    "df_news = df_news.drop('Comment_Len', axis=1)\n",
    "\n",
    "# 댓글기준 explode\n",
    "df_news_explode = df_news.copy()\n",
    "df_news_explode = df_news_explode.explode('Comment')\n",
    "\n",
    "# 저장\n",
    "df_news.to_csv(os.path.join(SAVE_LOCATION, 'df_news_crawling.csv'), index=False, encoding='utf-8-sig')\n",
    "df_news_explode.to_csv(os.path.join(SAVE_LOCATION, 'df_news_explode_crawling.csv'), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 불러오기\n",
    "df_news_nv = pd.read_csv(os.path.join(SAVE_LOCATION, 'df_news_crawling.csv'))\n",
    "print('Category: ', df_news_nv.Category.value_counts())\n",
    "df_newse_nv = pd.read_csv(os.path.join(SAVE_LOCATION, 'df_news_explode_crawling.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8f9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6f076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab82a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.591px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
