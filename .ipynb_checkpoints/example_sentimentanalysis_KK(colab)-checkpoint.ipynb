{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "S-YBXlce5XvH",
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1697782638156,
     "user": {
      "displayName": "Kyungwon Kim",
      "userId": "12494571024836189016"
     },
     "user_tz": -540
    },
    "id": "S-YBXlce5XvH"
   },
   "outputs": [],
   "source": [
    "# https://dacon.io/en/codeshare/1803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1RK-GkfNjpT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35258,
     "status": "ok",
     "timestamp": 1697782673856,
     "user": {
      "displayName": "Kyungwon Kim",
      "userId": "12494571024836189016"
     },
     "user_tz": -540
    },
    "id": "a1RK-GkfNjpT",
    "outputId": "d2a73942-7ba5-4e4e-95e6-4ccbdbe42937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nsmc' already exists and is not an empty directory.\n",
      "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: mxnet in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.5)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2023.7.22)\n",
      "Requirement already satisfied: gluonnlp==0.8.0 in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0) (1.23.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
      "Collecting kobert_tokenizer\n",
      "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-qqjz61l9/kobert-tokenizer_4ded8bfa2a154e799f1331b33c2060d2\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-qqjz61l9/kobert-tokenizer_4ded8bfa2a154e799f1331b33c2060d2\n",
      "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/e9t/nsmc.git\n",
    "!pip install tensorflow_addons\n",
    "!pip install torch>=1.8.1\n",
    "!pip install mxnet\n",
    "!pip install gluonnlp==0.8.0\n",
    "!pip install sentencepiece\n",
    "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8GgxHk2bCZI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2199,
     "status": "ok",
     "timestamp": 1697782676052,
     "user": {
      "displayName": "Kyungwon Kim",
      "userId": "12494571024836189016"
     },
     "user_tz": -540
    },
    "id": "c8GgxHk2bCZI",
    "outputId": "57c2ea7a-8596-4256-a38e-0881bc9416ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "\n",
    "TPU = False\n",
    "if TPU:\n",
    "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "  tf.config.experimental_connect_to_cluster(resolver)\n",
    "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "else:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "VYJ3D4yiNFrg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7822,
     "status": "ok",
     "timestamp": 1697782683873,
     "user": {
      "displayName": "Kyungwon Kim",
      "userId": "12494571024836189016"
     },
     "user_tz": -540
    },
    "id": "VYJ3D4yiNFrg",
    "outputId": "9d582a8f-597d-465c-de18-e8fa6ba638af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mxnet/optimizer/optimizer.py:163: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  warnings.warn('WARNING: New optimizer %s.%s is overriding '\n"
     ]
    }
   ],
   "source": [
    "# 모듈 파일 업로드\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# 모듈 로드\n",
    "from algorithm_KoBERT_KK import EarlyStopping #pytorchtools.py를 test accuracy 기준으로 작동시키기 위해 위의 클래스로 바꿔주었다.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "import gluonnlp as nlp\n",
    "from transformers import pipeline, AutoTokenizer, BertTokenizer, BertTokenizerFast\n",
    "from transformers import AutoModel, BertModel, TFBertModel, TFBertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 하이퍼파라미터\n",
    "device = torch.device(\"cuda:0\")\n",
    "max_len = 64\n",
    "batch_size = 32\n",
    "epoch = 100\n",
    "learning_rate =  5e-5\n",
    "warmup_ratio = 0.1\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "modelsave_location = os.path.join(os.getcwd(), 'gdrive', 'MyDrive', 'Research',\n",
    "                                  'Colab', 'Model', 'modeling_KoBERT_20231018.pt')\n",
    "predfile_location = os.path.join(os.getcwd(), 'gdrive', 'MyDrive', 'Research',\n",
    "                                  'Colab', 'Data', 'df_news.csv')\n",
    "\n",
    "class BERTSentenceTransform:\n",
    "    r\"\"\"BERT style data transformation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : BERTTokenizer.\n",
    "        Tokenizer for the sentences.\n",
    "    max_seq_length : int.\n",
    "        Maximum sequence length of the sentences.\n",
    "    pad : bool, default True\n",
    "        Whether to pad the sentences to maximum length.\n",
    "    pair : bool, default True\n",
    "        Whether to transform sentences or sentence pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_seq_length = max_seq_length\n",
    "        self._pad = pad\n",
    "        self._pair = pair\n",
    "        self._vocab = vocab\n",
    "\n",
    "    def __call__(self, line):\n",
    "        \"\"\"Perform transformation for sequence pairs or single sequences.\n",
    "\n",
    "        The transformation is processed in the following steps:\n",
    "        - tokenize the input sequences\n",
    "        - insert [CLS], [SEP] as necessary\n",
    "        - generate type ids to indicate whether a token belongs to the first\n",
    "        sequence or the second sequence.\n",
    "        - generate valid length\n",
    "\n",
    "        For sequence pairs, the input is a tuple of 2 strings:\n",
    "        text_a, text_b.\n",
    "\n",
    "        Inputs:\n",
    "            text_a: 'is this jacksonville ?'\n",
    "            text_b: 'no it is not'\n",
    "        Tokenization:\n",
    "            text_a: 'is this jack ##son ##ville ?'\n",
    "            text_b: 'no it is not .'\n",
    "        Processed:\n",
    "            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n",
    "            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "            valid_length: 14\n",
    "\n",
    "        For single sequences, the input is a tuple of single string:\n",
    "        text_a.\n",
    "\n",
    "        Inputs:\n",
    "            text_a: 'the dog is hairy .'\n",
    "        Tokenization:\n",
    "            text_a: 'the dog is hairy .'\n",
    "        Processed:\n",
    "            text_a: '[CLS] the dog is hairy . [SEP]'\n",
    "            type_ids: 0     0   0   0  0     0 0\n",
    "            valid_length: 7\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        line: tuple of str\n",
    "            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n",
    "            (text_a, text_b). For single sequences, the input is a tuple of single\n",
    "            string: (text_a,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n",
    "        np.array: valid length in 'int32', shape (batch_size,)\n",
    "        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to unicode\n",
    "        text_a = line[0]\n",
    "        if self._pair:\n",
    "            assert len(line) == 2\n",
    "            text_b = line[1]\n",
    "\n",
    "        tokens_a = self._tokenizer.tokenize(text_a)\n",
    "        tokens_b = None\n",
    "\n",
    "        if self._pair:\n",
    "            tokens_b = self._tokenizer(text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            self._truncate_seq_pair(tokens_a, tokens_b,\n",
    "                                    self._max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > self._max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
    "\n",
    "        # The embedding vectors for `type=0` and `type=1` were learned during\n",
    "        # pre-training and are added to the wordpiece embedding vector\n",
    "        # (and position vector). This is not *strictly* necessary since\n",
    "        # the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        #vocab = self._tokenizer.vocab\n",
    "        vocab = self._vocab\n",
    "        tokens = []\n",
    "        tokens.append(vocab.cls_token)\n",
    "        tokens.extend(tokens_a)\n",
    "        tokens.append(vocab.sep_token)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens.extend(tokens_b)\n",
    "            tokens.append(vocab.sep_token)\n",
    "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
    "\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The valid length of sentences. Only real  tokens are attended to.\n",
    "        valid_length = len(input_ids)\n",
    "\n",
    "        if self._pad:\n",
    "            # Zero-pad up to the sequence length.\n",
    "            padding_length = self._max_seq_length - valid_length\n",
    "            # use padding tokens for the rest\n",
    "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
    "            segment_ids.extend([0] * padding_length)\n",
    "\n",
    "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
    "            np.array(segment_ids, dtype='int32')\n",
    "\n",
    "class BERTDataset():\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
    "                 pad, pair):\n",
    "        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        #transform = nlp.data.BERTSentenceTransform(\n",
    "        #    tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=5,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict=False)\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ZxlrtEzrMRPO",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1697782683873,
     "user": {
      "displayName": "Kyungwon Kim",
      "userId": "12494571024836189016"
     },
     "user_tz": -540
    },
    "id": "ZxlrtEzrMRPO"
   },
   "outputs": [],
   "source": [
    "# 데이터처리\n",
    "train = pd.read_table(\"nsmc/\"+\"ratings_train.txt\")\n",
    "train = train.dropna().sample(5000).reset_index().iloc[:,1:].iloc[:,1:].values.tolist()\n",
    "test = pd.read_table(\"nsmc/\"+\"ratings_test.txt\")\n",
    "test = test.dropna().sample(5000).reset_index().iloc[:,1:].iloc[:,1:].values.tolist()\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "\n",
    "data_train = BERTDataset(train, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "data_test = BERTDataset(test, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "train_dataloader = DataLoader(data_train, batch_size=batch_size, num_workers=2)\n",
    "test_dataloader = DataLoader(data_test, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "# 모델링세팅\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "t_total = len(train_dataloader) * epoch\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 학습\n",
    "train_history = []\n",
    "test_history = []\n",
    "loss_history = []\n",
    "## EarlyStopping 설정\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True, mode='max') #mode='min은 손실 기준\n",
    "## 초기 정확도 설정\n",
    "best_test_accuracy = 0.0\n",
    "\n",
    "for e in range(epoch):\n",
    "    ## Training\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        # print(label.shape, out.shape)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "            train_history.append(train_acc / (batch_id+1))\n",
    "            loss_history.append(loss.data.cpu().numpy())\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    # train_history.append(train_acc / (batch_id+1))\n",
    "    \n",
    "    ## Testing\n",
    "    model.eval() # 모델을 평가 모드로 설정\n",
    "    test_acc = 0.0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids) # 모델에 입력 데이터 전달하여 출력 얻기\n",
    "        test_acc += calc_accuracy(out, label) # 정확도 계산하여 누적\n",
    "    test_acc /= len(test_dataloader)\n",
    "    print(f'epoch {e + 1} - Test Accuracy: {test_acc}')\n",
    "    \n",
    "    ## EarlyStopping 적용\n",
    "    early_stopping(test_acc, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    test_history.append(test_acc)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if test_acc > best_test_accuracy:\n",
    "        best_test_accuracy = test_acc\n",
    "        best_epoch = e + 1\n",
    "        print(f'Validation max increased ({best_test_accuracy:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), modelsave_location)\n",
    "        \n",
    "# 최종 best test accuracy와 epoch 출력\n",
    "print(f'Best Test Accuracy: {best_test_accuracy} at Epoch {best_epoch}')\n",
    "\n",
    "# 시각화\n",
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "## Training Accuracy\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Accuracy', color=color)\n",
    "ax1.plot(range(1, len(train_history) + 1), train_history, color=color, label='Training Accuracy')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "## Test Accuracy\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Test Accuracy', color=color)\n",
    "ax2.plot(range(1, len(test_history) + 1), test_history, color=color, label='Test Accuracy')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "## validation loss\n",
    "min_loss_epoch = loss_history.index(min(loss_history)) + 1\n",
    "ax1.axvline(min_loss_epoch, linestyle='--', color='r', label='Best Loss')\n",
    "## Best test accuracy\n",
    "max_test_acc_epoch = test_history.index(max(test_history)) + 1\n",
    "ax1.axvline(max_test_acc_epoch, linestyle='--', color='g', label='Best Test Accuracy')\n",
    "## 라벨 표시\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.grid(True)\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "plt.title('Training Accuracy & Test Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AZRB_Jpyax-K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZRB_Jpyax-K",
    "outputId": "26c3a052-eac9-4be8-a1e6-dd44a712ae36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "100%|██████████| 10000/10000 [50:59<00:00,  3.27it/s]\n",
      " 73%|███████▎  | 7335/10000 [51:34<22:20,  1.99it/s]"
     ]
    }
   ],
   "source": [
    "# 모델 및 예측데이터 로딩\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "model.load_state_dict(torch.load(modelsave_location))\n",
    "df_news = pd.read_csv(predfile_location)\n",
    "## 예측함수\n",
    "def predict_sentiment(sentence):\n",
    "    data = [sentence, '0']\n",
    "    dataset = [data]\n",
    "    test_data = BERTDataset(dataset, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        predicted_index = out.argmax() + 1\n",
    "\n",
    "    return predicted_index.item()\n",
    "## 예측\n",
    "tqdm.pandas()\n",
    "# df_news_sentiment = df_news['제목'][:100000].progress_apply(predict_sentiment)\n",
    "# predsave_location = os.path.join(os.getcwd(), 'gdrive', 'MyDrive', 'Research',\n",
    "#                                   'Colab', 'Data', 'df_news_sentiment1.csv')\n",
    "# df_news_sentiment.to_csv(predsave_location)\n",
    "for i in range(290000, 300000, 10000):\n",
    "    df_news_sentiment = df_news['제목'][i:i+10000].progress_apply(predict_sentiment)\n",
    "    predsave_location = os.path.join(os.getcwd(), 'gdrive', 'MyDrive', 'Research',\n",
    "                                     'Colab', 'Data', 'df_news_sentiment'+str(i)+'.csv')\n",
    "    df_news_sentiment.to_csv(predsave_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uuc69lF3e4q7",
   "metadata": {
    "id": "uuc69lF3e4q7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bKk_MJe4t3",
   "metadata": {
    "id": "06bKk_MJe4t3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YdyvMEeMKHTs",
   "metadata": {
    "id": "YdyvMEeMKHTs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20421f35",
   "metadata": {
    "id": "20421f35"
   },
   "outputs": [],
   "source": [
    "# # !git clone https://github.com/e9t/nsmc.git\n",
    "# train = pd.read_table(\"nsmc/\"+\"ratings_train.txt\")\n",
    "# train = train.dropna().sample(5000).reset_index().iloc[:,1:]\n",
    "# test = pd.read_table(\"nsmc/\"+\"ratings_test.txt\")\n",
    "# test = test.dropna().sample(5000).reset_index().iloc[:,1:]\n",
    "\n",
    "# data_train = BERTDataset(train, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "# data_test = BERTDataset(test, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "\n",
    "\n",
    "# def preprocessing_sentence_to_BERTinput(df, tokenizer, colname_data, colname_target=None, seq_len=128,\n",
    "#                                         return_type='tensor'):\n",
    "#     tokens, masks, segments, targets = [], [], [], []\n",
    "#     for i in tqdm(range(len(df))):\n",
    "#         # 변환\n",
    "#         token = tokenizer.encode_plus(df[colname_data][i], max_length=seq_len,\n",
    "#                                       pad_to_max_length=True, truncation=True,\n",
    "#                                       return_attention_mask=True,\n",
    "#                                       add_special_tokens=True)\n",
    "\n",
    "#         # 정리\n",
    "#         tokens.append(token['input_ids'])\n",
    "#         masks.append(token['attention_mask'])\n",
    "#         segments.append(token['token_type_ids'])\n",
    "#         if colname_target != None:\n",
    "#             targets.append(df[colname_target][i])\n",
    "\n",
    "#     # array 변환\n",
    "#     tokens = np.array(tokens)\n",
    "#     masks = np.array(masks)\n",
    "#     segments = np.array(segments)\n",
    "#     if colname_target != None:\n",
    "#         targets = np.array(targets)\n",
    "\n",
    "#     # tensor 변환\n",
    "#     if return_type == 'tensor':\n",
    "#         tokens = tf.convert_to_tensor(tokens, dtype=tf.int32)\n",
    "#         masks = tf.convert_to_tensor(masks, dtype=tf.int32)\n",
    "#         segments = tf.convert_to_tensor(segments, dtype=tf.int32)\n",
    "\n",
    "#     return [tokens, masks, segments], targets\n",
    "\n",
    "# import tensorflow_addons as tfa\n",
    "# from transformers import pipeline, AutoTokenizer, BertTokenizer, BertTokenizerFast\n",
    "# from transformers import AutoModel, AutoModelForTokenClassification, TFBertModel, TFBertForSequenceClassification\n",
    "\n",
    "# MODEL_NAME = 'monologg/kobert'    # 'bert-base-multilingual-cased', 'klue/roberta-base'\n",
    "# # OPTIMIZER = tfa.optimizers.RectifiedAdam(lr=1.0e-5, weight_decay=0.0025, warmup_proportion=0.05)\n",
    "# OPTIMIZER = tf.keras.optimizers.Adam(lr=1.0e-5)\n",
    "# NUM_LABELS = 2\n",
    "# SEQ_LEN = 64\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "# X_train, Y_train = preprocessing_sentence_to_BERTinput(train, tokenizer=tokenizer,\n",
    "#                                                        colname_target='label', colname_data='document', seq_len=SEQ_LEN)\n",
    "# X_test, Y_test = preprocessing_sentence_to_BERTinput(test, tokenizer=tokenizer,\n",
    "#                                                        colname_target='label', colname_data='document', seq_len=SEQ_LEN)\n",
    "\n",
    "# def modeling_BERTsentiment(model_name, optimizer, num_labels=2, seq_len=128):\n",
    "#     # 모델 로딩\n",
    "#     model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "#     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#     metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "#     model.compile(optimizer=optimizer, loss=loss, metrics=metric)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # def modeling_BERTsentiment(model_name, optimizer, num_labels=2, seq_len=128):\n",
    "# #     # 모델 로딩\n",
    "# #     model = TFBertModel.from_pretrained(model_name, num_labels=num_labels, output_hidden_states=True)\n",
    "# #     outputs = model([tokens, masks, segments])[1]\n",
    "\n",
    "# #     # 모델 구성\n",
    "# #     layer = tf.keras.layers.Dense(1, activation='sigmoid',\n",
    "# #                                   kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(outputs)\n",
    "# #     model_sentiment = tf.keras.Model([tokens, masks, segments], layer)\n",
    "# #     model_sentiment.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])\n",
    "\n",
    "# #     return model_sentiment\n",
    "\n",
    "\n",
    "# model = modeling_BERTsentiment(model_name=MODEL_NAME, optimizer=OPTIMIZER, num_labels=NUM_LABELS, seq_len=SEQ_LEN)\n",
    "# model.fit(X_train, Y_train, epochs=10, shuffle=True, batch_size=100, validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mCMFpXiYO5zC",
   "metadata": {
    "id": "mCMFpXiYO5zC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HJFTUj4iO6xQ",
   "metadata": {
    "id": "HJFTUj4iO6xQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
