{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87bb4c76",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9d1de577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T11:51:26.518067Z",
     "start_time": "2023-10-15T11:51:24.739148Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "# warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# System related and data input controls\n",
    "import os\n",
    "\n",
    "# Python path\n",
    "import sys\n",
    "base_folder = 'DataScience'\n",
    "location_base = os.path.join(os.getcwd().split(base_folder)[0], base_folder)\n",
    "location_module = [os.path.join(location_base, 'Module')] \n",
    "for each in location_module:\n",
    "    if each not in sys.path:\n",
    "        sys.path.append(each)\n",
    "\n",
    "# Auto reload of library\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from import_KK import *\n",
    "DeviceStrategy_CPU()\n",
    "from preprocessing_KK import *\n",
    "from preprocessing_text_KK import * ##\n",
    "from visualization_KK import * ##\n",
    "from algorithm_KK import *\n",
    "from evaluation_KK import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a5fb6",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "806a928d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T11:51:28.302239Z",
     "start_time": "2023-10-15T11:51:26.520068Z"
    }
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "FOLDER_LOCATION = os.path.join(os.getcwd(), 'Data', '빅카인즈')\n",
    "FOLDER_NAME = True\n",
    "CATEGORY = ['경제', '사회', '문화', '국제']\n",
    "IMAGE_LOCATION = os.path.join('.', 'Data', 'baby-icon_ver1.png')\n",
    "COLNAME_CATEGORY = '일자'\n",
    "COLNAME_MINING = '제목'\n",
    "CONSERVATIVE = ['동아일보', '중앙일보', '조선일보', '매일경제', '한국경제']\n",
    "PROGRESSIVE = ['한겨례', '경향신문', '머니투데이']\n",
    "################\n",
    "test_criteria = '2023-01-01'\n",
    "SEQUENCE = 5\n",
    "Y_SCALING = True\n",
    "MOVING_TYPE = 'sliding'    # 'sliding', 'expanding'\n",
    "TRAIN_WINDOW = 100\n",
    "FORECASTING_PERIOD = 4+24\n",
    "################\n",
    "# metrics.SCORERS.keys(): 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error'\n",
    "LOSS_ML = 'neg_mean_squared_error' \n",
    "PARAMS_BAG = None\n",
    "# PARAMS_BAG = {'n_estimators': [30, 50, 90],     \n",
    "#           'max_depth': [20, 30, 40, 50],  \n",
    "#           'max_leaf_nodes': [5, 10, 15]}\n",
    "PARAMS_BOOST = None\n",
    "# PARAMS_BOOST = {'n_estimators': [30, 50, 90],    \n",
    "#           'max_depth': [20, 30, 40, 50],   \n",
    "#           'num_leaves': [5, 10, 15],\n",
    "#           'min_child_weight': [3, 5, 7],\n",
    "#           'learning_rate': [0.1, 0.01],\n",
    "#           'force_col_wise': [False], 'force_row_wise': [True]}\n",
    "CV_SPLITS = 5\n",
    "################\n",
    "KERNEL_SIZE = 2\n",
    "STRIDE = 1\n",
    "POOL_SIZE = 1\n",
    "POOL_STRIDE = 1\n",
    "HIDDEN_ACTIVATION = 'relu'\n",
    "OUTPUT_ACTIVATION = 'linear'\n",
    "REGULARIZER = None\n",
    "DROPOUT_RATIO = 0.2\n",
    "MODEL_SUMMARY = False\n",
    "LOSS = 'mse'\n",
    "LEARNING_RATE = 0.01\n",
    "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "METRICS = ['mse']\n",
    "VALIDATION_SPLIT = 0.2\n",
    "VALIDATION_DATA = None\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500\n",
    "VERBOSE = 0\n",
    "################\n",
    "EARLYSTOP_PATIENT = EPOCHS*0.4\n",
    "MONITOR = 'val_loss'\n",
    "LEARNING_PLOT = False\n",
    "PLOT_TITLE = \"How People's Interest Changes Over Time\"\n",
    "PLOT_XLABEL = 'Time'\n",
    "PLOT_YLABEL = 'Interest Forecasting'\n",
    "################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471c419",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7cd01",
   "metadata": {},
   "source": [
    "## Google Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "22e01525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T11:51:30.092096Z",
     "start_time": "2023-10-15T11:51:28.303284Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터로딩\n",
    "file_location = os.path.join(os.getcwd(), 'Data', 'ageism_survey_KK.xlsx')\n",
    "df_gt = pd.read_excel(file_location, sheet_name='GT_Trend', index_col='Time')\n",
    "df_gt_global = df_gt[['Ageism']].copy()\n",
    "df_gt_local = df_gt[['고령화']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb30016",
   "metadata": {},
   "source": [
    "## BigKinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ccf1a57b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:00:32.618638Z",
     "start_time": "2023-10-15T11:51:30.094147Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from 39 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.03s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:42<00:00,  3.88s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.42s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:18<00:00,  6.22s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.05s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.14s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.66s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.08s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.40s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.74s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.99s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.85s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:52<00:00,  4.79s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.01s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.43s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.49s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.71s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:20<00:00,  6.76s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.52s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.48s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.04s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.05s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.52s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.28s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.78s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.12s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사회>의료_건강', '문화>방송_연예', '사회>노동_복지', '경제>금융_재테크', '경제>경제일반', '국제>중국', '국제>유럽_EU', '문화>출판', '경제>취업_창업', '경제>부동산', '국제>일본', '경제>산업_기업', '경제>유통', '사회>교육_시험', '사회>여성', '국제>미국_북미', '경제>증권_증시', '문화>미술_건축', '문화>음악', '문화>문화일반', '경제>자동차', '문화>학술_문화재', '국제>중동_아프리카', '사회>사회일반', '문화>생활', '문화>종교', '경제>국제경제', '사회>미디어', '경제>무역', '국제>아시아', '국제>중남미', '경제>서비스_쇼핑', '문화>전시_공연', '경제>반도체', '문화>요리_여행', '문화>영화', '사회>사건_사고', '국제>국제일반', '경제>자원', '사회>장애인', '국제>러시아', '경제>외환', '사회>환경', '사회>날씨']\n"
     ]
    }
   ],
   "source": [
    "# ### 하위의 모든 데이터 결합하여 출력\n",
    "# def get_data_from_path(folder_location, folder_name=False, concat_axis='row'):\n",
    "#     # path_folder 하위의 모든 폴더위치와 내부 file 출력\n",
    "#     df = pd.DataFrame()\n",
    "#     print('Getting data from', len(os.listdir(folder_location)), 'folders...')\n",
    "#     for (path, dir, files) in os.walk(folder_location):\n",
    "#         for file in tqdm(files):\n",
    "#             path_file = os.path.join(path, file)\n",
    "\n",
    "#             ## 데이터 로딩\n",
    "#             if path_file.split('.')[1] == 'xlsx':\n",
    "#                 df_sub = pd.read_excel(path_file)\n",
    "#             elif path_file.split('.')[1] == 'csv':\n",
    "#                 df_sub = pd.read_csv(path_file)\n",
    "                \n",
    "#             ## 키워드 태깅 여부\n",
    "#             if folder_name:\n",
    "#                 df_sub['Folder_Name'] = os.path.basename(path)\n",
    "            \n",
    "#             ## 정리\n",
    "#             if concat_axis == 'col':\n",
    "#                 df = pd.concat([df, df_sub], axis=1)\n",
    "#             elif concat_axis == 'row':\n",
    "#                 df = pd.concat([df, df_sub], axis=0)\n",
    "                \n",
    "#     return df\n",
    "\n",
    "# # 데이터로딩\n",
    "# df_news = get_data_from_path(FOLDER_LOCATION, folder_name=FOLDER_NAME)\n",
    "# ## 중복 처리\n",
    "# df_news.drop_duplicates(subset=['뉴스 식별자', '언론사', '제목'], inplace=True, ignore_index=True)\n",
    "# ## 불필요 변수 삭제\n",
    "# colname_delete = ['뉴스 식별자', '통합 분류3', '사건/사고 분류1', '사건/사고 분류2', '사건/사고 분류3',\n",
    "#                   '키워드', '특성추출(가중치순 상위 50개)', 'URL', '분석제외 여부']\n",
    "# df_news = df_news[[col for col in df_news.columns if col not in colname_delete]]\n",
    "# ## 날짜 인식\n",
    "# df_news['일자'] = pd.to_datetime(df_news['일자'].astype(str))\n",
    "# ## 월별 트랜드용 데이터분리\n",
    "# df_news_rising = df_news[['Folder_Name', '일자', '제목']].copy()\n",
    "# df_news_rising['일자'] = pd.to_datetime(df_news_rising['일자'].dt.strftime('%Y-%m'))\n",
    "# df_news_rising = df_news_rising.groupby(['Folder_Name', '일자'])['제목'].count().unstack(level=0).fillna(0)\n",
    "# ## 연도만 남기기\n",
    "# df_news['일자'] = df_news['일자'].dt.year\n",
    "# ## 카테고리 필터\n",
    "# category_filter = [each for each in df_news['통합 분류1'].unique() if each.split('>')[0] in CATEGORY]\n",
    "# print(category_filter)\n",
    "# df_news = df_news[df_news['통합 분류1'].apply(lambda x: x in category_filter)].reset_index().iloc[:,1:]\n",
    "# ## 전처리\n",
    "# df_news['제목'] = df_news['제목'].apply(lambda x: text_preprocessor(x, del_number=False, del_bracket_content=False))\n",
    "# ## 결측치 및 빈문자 제거\n",
    "# df_news = df_news[~df_news['제목'].isnull()].reset_index().iloc[:,1:].copy()\n",
    "# df_news = df_news[df_news['제목'].str.len() != 0].reset_index().iloc[:,1:]\n",
    "# ## 언론사 분리\n",
    "# df_newsc = df_news[df_news['언론사'].isin(CONSERVATIVE)].reset_index()\n",
    "# df_newsp = df_news[df_news['언론사'].isin(PROGRESSIVE)].reset_index()\n",
    "# ## 날짜 변환 데이터\n",
    "# df_news_era, df_newsc_era, df_newsp_era = df_news.copy(), df_newsc.copy(), df_newsp.copy()\n",
    "# df_news_era['일자'] = df_news_era['일자'].apply(lambda x: '2013 ~ 2017' if x in [2013, 2014, 2015, 2016, 2017] \n",
    "#                                                                          else '2018 ~ 2023')\n",
    "# df_newsc_era['일자'] = df_newsc_era['일자'].apply(lambda x: '2013 ~ 2017' if x in [2013, 2014, 2015, 2016, 2017] \n",
    "#                                                                          else '2018 ~ 2023')\n",
    "# df_newsp_era['일자'] = df_newsp_era['일자'].apply(lambda x: '2013 ~ 2017' if x in [2013, 2014, 2015, 2016, 2017] \n",
    "#                                                                          else '2018 ~ 2023')\n",
    "# ## 저장\n",
    "# df_news_rising.to_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_news_rising.csv'), index=False, encoding='utf-8-sig')\n",
    "# df_news.to_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_news.csv'), index=False, encoding='utf-8-sig')\n",
    "# df_newsc.to_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_newsc.csv'), index=False, encoding='utf-8-sig')\n",
    "# df_newsp.to_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_newsp.csv'), index=False, encoding='utf-8-sig')\n",
    "# df_news_era.to_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_news_era.csv'), index=False, encoding='utf-8-sig')\n",
    "# df_newsc_era.to_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_newsc_era.csv'), index=False, encoding='utf-8-sig')\n",
    "# df_newsp_era.to_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_newsp_era.csv'), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "049b5828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:00:59.726351Z",
     "start_time": "2023-10-15T12:00:32.620676Z"
    }
   },
   "outputs": [],
   "source": [
    "## 불러오기\n",
    "df_news_rising = pd.read_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_news_rising.csv'))\n",
    "df_news = pd.read_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_news.csv'))\n",
    "df_newsc = pd.read_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_newsc.csv'))\n",
    "df_newsp = pd.read_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_newsp.csv'))\n",
    "df_news_era = pd.read_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_news_era.csv'))\n",
    "df_newsc_era = pd.read_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_newsc_era.csv'))\n",
    "df_newsp_era = pd.read_csv(os.path.join(r'C:\\Users\\KK\\Desktop', 'df_newsp_era.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8fd42",
   "metadata": {},
   "source": [
    "## Word Frequency\n",
    "\n",
    "- **데이터:** 구글 뉴스 + 네이버 뉴스\n",
    "- **카테고리:** 경세 + 사회 + 문화 + 국제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "03638b4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:01:01.565251Z",
     "start_time": "2023-10-15T12:00:59.728361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 언론사: ['아주경제' '동아일보' '세계일보' '머니투데이' '중앙일보' '서울경제' '헤럴드경제' '문화일보' '국민일보' '서울신문'\n",
      " '디지털타임스' '한국경제' '아시아경제' '매일경제' '내일신문' 'SBS' '파이낸셜뉴스' '경향신문' '전자신문' '한겨레'\n",
      " 'YTN' '한국일보' 'MBC' 'OBS' 'KBS' '조선일보']\n",
      "\n",
      "보수 언론사: ['동아일보' '중앙일보' '한국경제' '매일경제' '조선일보']\n",
      "\n",
      "진보 언론하: ['머니투데이' '경향신문']\n"
     ]
    }
   ],
   "source": [
    "print('전체 언론사:', df_news['언론사'].unique())\n",
    "print('\\n보수 언론사:', df_newsc['언론사'].unique())\n",
    "print('\\n진보 언론하:', df_newsp['언론사'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0904f61e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T15:48:08.944284Z",
     "start_time": "2023-10-15T15:37:56.483075Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.35s/it]\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 2.442 Gbory 2.435 Gb\n",
      "all cohesion probabilities was computed. # words = 34750\n",
      "all branching entropies was computed # words = 49321\n",
      "all accessor variety was computed # words = 49321\n",
      "C:/Users/KK/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 41055\n",
      "_noun_scores_ 8983\n",
      "after postprocessing 6194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [05:02<05:02, 302.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 4.023 Gbory 3.732 Gb\n",
      "all cohesion probabilities was computed. # words = 46259\n",
      "all branching entropies was computed # words = 58545\n",
      "all accessor variety was computed # words = 58545\n",
      "C:/Users/KK/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 53983\n",
      "_noun_scores_ 12359\n",
      "after postprocessing 8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [08:36<00:00, 258.45s/it]\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.812 Gbory 1.808 Gb\n",
      "all cohesion probabilities was computed. # words = 16599\n",
      "all branching entropies was computed # words = 32282\n",
      "all accessor variety was computed # words = 32282\n",
      "C:/Users/KK/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 20981\n",
      "_noun_scores_ 4582\n",
      "after postprocessing 3048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:44<00:44, 44.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 3.987 Gbory 3.983 Gb\n",
      "all cohesion probabilities was computed. # words = 17030\n",
      "all branching entropies was computed # words = 22021\n",
      "all accessor variety was computed # words = 22021\n",
      "C:/Users/KK/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 20810\n",
      "_noun_scores_ 4728\n",
      "after postprocessing 3160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:29<00:00, 44.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# # 연도데이터 기준 전처리\n",
    "# wf_soy, waf_soy, wf_tf, waf_tf = preprocessing_wordfreq(df_news, colname_target='제목', colname_category='일자', \n",
    "#                          save_local=True)\n",
    "# wfc_soy, wafc_soy, wfc_tf, wafc_tf = preprocessing_wordfreq(df_newsc, colname_target='제목', colname_category='일자', \n",
    "#                          save_local=True,\n",
    "#                          save_name_list=['word_freq_soynlp_c.csv', 'wordadj_freq_soynlp_c.csv', \n",
    "#                                          'word_freq_tfidf_c.csv', 'wordadj_freq_tfidf_c.csv'])\n",
    "# wfp_soy, wafp_soy, wfp_tf, wafp_tf = preprocessing_wordfreq(df_newsp, colname_target='제목', colname_category='일자', \n",
    "#                          save_local=True,\n",
    "#                          save_name_list=['word_freq_soynlp_p.csv', 'wordadj_freq_soynlp_p.csv', \n",
    "#                                          'word_freq_tfidf_p.csv', 'wordadj_freq_tfidf_p.csv'])\n",
    "# # 연도그룹데이터 기준 전처리\n",
    "# wf_era_soy, waf_era_soy, wf_era_tf, waf_era_tf = preprocessing_wordfreq(df_news_era, colname_target='제목', colname_category='일자', \n",
    "#                          save_local=True,\n",
    "#                          save_name_list=['word_freq_soynlp_era.csv', 'wordadj_freq_soynlp_era.csv', \n",
    "#                                          'word_freq_tfidf_era.csv', 'wordadj_freq_tfidf_era.csv'])\n",
    "# ## 안되면 아래줄 실행\n",
    "# if wf_era_tf.shape[0] == 0:\n",
    "#     wf_era_tf = wf_tf.copy()\n",
    "#     wf_era_tf.category.apply(lambda x: '2013 ~ 2017' if x in ['2013', '2014', '2015', '2016', '2017']\n",
    "#                                                       else '2018 ~ 2023')\n",
    "#     wf_era_tf = wf_era_tf.groupby(list(wf_era_tf.columns[:2])).mean().reset_index()\n",
    "#     waf_era_tf = pd.DataFrame()\n",
    "#     for category in tqdm(sorted(df_news_era[COLNAME_CATEGORY].unique())):\n",
    "#         df_sub = df_news_era[df_news_era[COLNAME_CATEGORY] == category]\n",
    "#         waf_era = preprocessing_adjwordcount(wf_era_tf[['word']], df_sub[COLNAME_MINING], num_showkeyword=5)\n",
    "#         waf_era['category'] = str(category)\n",
    "#         waf_era = waf_era[['category']+list(waf_era.columns[:-1])]\n",
    "#         waf_era_tf = pd.concat([waf_era_tf, waf_era], axis=0, ignore_index=True)\n",
    "#     save_name = os.path.join(os.getcwd(), 'Data', 'word_freq_tfidf_era.csv')\n",
    "#     wf_era_tf.to_csv(save_name, index=False, encoding='utf-8-sig')\n",
    "#     save_name = os.path.join(os.getcwd(), 'Data', 'wordadj_freq_tfidf_era.csv')\n",
    "#     waf_era_tf.to_csv(save_name, index=False, encoding='utf-8-sig')\n",
    "# ######################\n",
    "# wfc_era_soy, wafc_era_soy, wfc_era_tf, wafc_era_tf = preprocessing_wordfreq(df_newsc_era, colname_target='제목', colname_category='일자', \n",
    "#                          save_local=True,\n",
    "#                          save_name_list=['word_freq_soynlp_erac.csv', 'wordadj_freq_soynlp_erac.csv', \n",
    "#                                          'word_freq_tfidf_erac.csv', 'wordadj_freq_tfidf_erac.csv'])\n",
    "# wfp_era_soy, wafp_era_soy, wfp_era_tf, wafp_era_tf = preprocessing_wordfreq(df_newsp_era, colname_target='제목', colname_category='일자', \n",
    "#                          save_local=True,\n",
    "#                          save_name_list=['word_freq_soynlp_erap.csv', 'wordadj_freq_soynlp_erap.csv', \n",
    "#                                          'word_freq_tfidf_erap.csv', 'wordadj_freq_tfidf_erap.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f682cbf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T15:48:12.287505Z",
     "start_time": "2023-10-15T15:48:08.947276Z"
    }
   },
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "save_name_list=['word_freq_soynlp.csv', 'wordadj_freq_soynlp.csv', \n",
    "                'word_freq_tfidf.csv', 'wordadj_freq_tfidf.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[0])\n",
    "wf_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[1])\n",
    "waf_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[2])\n",
    "wf_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[3])\n",
    "waf_tfidf = pd.read_csv(save_name)\n",
    "\n",
    "save_name_list=['word_freq_soynlp_c.csv', 'wordadj_freq_soynlp_c.csv', \n",
    "                'word_freq_tfidf_c.csv', 'wordadj_freq_tfidf_c.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[0])\n",
    "wfc_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[1])\n",
    "wafc_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[2])\n",
    "wfc_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[3])\n",
    "wafc_tfidf = pd.read_csv(save_name)\n",
    "\n",
    "save_name_list=['word_freq_soynlp_p.csv', 'wordadj_freq_soynlp_p.csv', \n",
    "                'word_freq_tfidf_p.csv', 'wordadj_freq_tfidf_p.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[0])\n",
    "wfp_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[1])\n",
    "wafp_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[2])\n",
    "wfp_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[3])\n",
    "wafp_tfidf = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['word_freq_soynlp_era.csv', 'wordadj_freq_soynlp_era.csv', \n",
    "                'word_freq_tfidf_era.csv', 'wordadj_freq_tfidf_era.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[0])\n",
    "wf_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[1])\n",
    "waf_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[2])\n",
    "wf_era_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[3])\n",
    "waf_era_tfidf = pd.read_csv(save_name)\n",
    "\n",
    "save_name_list=['word_freq_soynlp_erac.csv', 'wordadj_freq_soynlp_erac.csv', \n",
    "                'word_freq_tfidf_erac.csv', 'wordadj_freq_tfidf_erac.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[0])\n",
    "wfc_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[1])\n",
    "wafc_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[2])\n",
    "wfc_era_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[3])\n",
    "wafc_era_tfidf = pd.read_csv(save_name)\n",
    "\n",
    "save_name_list=['word_freq_soynlp_erap.csv', 'wordadj_freq_soynlp_erap.csv', \n",
    "                'word_freq_tfidf_erap.csv', 'wordadj_freq_tfidf_erap.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[0])\n",
    "wfp_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[1])\n",
    "wafp_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[2])\n",
    "wfp_era_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[3])\n",
    "wafp_era_tfidf = pd.read_csv(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5d3ec",
   "metadata": {},
   "source": [
    "## Word Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b5972ab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T16:12:59.100938Z",
     "start_time": "2023-10-15T15:48:12.288551Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:38<00:00, 19.01s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.19s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:12<00:00,  1.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.15s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.49s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [15:23<00:00, 923.15s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.72s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [07:54<00:00, 237.19s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# # 관련성 전처리\n",
    "# wf_corr_soynlp = preprocessing_wordfreq_to_corr(wf_era_soynlp, df_news_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='word_corrpair_soynlp_era.csv')\n",
    "# waf_corr_soynlp = preprocessing_wordfreq_to_corr(waf_era_soynlp, df_news_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='wordadj_corrpair_soynlp_era.csv')\n",
    "# wf_corr_tfidf = preprocessing_wordfreq_to_corr(wf_era_tfidf, df_news_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='word_corrpair_tfidf_era.csv')\n",
    "# waf_corr_tfidf = preprocessing_wordfreq_to_corr(waf_era_tfidf, df_news_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='wordadj_corrpair_tfidf_era.csv')\n",
    "\n",
    "# wfc_corr_soynlp = preprocessing_wordfreq_to_corr(wfc_era_soynlp, df_newsc_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='word_corrpair_soynlp_era_c.csv')\n",
    "# wafc_corr_soynlp = preprocessing_wordfreq_to_corr(wafc_era_soynlp, df_newsc_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='wordadj_corrpair_soynlp_era_c.csv')\n",
    "# wfc_corr_tfidf = preprocessing_wordfreq_to_corr(wfc_era_tfidf, df_newsc_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='word_corrpair_tfidf_era_c.csv')\n",
    "# wafc_corr_tfidf = preprocessing_wordfreq_to_corr(wafc_era_tfidf, df_newsc_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='wordadj_corrpair_tfidf_era_c.csv')\n",
    "\n",
    "# wfp_corr_soynlp = preprocessing_wordfreq_to_corr(wfp_era_soynlp, df_newsp_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='word_corrpair_soynlp_era_p.csv')\n",
    "# wafp_corr_soynlp = preprocessing_wordfreq_to_corr(wafp_era_soynlp, df_newsp_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='wordadj_corrpair_soynlp_era_p.csv')\n",
    "# wfp_corr_tfidf = preprocessing_wordfreq_to_corr(wfp_era_tfidf, df_newsp_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='word_corrpair_tfidf_era_p.csv')\n",
    "# wafp_corr_tfidf = preprocessing_wordfreq_to_corr(wafp_era_tfidf, df_newsp_era, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY, num_showkeyword=100, save_name='wordadj_corrpair_tfidf_era_p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6d660ab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T16:13:01.379580Z",
     "start_time": "2023-10-15T16:12:59.113964Z"
    }
   },
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "save_name_list=['word_corrpair_soynlp_era.csv', 'wordadj_corrpair_soynlp_era.csv', \n",
    "                'word_corrpair_tfidf_era.csv', 'wordadj_corrpair_tfidf_era.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[0])\n",
    "wf_corr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[1])\n",
    "waf_corr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[2])\n",
    "wf_corr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[3])\n",
    "waf_corr_tfidf = pd.read_csv(save_name)\n",
    "\n",
    "save_name_list=['word_corrpair_soynlp_era_c.csv', 'wordadj_corrpair_soynlp_era_c.csv', \n",
    "                'word_corrpair_tfidf_era_c.csv', 'wordadj_corrpair_tfidf_era_c.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[0])\n",
    "wfc_corr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[1])\n",
    "wafc_corr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[2])\n",
    "wfc_corr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[3])\n",
    "wafc_corr_tfidf = pd.read_csv(save_name)\n",
    "\n",
    "save_name_list=['word_corrpair_soynlp_era_p.csv', 'wordadj_corrpair_soynlp_era_p.csv', \n",
    "                'word_corrpair_tfidf_era_p.csv', 'wordadj_corrpair_tfidf_era_p.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[0])\n",
    "wfp_corr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[1])\n",
    "wafp_corr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[2])\n",
    "wfp_corr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', save_name_list[3])\n",
    "wafp_corr_tfidf = pd.read_csv(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e555054",
   "metadata": {},
   "source": [
    "# 연구문제0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a248e0",
   "metadata": {},
   "source": [
    "## Ageism Global Trend and Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0769485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.757736Z",
     "start_time": "2023-10-15T12:18:41.757736Z"
    }
   },
   "outputs": [],
   "source": [
    "for each in [df_gt_global, df_gt_local]:\n",
    "    model, Score_te = modeling_LGBMRegressor_slidingwindow1D(each, SEQUENCE=SEQUENCE, Y_SCALING=Y_SCALING, MOVING_TYPE=MOVING_TYPE,\n",
    "                                                   TRAIN_WINDOW=TRAIN_WINDOW, FORECASTING_PERIOD=FORECASTING_PERIOD,\n",
    "                                                   LOSS=LOSS_ML, GRIDSEARCH_PARAMS=PARAMS_BOOST, CV_SPLITS=CV_SPLITS,\n",
    "                                                   PLOT_TITLE=PLOT_TITLE, PLOT_XLABEL=PLOT_XLABEL, PLOT_YLABEL=PLOT_YLABEL)\n",
    "    display(Score_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28167b93",
   "metadata": {},
   "source": [
    "## Korea Trend by Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a8de0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.759730Z",
     "start_time": "2023-10-15T12:18:41.759730Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 증가변수 필터\n",
    "RISING_CATEG = ['고령화', '노인+경제적+자립', '노인+봉사', '노인+의료', \n",
    "                '노인+주택+문제', '노인+혐오', '돌봄서비스', '세대+갈등', '소회', \n",
    "                '안락사', '알츠하이머', '연령+통합']\n",
    "df_news_rising = df_news_rising[RISING_CATEG].copy()\n",
    "## 시각화\n",
    "plot_timeseries_dforigin(df_news_rising, save_local=True, save_name_initial='gt_total_origin.png')\n",
    "plot_timeseries_dfmeanstd(df_news_rising, scaled=True, save_local=True, save_name_initial='gt_total_scaled.png')\n",
    "plot_timeseries(df_news_rising, save_local=True, save_name_initial='gt_each_origin.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda85dce",
   "metadata": {},
   "source": [
    "# 연구문제1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426b804",
   "metadata": {},
   "source": [
    "## 과거 10년 동안의 온라인 검색어 트랜드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a43138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.761723Z",
     "start_time": "2023-10-15T12:18:41.761723Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# word freq 비교 트랜드 시각화\n",
    "plot_treemap_wordfreq(wf_soynlp, num_showkeyword=100, title='Ageism by Year', plot_studio=True, save_local=True, save_name='trend_year_compare_treemap.html')\n",
    "plot_sunburst_wordfreq(waf_soynlp, title='Ageism by Year', plot_studio=True, save_local=True, save_name='trend_year_compare_sunburst.html')\n",
    "plot_bar_wordfreq(wf_soynlp, figsize=(30,8), num_showkeyword=20, title='Ageism by Year', save_local=True, save_name='trend_year_compare_bar_byword.png')\n",
    "plot_bar_wordfreq(waf_soynlp, figsize=(30,8), num_showkeyword=20, title='Ageism by Year', save_local=True, save_name='trend_year_compare_bar_bywordadj.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0719708",
   "metadata": {},
   "source": [
    "## 고령사회 전후의 온라인 검색어 트랜드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fe1a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.762719Z",
     "start_time": "2023-10-15T12:18:41.762719Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 연도별 word freq 시각화\n",
    "centrality_categ = pd.DataFrame()\n",
    "for category in wf_era_soynlp[wf_era_soynlp.columns[0]].unique():\n",
    "    print(category)\n",
    "    wf_sub = wf_era_soynlp.groupby(wf_era_soynlp.columns[0]).get_group(category)\n",
    "    wc_sub = wf_corr_soynlp[wf_corr_soynlp.category.isin([category])]\n",
    "    \n",
    "    plot_wordcloud(wf_sub.iloc[:,1:], mask_colorgen=False, max_words=100, mask_location=IMAGE_LOCATION, save_local=True, save_name='trend_era_wordcloud_'+'~'.join(category.split(' ~ '))+'.png')\n",
    "    plot_bar_wordfreq(wf_sub.iloc[:,1:], figsize=(16,8), num_showkeyword=100, title='Ageism by Era', \n",
    "                      save_local=True, save_name='trend_era_bar_'+'~'.join(category.split(' ~ '))+'.png')\n",
    "    plot_donut_wordfreq(wf_sub.iloc[:,1:], num_showkeyword=30, save_local=True, save_name='trend_era_donut_'+'~'.join(category.split(' ~ '))+'.html')\n",
    "    _, centrality = plot_networkx(wf_sub, wc_sub.iloc[:,1:], \n",
    "                                  filter_criteria=0.02, plot=True, node_size='pagerank', save_local=True, save_name='trend_era_networkx_'+'~'.join(category.split(' ~ '))+'.png')\n",
    "    ## 카테고리 추가\n",
    "    centrality['category'] = str(category)\n",
    "    centrality_categ = pd.concat([centrality_categ, centrality], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57352d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.764713Z",
     "start_time": "2023-10-15T12:18:41.764713Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# word freq 비교 트랜드 시각화\n",
    "plot_treemap_wordfreq(wf_era_soynlp, num_showkeyword=100, title='Ageism_Era', plot_studio=True, save_name='trend_era_compare_treemap_'+'~'.join(category.split(' ~ '))+'.html')\n",
    "plot_sunburst_wordfreq(waf_era_soynlp, title='Ageism_Era', plot_studio=True, save_name='trend_era_compare_sunburst_'+'~'.join(category.split(' ~ '))+'.html')\n",
    "plot_bar_wordfreq(wf_era_soynlp, figsize=(20,8), num_showkeyword=20, title='Ageism_Era', save_local=True, save_name='trend_era_compare_bar_byword_'+'~'.join(category.split(' ~ '))+'.png')\n",
    "plot_bar_wordfreq(waf_era_soynlp, figsize=(20,8), num_showkeyword=20, title='Ageism_Era', save_local=True, save_name='trend_era_compare_bar_bywordadj_'+'~'.join(category.split(' ~ '))+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b352bf",
   "metadata": {},
   "source": [
    "## 과거 10년 동안의 보수 및 진보 트랜드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173a997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.766706Z",
     "start_time": "2023-10-15T12:18:41.766706Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# word freq 비교 트랜드 시각화\n",
    "plot_treemap_wordfreq(wfc_soynlp, num_showkeyword=100, title='Ageism by Year+Conservative', plot_studio=True, save_local=True, save_name='trend_yearc_compare_treemap.html')\n",
    "plot_treemap_wordfreq(wfp_soynlp, num_showkeyword=100, title='Ageism by Year+Progressive', plot_studio=True, save_local=True, save_name='trend_yearp_compare_treemap.html')\n",
    "plot_sunburst_wordfreq(wafc_soynlp, title='Ageism by Year+Conservative', plot_studio=True, save_local=True, save_name='trend_yearc_compare_sunburst.html')\n",
    "plot_sunburst_wordfreq(wafp_soynlp, title='Ageism by Year+Progressive', plot_studio=True, save_local=True, save_name='trend_yearp_compare_sunburst.html')\n",
    "plot_bar_wordfreq(wfc_soynlp, figsize=(30,8), num_showkeyword=20, title='Ageism by Year+Conservative', save_local=True, save_name='trend_yearc_compare_bar_byword.png')\n",
    "plot_bar_wordfreq(wfp_soynlp, figsize=(30,8), num_showkeyword=20, title='Ageism by Year+Progressive', save_local=True, save_name='trend_yearp_compare_bar_byword.png')\n",
    "plot_bar_wordfreq(wafc_soynlp, figsize=(30,8), num_showkeyword=20, title='Ageism by Year+Conservative', save_local=True, save_name='trend_yearc_compare_bar_bywordadj.png')\n",
    "plot_bar_wordfreq(wafp_soynlp, figsize=(30,8), num_showkeyword=20, title='Ageism by Year+Progressive', save_local=True, save_name='trend_yearp_compare_bar_bywordadj.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd4194",
   "metadata": {},
   "source": [
    "## 고령사회 전후의 보수 및 진보 트랜드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887dd28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.767702Z",
     "start_time": "2023-10-15T12:18:41.767702Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 연도별 word freq 시각화\n",
    "centrality_categ = pd.DataFrame()\n",
    "for category in wfc_era_soynlp[wfc_era_soynlp.columns[0]].unique():\n",
    "    print(category)\n",
    "    wf_subc = wfc_era_soynlp.groupby(wfc_era_soynlp.columns[0]).get_group(category)\n",
    "    wc_subc = wfc_corr_soynlp[wfc_corr_soynlp.category.isin([category])]\n",
    "    wf_subp = wfp_era_soynlp.groupby(wfp_era_soynlp.columns[0]).get_group(category)\n",
    "    wc_subp = wfp_corr_soynlp[wfp_corr_soynlp.category.isin([category])]\n",
    "    \n",
    "    plot_wordcloud(wf_subc.iloc[:,1:], mask_colorgen=False, max_words=100, mask_location=IMAGE_LOCATION, save_local=True, save_name='trend_erac_wordcloud_'+'~'.join(category.split(' ~ '))+'.png')\n",
    "    plot_wordcloud(wf_subp.iloc[:,1:], mask_colorgen=False, max_words=100, mask_location=IMAGE_LOCATION, save_local=True, save_name='trend_erap_wordcloud_'+'~'.join(category.split(' ~ '))+'.png')\n",
    "    plot_bar_wordfreq(wf_subc.iloc[:,1:], num_showkeyword=100, save_local=True, save_name='trend_erac_bar_'+'~'.join(category.split(' ~ '))+'.png')\n",
    "    plot_bar_wordfreq(wf_subp.iloc[:,1:], num_showkeyword=100, save_local=True, save_name='trend_erap_bar_'+'~'.join(category.split(' ~ '))+'.png')\n",
    "    plot_donut_wordfreq(wf_subc.iloc[:,1:], num_showkeyword=30, save_local=True, save_name='trend_erac_donut_'+'~'.join(category.split(' ~ '))+'.html')\n",
    "    plot_donut_wordfreq(wf_subp.iloc[:,1:], num_showkeyword=30, save_local=True, save_name='trend_erap_donut_'+'~'.join(category.split(' ~ '))+'.html')\n",
    "    _, centrality = plot_networkx(wf_subc, wc_subc.iloc[:,1:], \n",
    "                                  filter_criteria=0.025, plot=True, node_size='pagerank', save_local=True, save_name='trend_erac_networkx_'+'~'.join(category.split(' ~ '))+'.png')\n",
    "    _, centrality = plot_networkx(wf_subp, wc_subp.iloc[:,1:], \n",
    "                                  filter_criteria=0.025, plot=True, node_size='pagerank', save_local=True, save_name='trend_erap_networkx_'+'~'.join(category.split(' ~ '))+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8042b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.769695Z",
     "start_time": "2023-10-15T12:18:41.769695Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# word freq 비교 트랜드 시각화\n",
    "plot_treemap_wordfreq(wfc_era_soynlp, num_showkeyword=100, title='Ageism_Era_Conservative', plot_studio=True, save_local=True, save_name='trend_erac_compare_treemap.html')\n",
    "plot_treemap_wordfreq(wfp_era_soynlp, num_showkeyword=100, title='Ageism_Era_Progressive', plot_studio=True, save_local=True, save_name='trend_erap_compare_treemap.html')\n",
    "plot_sunburst_wordfreq(wafc_era_soynlp, title='Ageism_Era_Conservative', plot_studio=True, save_name='trend_erac_compare_sunburst.html')\n",
    "plot_sunburst_wordfreq(wafp_era_soynlp, title='Ageism_Era_Progressive', plot_studio=True, save_name='trend_erap_compare_sunburst.html')\n",
    "plot_bar_wordfreq(wfc_era_soynlp, figsize=(20,8), num_showkeyword=20, title='Ageism_Era_Conservative', save_local=True, save_name='trend_erac_compare_bar_byword.png')\n",
    "plot_bar_wordfreq(wfp_era_soynlp, figsize=(20,8), num_showkeyword=20, title='Ageism_Era_Progressive', save_local=True, save_name='trend_erap_compare_bar_byword.png')\n",
    "plot_bar_wordfreq(wafc_era_soynlp, figsize=(20,8), num_showkeyword=20, title='Ageism_Era_Conservative', save_local=True, save_name='trend_erac_compare_bar_bywordadj.png')\n",
    "plot_bar_wordfreq(wafp_era_soynlp, figsize=(20,8), num_showkeyword=20, title='Ageism_Era_Progressive', save_local=True, save_name='trend_erap_compare_bar_bywordadj.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f05219",
   "metadata": {},
   "source": [
    "## 과거 10년 동안의 온라인 검색어 긍부정 데이터량 트랜드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f0abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcac9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20095bca",
   "metadata": {},
   "source": [
    "## 고령사회 전후의 온라인 검색어 긍부정 데이터량 트랜드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5936f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea3629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40c8f538",
   "metadata": {},
   "source": [
    "## 고령사회 전후의 보수 및 진보 긍부정 데이터량 트랜드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2d39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25496acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5edbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fe06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb5f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05f00af2",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26915b88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.771688Z",
     "start_time": "2023-10-15T12:18:41.771688Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e83df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.773682Z",
     "start_time": "2023-10-15T12:18:41.773682Z"
    }
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/e9t/nsmc.git\n",
    "train = pd.read_table(\"nsmc/\"+\"ratings_train.txt\")\n",
    "train = train.dropna().sample(5000).reset_index().iloc[:,1:]\n",
    "test = pd.read_table(\"nsmc/\"+\"ratings_test.txt\")\n",
    "test = test.dropna().sample(5000).reset_index().iloc[:,1:]\n",
    "\n",
    "def preprocessing_sentence_to_BERTinput(df, tokenizer, colname_data, colname_target=None, seq_len=128,\n",
    "                                        return_type='tensor'):\n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        # 변환\n",
    "        token = tokenizer.encode_plus(df[colname_data][i], max_length=seq_len,\n",
    "                                      pad_to_max_length=True, truncation=True,\n",
    "                                      return_attention_mask=True,\n",
    "                                      add_special_tokens=True)\n",
    "\n",
    "        # 정리\n",
    "        tokens.append(token['input_ids'])\n",
    "        masks.append(token['attention_mask'])\n",
    "        segments.append(token['token_type_ids'])\n",
    "        if colname_target != None:\n",
    "            targets.append(df[colname_target][i])\n",
    "    \n",
    "    # array 변환\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    if colname_target != None:\n",
    "        targets = np.array(targets)\n",
    "        \n",
    "    # tensor 변환\n",
    "    if return_type == 'tensor':\n",
    "        tokens = tf.convert_to_tensor(tokens, dtype=tf.int32)\n",
    "        masks = tf.convert_to_tensor(masks, dtype=tf.int32)\n",
    "        segments = tf.convert_to_tensor(segments, dtype=tf.int32)\n",
    "\n",
    "    return [tokens, masks, segments], targets\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from transformers import pipeline, AutoTokenizer, BertTokenizer, BertTokenizerFast\n",
    "from transformers import AutoModel, AutoModelForTokenClassification, TFBertModel, TFBertForSequenceClassification\n",
    "\n",
    "MODEL_NAME = 'monologg/kobert'    # 'bert-base-multilingual-cased' \n",
    "# OPTIMIZER = tfa.optimizers.RectifiedAdam(lr=1.0e-5, weight_decay=0.0025, warmup_proportion=0.05)\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(lr=1.0e-5)\n",
    "NUM_LABELS = 2\n",
    "SEQ_LEN = 64\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "X_train, Y_train = preprocessing_sentence_to_BERTinput(train, tokenizer=tokenizer,\n",
    "                                                       colname_target='label', colname_data='document', seq_len=SEQ_LEN)\n",
    "X_test, Y_test = preprocessing_sentence_to_BERTinput(test, tokenizer=tokenizer,\n",
    "                                                       colname_target='label', colname_data='document', seq_len=SEQ_LEN)\n",
    "\n",
    "def modeling_BERTsentiment(model_name, optimizer, num_labels=2, seq_len=128):\n",
    "    # 모델 로딩\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metric)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = modeling_BERTsentiment(model_name=MODEL_NAME, optimizer=OPTIMIZER, num_labels=NUM_LABELS, seq_len=SEQ_LEN)\n",
    "# model.fit(X_train, Y_train, epochs=10, shuffle=True, batch_size=100, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949ee0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532299d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ab2b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.774679Z",
     "start_time": "2023-10-15T12:18:41.774679Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://teddylee777.github.io/huggingface/bert-kor-text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4a219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.776672Z",
     "start_time": "2023-10-15T12:18:41.776672Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, BertTokenizer, BertTokenizerFast\n",
    "from transformers import AutoModel, AutoModelForTokenClassification, TFBertModel, TFBertForSequenceClassification\n",
    "\n",
    "TASK = 'text-classification'\n",
    "MODEL_NAME = 'monologg/kobert' # 'bert-base-multilingual-cased'\n",
    "SENTENCES = df_news['제목'].to_list()[:2]\n",
    "SEQ_LEN = 32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer(SENTENCES, max_length=SEQ_LEN, \n",
    "          padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "# model_sa = pipeline(task=TASK, model=MODEL_NAME)\n",
    "# result = model_sa(df_news['제목'].to_list()[:100])\n",
    "# for i, j in zip(df_news['제목'].to_list()[:100], result):\n",
    "#     print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da4617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.777669Z",
     "start_time": "2023-10-15T12:18:41.777669Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "X_train = preprocessing_sentence_to_BERTinput(df_news['제목'][:100], tokenizer=tokenizer)\n",
    "\n",
    "def modeling_BERTsentiment(model_name, optimizer, seq_len=128):\n",
    "    # 입력 변환\n",
    "    tokens = tf.keras.layers.Input((seq_len,), dtype=tf.int32, name='input_ids')\n",
    "    masks = tf.keras.layers.Input((seq_len,), dtype=tf.int32, name='input_masks')\n",
    "    segments = tf.keras.layers.Input((seq_len,), dtype=tf.int32, name='input_segments')\n",
    "    \n",
    "    # 모델 로딩\n",
    "    model = TFBertModel.from_pretrained(model_name)\n",
    "    outputs = model([tokens, masks, segments])[1]\n",
    "    \n",
    "    # 모델 구성\n",
    "    layer = tf.keras.layers.Dense(1, activation='sigmoid', \n",
    "                                  kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(outputs)\n",
    "    model_sentiment = tf.keras.Model([tokens, masks, segments], layer)\n",
    "    model_sentiment.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])\n",
    "    \n",
    "    return model_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35756a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df544a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421a001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.779662Z",
     "start_time": "2023-10-15T12:18:41.779662Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'word_freq_soynlp.csv')\n",
    "word_freq_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'wordadj_freq_soynlp.csv')\n",
    "wordadj_freq_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'word_score_tfidf.csv')\n",
    "word_score_tfidf = pd.read_csv(save_name)\n",
    "\n",
    "FREQ_USING = word_freq_soynlp.copy()    # word_freq_soynlp, wordadj_freq_soynlp, word_score_tfidf\n",
    "\n",
    "dict_word_corr, df_word_corrpair = dict(), pd.DataFrame()\n",
    "centrality_years = pd.DataFrame()\n",
    "for year in tqdm(sorted(df[COLNAME_CATEGORY].dt.year.unique())):\n",
    "    print(year)\n",
    "    \n",
    "    # 데이터 분리\n",
    "    df_sub = df[df[COLNAME_CATEGORY].dt.year == year]\n",
    "    df_subfreq = FREQ_USING[FREQ_USING[FREQ_USING.columns[0]] == year]\n",
    "    \n",
    "    # 단어 벡터화 및 상관관계\n",
    "    _, word_corr, word_corrpair = preprocessing_wordfreq_to_vectorcorr(df_subfreq.iloc[:,-2:], \n",
    "                                                                       df_sub[COLNAME_MINING])\n",
    "    dict_word_corr[year] = word_corr\n",
    "#     ## 카테고리 추가\n",
    "#     word_corrpair['year'] = str(year)\n",
    "#     word_corrpair = word_corrpair[['year']+list(word_corrpair.columns[:-1])]\n",
    "#     df_word_corrpair = pd.concat([df_word_corrpair, word_corrpair], axis=0, ignore_index=True)\n",
    "    \n",
    "#     # 시각화\n",
    "#     _, centrality = plot_networkx(df_subfreq, word_corrpair.iloc[:,1:], filter_criteria=None, \n",
    "#                                   plot=True, node_size='pagerank')\n",
    "#     ## 카테고리 추가\n",
    "#     centrality['year'] = str(year)\n",
    "#     centrality_years = pd.concat([centrality_years, centrality], axis=0, ignore_index=True)\n",
    "# G, centrality_total = plot_networkx(FREQ_USING, df_word_corrpair.iloc[:,1:], filter_criteria=None, \n",
    "#                                     plot=True, node_size='pagerank')\n",
    "    \n",
    "# 저장\n",
    "# save_name = os.path.join(os.getcwd(), 'Data', 'word_freq_soynlp.csv')\n",
    "# word_freq_soynlp.to_csv(save_name, index=False, encoding='utf-8-sig')\n",
    "# save_name = os.path.join(os.getcwd(), 'Data', 'wordadj_freq_soynlp.csv')\n",
    "# wordadj_freq_soynlp.to_csv(save_name, index=False, encoding='utf-8-sig')\n",
    "# save_name = os.path.join(os.getcwd(), 'Data', 'word_score_tfidf.csv')\n",
    "# word_score_tfidf.to_csv(save_name, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f61b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.780659Z",
     "start_time": "2023-10-15T12:18:41.780659Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sub_colnames = list(word_corr.columns[word_corr.sum()>=0.5])\n",
    "word_corr.loc[sub_colnames,:].loc[:,sub_colnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150e7fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.784645Z",
     "start_time": "2023-10-15T12:18:41.784645Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "mat_square = nx.from_numpy_array(word_corr.loc[sub_colnames,:].loc[:,sub_colnames].values)\n",
    "vis = Network(notebook=False, cdn_resources='local', height='1000px', width='1000px')\n",
    "vis.from_nx(mat_square)\n",
    "vis.show_buttons(filter_=True)\n",
    "vis.save_graph('nx.html')\n",
    "# vis.show('test1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0135c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.786639Z",
     "start_time": "2023-10-15T12:18:41.786639Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d0028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af62dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396041f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff344e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.787635Z",
     "start_time": "2023-10-15T12:18:41.787635Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/chord-diagrams-of-protein-interaction-networks-in-python-9589affc8b91\n",
    "    \n",
    "import nxviz as nv\n",
    "from nxviz import annotate, highlights\n",
    "from nxviz.plots import despine, rescale, respine\n",
    "\n",
    "def plot_nxviz(df_freq, df_pairweight, filter_criteria=None):\n",
    "    pass\n",
    "\n",
    "G, centrality_total = plot_networkx(FREQ_USING, df_word_corrpair, filter_criteria=None, plot=False)\n",
    "ax = nv.circos(G, group_by=\"group_node\", node_color_by=\"group_node\")\n",
    "annotate.circos_labels(G, group_by=\"group_node\", radius=np.pi)\n",
    "annotate.node_colormapping(G, color_by='group_node')\n",
    "# annotate.node_labels(G, group_by=\"group_node\", sort_by='score')\n",
    "annotate.circos_group(G, group_by=\"group_node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611b4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713aa0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.789629Z",
     "start_time": "2023-10-15T12:18:41.789629Z"
    }
   },
   "outputs": [],
   "source": [
    "# 과거 5년 이후 5년 시각화 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f4bad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.791623Z",
     "start_time": "2023-10-15T12:18:41.791623Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.from_pandas_edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ea390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525eb620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60edbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798115f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90df1d4c",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ee915",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T12:18:41.792619Z",
     "start_time": "2023-10-15T12:18:41.792619Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 데이터로딩\n",
    "# df_news = get_data_from_path(FOLDER_LOCATION)\n",
    "# df = df_news.copy()\n",
    "# ## 날짜 인식\n",
    "# df = df[df.Date != 'None'].reset_index().iloc[:,1:]\n",
    "# df.Date = pd.to_datetime(df.Date)\n",
    "# ## 중복 처리\n",
    "# df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "# ## 전처리\n",
    "# df.Category = df.Category.apply(lambda x: text_preprocessor(x, del_bracket_content=False))\n",
    "# df.Title = df.Title.apply(lambda x: text_preprocessor(x, del_bracket_content=True))\n",
    "# df.Content = df.Content.apply(lambda x: text_preprocessor(x, del_bracket_content=False))\n",
    "# display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6259d82e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8131b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497942ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce8135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37657a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
