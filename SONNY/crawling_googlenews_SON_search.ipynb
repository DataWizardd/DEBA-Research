{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31503ab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T14:44:17.273884Z",
     "start_time": "2023-09-21T14:44:13.494278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1c661f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T14:44:20.302695Z",
     "start_time": "2023-09-21T14:44:17.275873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: BeautifulSoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b661c310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T14:44:33.374788Z",
     "start_time": "2023-09-21T14:44:29.516929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "# warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "#크롤링\n",
    "import csv, json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a8f39b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T14:51:06.046128Z",
     "start_time": "2023-09-21T14:45:27.661739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: 고령화\n",
      "Crawling is complete. The data is saved in C:\\DATA\\DEBA\\SON\\crawling_google_search_SON.csv\n",
      "                                               links  \\\n",
      "0  https://www.hani.co.kr/arti/economy/economy_ge...   \n",
      "1  https://www.shinailbo.co.kr/news/articleView.h...   \n",
      "2        https://www.sedaily.com/NewsView/29S7E3M011   \n",
      "3           http://industryjournal.co.kr/news/233485   \n",
      "4  https://www.hani.co.kr/arti/economy/economy_ge...   \n",
      "\n",
      "                                      title  \\\n",
      "0                  '고령사회' 진입한 한국…일본보다 7년 빨라   \n",
      "1         2050년 취업자 평균연령 '53.7세'…저출산·고령화 심각   \n",
      "2  극심한 고령화에…취업자 평균연령, 47세→2050년 54세로 - 서울경제   \n",
      "3  [카드뉴스] ‘고령화’된 일터, 2050 취업자 평균연령 53.7세 예측   \n",
      "4     2026년 가계저축률 마이너스 전환…고령화 영향 : 경제일반 ...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  저출산·고령화가 심화됨에 따라, 지난해 우리나라는 노인이 전체 인구의 14%를 웃도...   \n",
      "1  저출산·고령화 현상이 심화된 탓으로 국가경쟁력 유지·강화를 위해선 정책마련이 필요하...   \n",
      "2  저출산·고령화 심화로 2050년 국내 취업자 평균 연령이 약 54세까지 높아질 것이...   \n",
      "3  대한상공회의소 SGI(지속성장이니셔티브)는 최근 발표한 '부문별 취업자의 연령분포 ...   \n",
      "4  ... 고령화 연구보고서 발표 고령인구 1%P상승→저축률 1.08%P 하락 75 ....   \n",
      "\n",
      "                                            contents          date   press  \n",
      "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n페이스북\\n\\n\\n트위터\\n\\n\\n카카오\\n\\n\\n...  2018. 8. 27.     한겨레  \n",
      "1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'의류‧목재‧섬유' 저위기술 산업, 절반 5...  2023. 7. 20.    신아일보  \n",
      "2  \\n   viewer  한 시민이 지난달 19일 서울 마포구 서울서부고용복지플러스센...  2023. 7. 20.    서울경제  \n",
      "3          고령취업자 저위기술·저부가가치 산업 위주… 향후 고위기술 인력난 심화 우려  2023. 7. 26.  산업종합저널  \n",
      "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n페이스북\\n\\n\\n트위터\\n\\n\\n카카오\\n\\n\\n...   2017. 8. 2.     한겨레  \n"
     ]
    }
   ],
   "source": [
    "now_date = datetime.now()\n",
    "\n",
    "# 날짜정의\n",
    "start_date = datetime(2013, 1, 1)\n",
    "end_date = now_date\n",
    "cd_min = start_date.strftime('%m/%d/%Y')\n",
    "cd_max = end_date\n",
    "\n",
    "\n",
    "#article-view-content-div > p:nth-child(13)\n",
    "#article-view-content-div > p:nth-child(47)\n",
    "def google_news_data(query, save_path, num_pages=30):\n",
    "    google_news = []\n",
    "    class_list = [\n",
    "    'article-body',\n",
    "    'view_con cf',\n",
    "    'article_view',\n",
    "    'article',\n",
    "    'article_cont',\n",
    "    'content01 scroll-article-zone01',\n",
    "    'float-center max-width-1080',\n",
    "    'user-snb-wrapper',\n",
    "    'user-content',\n",
    "    'articleWrap',\n",
    "    'article | ',\n",
    "    'bbc-19j92fr ebmt73l0',\n",
    "    'td_sub_read_contents',\n",
    "    'cnt_view news_body_area',\n",
    "    'article | '\n",
    "        #article-view-content-div > p:nth-child(4)\n",
    "    ]\n",
    "    \n",
    "    for page in range(num_pages):\n",
    "        # Calculate the start parameter for pagination\n",
    "        start_param = page * 10\n",
    "        \n",
    "        # Construct the URL for the current page\n",
    "        url = f\"https://www.google.com/search?q={query}&tbm=nws&tbs=cdr:1,cd_min:{cd_min},cd_max:{cd_max}&start={start_param}\"\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers,verify = False)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        for con in soup.select('div.SoaBEf'):\n",
    "            news_dict = {\n",
    "                'links': con.find('a')['href'],\n",
    "                'title': con.select_one('div.MBeuO').get_text(),\n",
    "                'summary' : con.select_one('.GI74Re').get_text(),\n",
    "                'contents': '',  # 빈 문자열로 초기화\n",
    "                'date': con.select_one('.LfVVr').get_text(),\n",
    "                'press': con.select_one('.NUnG9d span').get_text(),\n",
    "            }\n",
    "\n",
    "            # 형식별로 처리\n",
    "            for i in range(60):  \n",
    "                if f'{i}시간 전' in news_dict['date']:\n",
    "                    news_dict['date'] = now_date.strftime('%Y. %m. %d.')  \n",
    "                elif f'{i}분 전' in news_dict['date']:\n",
    "                    news_dict['date'] = now_date.strftime('%Y. %m. %d.')  \n",
    "                elif f'{i}일 전' in news_dict['date']:\n",
    "                    news_dict['date'] = (now_date - timedelta(days=i)).strftime('%Y. %m. %d.')\n",
    "                elif f'{i}주 전' in news_dict['date']:\n",
    "                    news_dict['date'] = (now_date - timedelta(weeks=i)).strftime('%Y. %m. %d.')\n",
    "                elif '1개월 전' in news_dict['date']:\n",
    "                    news_dict['date'] = (now_date - timedelta(weeks=4)).strftime('%Y. %m. %d.')  # 수정\n",
    "\n",
    "            # 각 뉴스 기사의 링크로 이동하여 본문 스크래핑\n",
    "            news_url = news_dict['links']\n",
    "            news_response = requests.get(news_url, headers=headers, verify = False)\n",
    "            news_response.encoding = 'utf-8'\n",
    "            news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "            \n",
    "            \n",
    "            news_content = news_soup.find('div', {'class': class_list})\n",
    "            \n",
    "            if news_content:\n",
    "                news_dict['contents'] = news_content.get_text()  \n",
    "                \n",
    "                \n",
    "            # 지정된 CSS 선택자를 사용하여 내용을 찾습니다.\n",
    "            for n in range(1, 100):  # 1부터 100까지의 값을 확인합니다.\n",
    "                news_content = news_soup.select_one(f'div > p:nth-child({n})')\n",
    "\n",
    "                if news_content:\n",
    "                    content_text = news_content.get_text()\n",
    "                    news_dict['contents'] += content_text  # 기존 내용에 추가합니다.\n",
    "                    \n",
    "            for n in range(1, 100):  # 1부터 100까지의 값을 확인합니다.\n",
    "                news_content = news_soup.select_one(f'div > br:nth-child({n})')\n",
    "\n",
    "                if news_content:\n",
    "                    content_text = news_content.get_text()\n",
    "                    news_dict['contents'] += content_text  # 기존 내용에 추가합니다.\n",
    "\n",
    "            google_news.append(news_dict)\n",
    "\n",
    "    df = pd.DataFrame(google_news)\n",
    "    save_file_path = os.path.join(save_path, \"crawling_google_search_SON.csv\")\n",
    "    df.to_csv(save_file_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Crawling is complete. The data is saved in {save_file_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "query = input('Enter your search query: ')\n",
    "save_path = 'C:\\\\DATA\\\\DEBA\\\\SON'\n",
    "df = google_news_data(query, save_path, num_pages=30)  \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588605ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a3fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71423f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2191f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce2b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff78e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26c209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc272c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9fc38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
