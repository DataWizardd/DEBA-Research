{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31503ab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T11:48:41.548979Z",
     "start_time": "2023-09-22T11:48:38.215593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1c661f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T11:48:44.538626Z",
     "start_time": "2023-09-22T11:48:41.550133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: BeautifulSoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b661c310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T11:48:47.220270Z",
     "start_time": "2023-09-22T11:48:44.540626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "# warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "#크롤링\n",
    "import csv, json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71270f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T11:57:19.872176Z",
     "start_time": "2023-09-22T11:51:43.326904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: 고령화\n",
      "Crawling is complete. The data is saved in C:\\DATA\\DEBA\\SON\\crawling_google_search_SON.csv\n",
      "                                               links  \\\n",
      "0  https://www.hani.co.kr/arti/economy/economy_ge...   \n",
      "1  https://www.shinailbo.co.kr/news/articleView.h...   \n",
      "2        https://www.sedaily.com/NewsView/29S7E3M011   \n",
      "3           http://industryjournal.co.kr/news/233485   \n",
      "4  https://www.hani.co.kr/arti/economy/economy_ge...   \n",
      "\n",
      "                                      title  \\\n",
      "0                  '고령사회' 진입한 한국…일본보다 7년 빨라   \n",
      "1         2050년 취업자 평균연령 '53.7세'…저출산·고령화 심각   \n",
      "2  극심한 고령화에…취업자 평균연령, 47세→2050년 54세로 - 서울경제   \n",
      "3  [카드뉴스] ‘고령화’된 일터, 2050 취업자 평균연령 53.7세 예측   \n",
      "4     2026년 가계저축률 마이너스 전환…고령화 영향 : 경제일반 ...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  저출산·고령화가 심화됨에 따라, 지난해 우리나라는 노인이 전체 인구의 14%를 웃도...   \n",
      "1  저출산·고령화 현상이 심화된 탓으로 국가경쟁력 유지·강화를 위해선 정책마련이 필요하...   \n",
      "2  저출산·고령화 심화로 2050년 국내 취업자 평균 연령이 약 54세까지 높아질 것이...   \n",
      "3  대한상공회의소 SGI(지속성장이니셔티브)는 최근 발표한 '부문별 취업자의 연령분포 ...   \n",
      "4  ... 고령화 연구보고서 발표 고령인구 1%P상승→저축률 1.08%P 하락 75 ....   \n",
      "\n",
      "                                            contents          date   press  \n",
      "0  \\n\\n\\n\\n\\n\\n\\n* 이미지를 누르면 크게 볼 수 있습니다.\\n\\n\\r\\n ...  2018. 8. 27.     한겨레  \n",
      "1  2050년 국내 취업자 평균연령이 약 54세까지 높아질 전망이다. 저출산·고령화 현...  2023. 7. 20.    신아일보  \n",
      "2  \\n\\n구독\\n\\n\\n\\n\\n\\n\\n\\r\\n                      ...  2023. 7. 20.    서울경제  \n",
      "3  \\n\\n산업전시회 일정\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  2023. 7. 26.  산업종합저널  \n",
      "4  \\n\\n\\n\\n\\n서울 종로구 탑골 공원에서 8일 오후 어버이날을 맞아 어르신들이 ...   2017. 8. 2.     한겨레  \n"
     ]
    }
   ],
   "source": [
    "now_date = datetime.now()\n",
    "\n",
    "# 날짜정의\n",
    "start_date = datetime(2013, 1, 1)\n",
    "end_date = now_date\n",
    "cd_min = start_date.strftime('%m/%d/%Y')\n",
    "cd_max = end_date\n",
    "\n",
    "def google_news_data(query, save_path, num_pages=30):\n",
    "    google_news = []\n",
    "    class_list = [\n",
    "        'text',\n",
    "        'article'\n",
    "        'article-body',\n",
    "        'view-article view-box'\n",
    "        'article_view',\n",
    "        'box'\n",
    "        \n",
    "    ]\n",
    "    article_list=[\n",
    "        'story-news article',\n",
    "        'article-veiw-body view-page font-size17'\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    section_list=[\n",
    "        'article-body',\n",
    "        'articleBody sa_area'\n",
    "    ]\n",
    "    \n",
    "    for page in range(num_pages):\n",
    "        # Calculate the start parameter for pagination\n",
    "        start_param = page * 10\n",
    "        \n",
    "        # Construct the URL for the current page\n",
    "        url = f\"https://www.google.com/search?q={query}&tbm=nws&tbs=cdr:1,cd_min:{cd_min},cd_max:{cd_max}&start={start_param}\"\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers,verify = False)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        for con in soup.select('div.SoaBEf'):\n",
    "            news_dict = {\n",
    "                'links': con.find('a')['href'],\n",
    "                'title': con.select_one('div.MBeuO').get_text(),\n",
    "                'summary' : con.select_one('.GI74Re').get_text(),\n",
    "                'contents': '',  # 빈 문자열로 초기화\n",
    "                'date': con.select_one('.LfVVr').get_text(),\n",
    "                'press': con.select_one('.NUnG9d span').get_text(),\n",
    "            }\n",
    "\n",
    "            # 형식별로 처리\n",
    "            for i in range(60):  \n",
    "                if f'{i}시간 전' in news_dict['date']:\n",
    "                    news_dict['date'] = now_date.strftime('%Y. %m. %d.')  \n",
    "                elif f'{i}분 전' in news_dict['date']:\n",
    "                    news_dict['date'] = now_date.strftime('%Y. %m. %d.')  \n",
    "                elif f'{i}일 전' in news_dict['date']:\n",
    "                    news_dict['date'] = (now_date - timedelta(days=i)).strftime('%Y. %m. %d.')\n",
    "                elif f'{i}주 전' in news_dict['date']:\n",
    "                    news_dict['date'] = (now_date - timedelta(weeks=i)).strftime('%Y. %m. %d.')\n",
    "                elif '1개월 전' in news_dict['date']:\n",
    "                    news_dict['date'] = (now_date - timedelta(weeks=4)).strftime('%Y. %m. %d.')  # 수정\n",
    "\n",
    "            # 각 뉴스 기사의 링크로 이동하여 본문 스크래핑\n",
    "            news_url = news_dict['links']\n",
    "            news_response = requests.get(news_url, headers=headers, verify = False)\n",
    "            news_response.encoding = 'utf-8'\n",
    "            news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "            \n",
    "            \n",
    "            news_content = news_soup.find('div', {'class': class_list})\n",
    "            \n",
    "            if news_content:\n",
    "                news_dict['contents'] = news_content.get_text()  \n",
    "            \n",
    "            news_content = news_soup.find('article', {'class': article_list})\n",
    "            \n",
    "            if news_content:\n",
    "                news_dict['contents'] = news_content.get_text() \n",
    "            \n",
    "            news_content = news_soup.find('section', {'class': section_list})\n",
    "            \n",
    "            if news_content:\n",
    "                news_dict['contents'] = news_content.get_text() \n",
    "                \n",
    "            # 지정된 CSS 선택자를 사용하여 내용을 찾습니다.\n",
    "            for n in range(1, 100):  # 1부터 100까지의 값을 확인합니다.\n",
    "                news_content = news_soup.select_one(f'div > p:nth-child({n})')\n",
    "\n",
    "                if news_content:\n",
    "                    content_text = news_content.get_text()\n",
    "                    news_dict['contents'] += content_text  # 기존 내용에 추가합니다.\n",
    "                    \n",
    "            for n in range(1, 100):  # 1부터 100까지의 값을 확인합니다.\n",
    "                news_content = news_soup.select_one(f'div > br:nth-child({n})')\n",
    "\n",
    "                if news_content:\n",
    "                    content_text = news_content.get_text()\n",
    "                    news_dict['contents'] += content_text  # 기존 내용에 추가합니다.\n",
    "\n",
    "            google_news.append(news_dict)\n",
    "\n",
    "    df = pd.DataFrame(google_news)\n",
    "    save_file_path = os.path.join(save_path, \"crawling_google_search_SON.csv\")\n",
    "    df.to_csv(save_file_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Crawling is complete. The data is saved in {save_file_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "query = input('Enter your search query: ')\n",
    "save_path = 'C:\\\\DATA\\\\DEBA\\\\SON'\n",
    "df = google_news_data(query, save_path, num_pages=30)  \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "588605ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T11:50:35.854148Z",
     "start_time": "2023-09-22T11:48:47.225271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: 고령화\n",
      "Crawling is complete. The data is saved in C:\\DATA\\DEBA\\SON\\crawling_google_search_SON.csv\n",
      "                                               links  \\\n",
      "0  https://news.einfomax.co.kr/news/articleView.h...   \n",
      "1  https://www.edaily.co.kr/news/read?newsId=0142...   \n",
      "2  https://m.dnews.co.kr/m_home/view.jsp?idxno=20...   \n",
      "3    https://news.kbs.co.kr/news/view.do?ncd=7758684   \n",
      "4  http://www.pharmnews.com/news/articleView.html...   \n",
      "\n",
      "                                      title  \\\n",
      "0                한은 \"독일경제 역성장할듯…中의존도·고령화 탓\"   \n",
      "1          中의존도 높고 인구는 고령화되고, 독일의 몰락…韓 시사점은   \n",
      "2              “인구 고령화…고숙련 노동자 활용ㆍ산업 전환 필요”   \n",
      "3                    늙어가는 울산…고령화에도 의료 수준 열악   \n",
      "4  그랜드 제네레이션 시대...‘위기와 기회’ 초고령화를 보는 두 가지 시선   \n",
      "\n",
      "                                             summary  \\\n",
      "0  ... man of Europe)로 전락할 수 있다는 우려가 있다\"면서 이같이 전망...   \n",
      "1  내연기관차 비중이 높은 제조업 국가인데다 중국 의존도가 높고 노동 인구가 고령화되고...   \n",
      "2  [대한경제=이종무 기자] 우리나라 제조업의 중국 의존도가 높고 인구 고령화에 따른 ...   \n",
      "3  오는 2050년에는 울산의 65세 이상 고령인구 비중이 전체 인구의 40%를 넘어선...   \n",
      "4  한 걸음 앞으로 다가온 초고령화 사회의 '위기와 기회'에 대해 숙명여자대학교 경영학...   \n",
      "\n",
      "                                            contents           date   press  \n",
      "0  \\n\\n\\n(서울=연합인포맥스) 김정현 기자 = 독일 경제가 금리인상 파급효과와 중...  2023. 09. 01.  연합인포맥스  \n",
      "1                         \\n\\n\\n\\n\\n최정희 기자\\n\\n기자구독\\n  2023. 09. 01.    이데일리  \n",
      "2                                                     2023. 09. 01.    대한경제  \n",
      "3  \\n\\n\\n\\n\\n\\n\\n동영상 고정 취소\\n\\nLoading the player....  2023. 09. 01.  KBS 뉴스  \n",
      "4  \\n\\n\\n\\n\\n숙명여자대학교 경영학부 서용구 교수\\n\\n\\n[팜뉴스=김태일 기자...  2023. 09. 01.     팜뉴스  \n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "# import os\n",
    "\n",
    "# def parse_date(date_str):\n",
    "#     now_date = datetime.now()\n",
    "    \n",
    "#     for i in range(60):\n",
    "#         if f'{i}시간 전' in date_str or f'{i}분 전' in date_str:\n",
    "#             return now_date.strftime('%Y. %m. %d.')\n",
    "#         elif f'{i}일 전' in date_str:\n",
    "#             return (now_date - timedelta(days=i)).strftime('%Y. %m. %d.')\n",
    "#         elif f'{i}주 전' in date_str:\n",
    "#             return (now_date - timedelta(weeks=i)).strftime('%Y. %m. %d.')\n",
    "    \n",
    "#     if '1개월 전' in date_str:\n",
    "#         return (now_date - timedelta(weeks=4)).strftime('%Y. %m. %d.')  # 수정\n",
    "    \n",
    "#     return date_str\n",
    "\n",
    "# def scrape_news_content(url, headers):\n",
    "#     response = requests.get(url, headers=headers, verify=False)\n",
    "#     response.encoding = 'utf-8'\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "#     class_list = [\n",
    "#         'text',\n",
    "#         'article',\n",
    "#         'article-body',\n",
    "#         'view-article',\n",
    "#         'view-box',\n",
    "#         'article_view',\n",
    "#         'box'\n",
    "#     ]\n",
    "    \n",
    "#     article_list = [\n",
    "#         'story-news article',\n",
    "#         'article-veiw-body view-page font-size17'\n",
    "#     ]\n",
    "    \n",
    "#     section_list = [\n",
    "#         'article-body',\n",
    "#         'articleBody sa_area'\n",
    "#     ]\n",
    "    \n",
    "#     for class_name in class_list + article_list + section_list:\n",
    "#         news_content = soup.find(['div', 'article', 'section'], {'class': class_name})\n",
    "#         if news_content:\n",
    "#             return news_content.get_text()\n",
    "    \n",
    "#     content_text = ''\n",
    "#     for n in range(1, 100):\n",
    "#         news_content = soup.select_one(f'div > p:nth-child({n})')\n",
    "#         if news_content:\n",
    "#             content_text += news_content.get_text()\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     for n in range(1, 100):\n",
    "#         news_content = soup.select_one(f'div > br:nth-child({n})')\n",
    "#         if news_content:\n",
    "#             content_text += news_content.get_text()\n",
    "#         else:\n",
    "#             break\n",
    "            \n",
    "            \n",
    "#     return content_text\n",
    "\n",
    "# def google_news_data(query, save_path, num_pages=30):\n",
    "#     google_news = []\n",
    "    \n",
    "#     for page in range(num_pages):\n",
    "#         # Calculate the start parameter for pagination\n",
    "#         start_param = page * 10\n",
    "        \n",
    "#         # Construct the URL for the current page\n",
    "#         url = f\"https://www.google.com/search?q={query}&tbm=nws&tbs=cdr:1,cd_min:{cd_min},cd_max:{cd_max}&start={start_param}\"\n",
    "        \n",
    "#         headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36'\n",
    "#         }\n",
    "\n",
    "#         response = requests.get(url, headers=headers, verify=False)\n",
    "#         response.encoding = 'utf-8'\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#         for con in soup.select('div.SoaBEf'):\n",
    "#             news_dict = {\n",
    "#                 'links': con.find('a')['href'],\n",
    "#                 'title': con.select_one('div.MBeuO').get_text(),\n",
    "#                 'summary': con.select_one('.GI74Re').get_text(),\n",
    "#                 'contents': '',  # 빈 문자열로 초기화\n",
    "#                 'date': parse_date(con.select_one('.LfVVr').get_text()),\n",
    "#                 'press': con.select_one('.NUnG9d span').get_text(),\n",
    "#             }\n",
    "\n",
    "#             news_url = news_dict['links']\n",
    "#             news_dict['contents'] = scrape_news_content(news_url, headers)\n",
    "            \n",
    "#             google_news.append(news_dict)\n",
    "\n",
    "#     df = pd.DataFrame(google_news)\n",
    "#     save_file_path = os.path.join(save_path, \"crawling_google_search_SON.csv\")\n",
    "#     df.to_csv(save_file_path, index=False, encoding='utf-8-sig')\n",
    "#     print(f\"Crawling is complete. The data is saved in {save_file_path}\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "# query = input('Enter your search query: ')\n",
    "# now_date = datetime.now()\n",
    "# cd_min = (now_date - timedelta(weeks=4)).strftime('%m/%d/%Y')\n",
    "# cd_max = now_date\n",
    "# save_path = 'C:\\\\DATA\\\\DEBA\\\\SON'\n",
    "# df = google_news_data(query, save_path, num_pages=30)\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a3fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71423f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2191f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce2b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff78e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26c209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc272c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9fc38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
