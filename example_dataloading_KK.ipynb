{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f30a5c",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cfa0985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T15:16:47.269286Z",
     "start_time": "2023-11-05T15:16:43.807645Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 00:16:45,168\tINFO worker.py:1476 -- Calling ray.init() again after it has already been called.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2023-11-06 00:16:46,829\tINFO worker.py:1476 -- Calling ray.init() again after it has already been called.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "# warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# System related and data input controls\n",
    "import os\n",
    "\n",
    "# Auto reload of library\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from preprocessing_text_KK import *\n",
    "\n",
    "def get_data_from_path(folder_location, folder_name=False, concat_axis='row'):\n",
    "    # path_folder 하위의 모든 폴더위치와 내부 file 출력\n",
    "    df = pd.DataFrame()\n",
    "    print('Getting data from', len(os.listdir(folder_location)), 'folders...')\n",
    "    for (path, dir, files) in os.walk(folder_location):\n",
    "#         print(path)\n",
    "        for file in tqdm(files):\n",
    "            path_file = os.path.join(path, file)\n",
    "\n",
    "            ## 데이터 로딩\n",
    "            if path_file[-4:] == 'xlsx':\n",
    "                df_sub = pd.read_excel(path_file)\n",
    "            elif path_file[-3:] == 'csv':\n",
    "                df_sub = pd.read_csv(path_file)\n",
    "\n",
    "            ## 키워드 태깅 여부\n",
    "            if folder_name:\n",
    "                df_sub['Folder_Name'] = os.path.basename(path)\n",
    "            \n",
    "            ## 정리\n",
    "            if concat_axis == 'col':\n",
    "                df = pd.concat([df, df_sub], axis=1)\n",
    "            elif concat_axis == 'row':\n",
    "                df = pd.concat([df, df_sub], axis=0)\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86745f91",
   "metadata": {},
   "source": [
    "# BigKinds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a53ffb",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELETE_KEYWORD = ['100세', '거주환경']\n",
    "CATEGORY_BK = ['경제', '사회', '문화', '국제']\n",
    "CATEGORY_BK_Sub = ['경제>경제일반', '경제>국제경제', '경제>취업_창업',\n",
    "                   '사회>노동_복지', '사회>사건_사고', '사회>사회일반', '사회>여성', '사회>장애인', '사회>의료_건강',\n",
    "                   '문화>미술_건축', '문화>요리_여행', '문화>출판',\n",
    "                   '국제>중국', '국제>유럽_EU', '국제>일본', '국제>미국_북미', '국제>중동_아프리카',\n",
    "                   '국제>아시아', '국제>중남미', '국제>국제일반', '국제>러시아']\n",
    "COLNAME_CATEGORY = '일자'\n",
    "COLNAME_MINING = '제목'\n",
    "### github에 업로드 되지 않도록 다른 폴더를 지정\n",
    "# 아래 예시는 내PC 바탕화면 Data 폴더를 지정\n",
    "# SAVE_LOCATION = r'C:\\Users\\user\\Desktop\\Data'    # inu\n",
    "SAVE_LOCATION = r'C:\\Users\\KK\\Desktop\\Data'    # home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333cf75",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3f0483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T16:23:38.004014Z",
     "start_time": "2023-11-03T16:23:38.000242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from 39 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  2.00s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:41<00:00,  3.78s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.02s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.96s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.86s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.96s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.01s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.57s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.89s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.00s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.64s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:51<00:00,  4.71s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:20<00:00,  6.76s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.59s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.54s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.57s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.52s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:24<00:00,  4.99s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 509413/509413 [04:20<00:00, 1955.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from 51 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 51/51 [00:00<00:00, 109.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터로딩\n",
    "df_news = get_data_from_path(os.path.join(os.getcwd(), 'Data', 'BigKinds'), folder_name=True)\n",
    "# 전처리\n",
    "## 중복 처리\n",
    "df_news.drop_duplicates(subset=['뉴스 식별자', '언론사', COLNAME_MINING], inplace=True, ignore_index=True)\n",
    "## 불필요 변수 삭제\n",
    "colname_delete = ['뉴스 식별자', '인물', '위치', '기관', '기고자', '통합 분류2', '통합 분류3', \n",
    "                  '사건/사고 분류1', '사건/사고 분류2', '사건/사고 분류3',\n",
    "                  '키워드', '특성추출(가중치순 상위 50개)', 'URL', '분석제외 여부']\n",
    "df_news = df_news[[col for col in df_news.columns if col not in colname_delete]]\n",
    "## 카테고리 필터\n",
    "category_filter = [each for each in df_news['통합 분류1'].unique() if each.split('>')[0] in CATEGORY_BK]\n",
    "df_news = df_news[df_news['통합 분류1'].apply(lambda x: x in category_filter)].reset_index().iloc[:,1:]\n",
    "df_news['Category'] = df_news['통합 분류1'].apply(lambda x: x.split('>')[0])\n",
    "## 전처리\n",
    "df_news[COLNAME_MINING] = df_news[COLNAME_MINING].progress_apply(lambda x: text_preprocessor(x, del_number=False, del_bracket_content=False))\n",
    "## 결측치 및 빈문자 제거\n",
    "df_news = df_news[~df_news[COLNAME_MINING].isnull()].reset_index().iloc[:,1:].copy()\n",
    "df_news = df_news[df_news[COLNAME_MINING].str.len() != 0].reset_index().iloc[:,1:]\n",
    "\n",
    "# 날짜 변환\n",
    "## 연도 반영\n",
    "df_news[COLNAME_CATEGORY+'_Year'] = pd.to_datetime(df_news[COLNAME_CATEGORY].astype(str)).dt.year\n",
    "## 연도+월 반영\n",
    "df_news[COLNAME_CATEGORY+'_YearMonth'] = pd.to_datetime(df_news[COLNAME_CATEGORY].astype(str)).dt.strftime('%Y-%m')\n",
    "## 연도그룹 반영\n",
    "df_news[COLNAME_CATEGORY+'_Era'] = df_news[COLNAME_CATEGORY].apply(lambda x: '2013 ~ 2017' if str(x)[:4] in ['2013', '2014', '2015', '2016', '2017']\n",
    "                                                                                            else '2018 ~ 2023')\n",
    "\n",
    "# 나이대 변수 추가\n",
    "df_news['Age'] = df_news['제목'].apply(lambda x: 20 if re.search(' 20대', x) != None else\n",
    "                                                 (30 if re.search(' 30대', x) != None else\n",
    "                                                 (40 if re.search(' 40대', x) != None else\n",
    "                                                 (50 if re.search(' 50대', x) != None else\n",
    "                                                 (60 if re.search(' 60대', x) != None else\n",
    "                                                 (70 if re.search(' 70대', x) != None else\n",
    "                                                 (80 if re.search(' 80대', x) != None else\n",
    "                                                 (90 if re.search(' 90대', x) != None else 0))))))))\n",
    "\n",
    "# 긍부정 라벨 추가\n",
    "df_news_sentiment = get_data_from_path(os.path.join(os.getcwd(), 'Data', 'Sentiment'), folder_name=False)\n",
    "df_news_sentiment = df_news_sentiment.sort_values(by='Unnamed: 0').reset_index().iloc[:,2:]\n",
    "df_news_sentiment.columns = ['Sentiment']\n",
    "df_news_sentiment['Sentiment'] = df_news_sentiment.Sentiment.apply(lambda x: 'Positive' if x==2 else 'Negative')\n",
    "df_news_sentiment['Positive'] = df_news_sentiment.Sentiment.apply(lambda x: 1 if x=='Positive' else 0)\n",
    "df_news_sentiment['Negative'] = df_news_sentiment.Sentiment.apply(lambda x: -1 if x=='Negative' else 0)\n",
    "df_news = pd.concat([df_news, df_news_sentiment], axis=1)\n",
    "## 최대 중복 처리\n",
    "df_news.drop_duplicates(subset=['언론사', COLNAME_MINING], inplace=True, ignore_index=True)\n",
    "df_news.drop_duplicates(subset=[COLNAME_MINING], inplace=True, ignore_index=True)\n",
    "\n",
    "# 저장\n",
    "df_news.to_csv(os.path.join(SAVE_LOCATION, 'df_news_bigkinds.csv'), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80eebe40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T14:25:44.142203Z",
     "start_time": "2023-11-05T14:25:38.632518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  Category\n",
      "경제    182058\n",
      "사회    175598\n",
      "문화     96737\n",
      "국제     37249\n",
      "Name: count, dtype: int64\n",
      "Category:  Category\n",
      "사회    143555\n",
      "경제     46072\n",
      "국제     35535\n",
      "문화     21051\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 필터링한 결과를 최종적으로 사용\n",
    "df_news = pd.read_csv(os.path.join(SAVE_LOCATION, 'df_news_bigkinds.csv'))\n",
    "print('Category: ', df_news.Category.value_counts())\n",
    "keyword_filter = [each for each in df_news['Folder_Name'].unique() if each not in DELETE_KEYWORD]\n",
    "df_news = df_news[df_news['Folder_Name'].apply(lambda x: x in keyword_filter)].reset_index().iloc[:,1:]\n",
    "category_filter = [each for each in df_news['통합 분류1'].unique() if each in CATEGORY_BK_Sub]\n",
    "df_news = df_news[df_news['통합 분류1'].apply(lambda x: x in category_filter)].reset_index().iloc[:,1:]\n",
    "print('Category: ', df_news.Category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a08332",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ea419f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T14:12:37.046370Z",
     "start_time": "2023-11-05T14:12:37.043055Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 연도데이터 기준 전처리\n",
    "# wf_year_soy, waf_year_soy, \\\n",
    "# wf_year_tf, waf_year_tf, \\\n",
    "# wf_year_kb, waf_year_kb = preprocessing_wordfreq(df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year',\n",
    "#                                                  num_showkeyword=10, save_local=True, save_name='wordfreq_year')\n",
    "\n",
    "# # 연도그룹데이터 기준 전처리\n",
    "# wf_era_soy, waf_era_soy, \\\n",
    "# wf_era_tf, waf_era_tf, \\\n",
    "# wf_era_kb, waf_era_kb = preprocessing_wordfreq(df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era',\n",
    "#                                                num_showkeyword=10, save_local=True, save_name='wordfreq_era')\n",
    "\n",
    "# # 연도감성데이터 기준 전처리\n",
    "# wf_senti_soy, waf_senti_soy, \\\n",
    "# wf_senti_tf, waf_senti_tf, \\\n",
    "# wf_senti_kb, waf_senti_kb = preprocessing_wordfreq(df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', \n",
    "#                                                    num_showkeyword=10, save_local=True, save_name='wordfreq_senti')\n",
    "\n",
    "# # 나이데이터 기준 전처리\n",
    "# wf_age_soy, waf_age_soy, \\\n",
    "# wf_age_tf, waf_age_tf, \\\n",
    "# wf_age_kb, waf_age_kb = preprocessing_wordfreq(df_news[df_news.Age != 0].reset_index().iloc[:,1:], \n",
    "#                                                colname_target=COLNAME_MINING, colname_category='Age',\n",
    "#                                                num_showkeyword=10, save_local=True, save_name='wordfreq_age')\n",
    "\n",
    "# # 나이+감성데이터 기준 전처리\n",
    "# ## 부정\n",
    "# SENTIMENT = 'Negative'\n",
    "# wf_age_soy, waf_age_soy, \\\n",
    "# wf_age_tf, waf_age_tf, \\\n",
    "# wf_age_kb, waf_age_kb = preprocessing_wordfreq(df_news[(df_news.Age != 0) & (df_news.Sentiment == SENTIMENT)].reset_index().iloc[:,1:], \n",
    "#                                                colname_target=COLNAME_MINING, colname_category='Age',\n",
    "#                                                num_showkeyword=10, save_local=False, save_name='wordfreq_age')\n",
    "# wf_age_soy['sentiment'], waf_age_soy['sentiment'] = SENTIMENT, SENTIMENT\n",
    "# wf_age_tf['sentiment'], waf_age_tf['sentiment'] = SENTIMENT, SENTIMENT\n",
    "# wf_age_kb['sentiment'], waf_age_kb['sentiment'] = SENTIMENT, SENTIMENT\n",
    "# ## 긍정\n",
    "# SENTIMENT = 'Positive'\n",
    "# wf_temp_soy, waf_temp_soy, \\\n",
    "# wf_temp_tf, waf_temp_tf, \\\n",
    "# wf_temp_kb, waf_temp_kb = preprocessing_wordfreq(df_news[(df_news.Age != 0) & (df_news.Sentiment == SENTIMENT)].reset_index().iloc[:,1:], \n",
    "#                                                colname_target=COLNAME_MINING, colname_category='Age',\n",
    "#                                                num_showkeyword=10, save_local=False, save_name='wordfreq_temp')\n",
    "# wf_temp_soy['sentiment'], waf_temp_soy['sentiment'] = SENTIMENT, SENTIMENT\n",
    "# wf_temp_tf['sentiment'], waf_temp_tf['sentiment'] = SENTIMENT, SENTIMENT\n",
    "# wf_temp_kb['sentiment'], waf_temp_kb['sentiment'] = SENTIMENT, SENTIMENT\n",
    "# ## 정리\n",
    "# wf_age_soy = pd.concat([wf_age_soy, wf_temp_soy], axis=0)\n",
    "# waf_age_soy = pd.concat([waf_age_soy, waf_temp_soy], axis=0)\n",
    "# wf_age_tf = pd.concat([wf_age_tf, wf_temp_tf], axis=0)\n",
    "# waf_age_tf = pd.concat([waf_age_tf, waf_temp_tf], axis=0)\n",
    "# wf_age_kb = pd.concat([wf_age_kb, wf_temp_kb], axis=0)\n",
    "# waf_age_kb = pd.concat([waf_age_kb, waf_temp_kb], axis=0)\n",
    "# ## 저장\n",
    "# wf_age_soy.to_csv(os.path.join(os.getcwd(), 'Data', 'WordFreq', 'wordfreq_agesenti_soynlp.csv'), index=False, encoding='utf-8-sig')\n",
    "# waf_age_soy.to_csv(os.path.join(os.getcwd(), 'Data', 'WordFreq', 'wordfreq_agesenti_soynlpadj.csv'), index=False, encoding='utf-8-sig')\n",
    "# wf_age_tf.to_csv(os.path.join(os.getcwd(), 'Data', 'WordFreq', 'wordfreq_agesenti_tfidf.csv'), index=False, encoding='utf-8-sig')\n",
    "# waf_age_tf.to_csv(os.path.join(os.getcwd(), 'Data', 'WordFreq', 'wordfreq_agesenti_tfidfadj.csv'), index=False, encoding='utf-8-sig')\n",
    "# wf_age_kb.to_csv(os.path.join(os.getcwd(), 'Data', 'WordFreq', 'wordfreq_agesenti_keybert.csv'), index=False, encoding='utf-8-sig')\n",
    "# waf_age_kb.to_csv(os.path.join(os.getcwd(), 'Data', 'WordFreq', 'wordfreq_agesenti_keybertadj.csv'), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ac71f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T15:59:00.588303Z",
     "start_time": "2023-11-06T15:59:00.293070Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 불러오기\u001b[39;00m\n\u001b[0;32m      2\u001b[0m save_name_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordfreq_year_soynlp.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordfreq_year_soynlpadj.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordfreq_year_tfidf.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordfreq_year_tfidfadj.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordfreq_year_keybert.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordfreq_year_keybertadj.csv\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m save_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWordFreq\u001b[39m\u001b[38;5;124m'\u001b[39m, save_name_list[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      6\u001b[0m wf_year_soynlp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(save_name)\n\u001b[0;32m      7\u001b[0m save_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWordFreq\u001b[39m\u001b[38;5;124m'\u001b[39m, save_name_list[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# 불러오기\n",
    "save_name_list=['wordfreq_year_soynlp.csv', 'wordfreq_year_soynlpadj.csv', \n",
    "                'wordfreq_year_tfidf.csv', 'wordfreq_year_tfidfadj.csv',\n",
    "                'wordfreq_year_keybert.csv', 'wordfreq_year_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_year_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_year_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_year_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_year_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_year_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_year_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordfreq_era_soynlp.csv', 'wordfreq_era_soynlpadj.csv', \n",
    "                'wordfreq_era_tfidf.csv', 'wordfreq_era_tfidfadj.csv',\n",
    "                'wordfreq_era_keybert.csv', 'wordfreq_era_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_era_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_era_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_era_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_era_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordfreq_senti_soynlp.csv', 'wordfreq_senti_soynlpadj.csv', \n",
    "                'wordfreq_senti_tfidf.csv', 'wordfreq_senti_tfidfadj.csv',\n",
    "                'wordfreq_senti_keybert.csv', 'wordfreq_senti_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_senti_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_senti_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_senti_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_senti_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_senti_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_senti_keybert = pd.read_csv(save_name)\n",
    "## 중복처리\n",
    "wf_senti_soynlp = wf_senti_soynlp.sort_values(by='score', ascending=False).drop_duplicates(['word'], keep='first')\n",
    "waf_senti_soynlp = waf_senti_soynlp.sort_values(by='score', ascending=False).drop_duplicates(['word'], keep='first')\n",
    "wf_senti_tfidf = wf_senti_tfidf.sort_values(by='score', ascending=False).drop_duplicates(['word'], keep='first')\n",
    "waf_senti_tfidf = waf_senti_tfidf.sort_values(by='score', ascending=False).drop_duplicates(['word'], keep='first')\n",
    "wf_senti_keybert = wf_senti_keybert.sort_values(by='score', ascending=False).drop_duplicates(['word'], keep='first')\n",
    "waf_senti_keybert = waf_senti_keybert.sort_values(by='score', ascending=False).drop_duplicates(['word'], keep='first')\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordfreq_age_soynlp.csv', 'wordfreq_age_soynlpadj.csv', \n",
    "                'wordfreq_age_tfidf.csv', 'wordfreq_age_tfidfadj.csv',\n",
    "                'wordfreq_age_keybert.csv', 'wordfreq_age_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_age_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_age_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_age_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_age_tfidf = pd.read_csv(save_name)\n",
    "# save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "# wf_age_keybert = pd.read_csv(save_name)\n",
    "# save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "# waf_age_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordfreq_agesenti_soynlp.csv', 'wordfreq_agesenti_soynlpadj.csv', \n",
    "                'wordfreq_agesenti_tfidf.csv', 'wordfreq_agesenti_tfidfadj.csv',\n",
    "                'wordfreq_agesenti_keybert.csv', 'wordfreq_agesenti_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_agesenti_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_agesenti_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_agesenti_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_agesenti_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_agesenti_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_agesenti_keybert = pd.read_csv(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669b8ca",
   "metadata": {},
   "source": [
    "## Word Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0080934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T14:30:08.984465Z",
     "start_time": "2023-11-05T14:30:08.553616Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 관련성 전처리\n",
    "# wf_yearcorr_soynlp = preprocessing_wordfreq_to_corr(wf_year_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_soynlp.csv')\n",
    "# waf_yearcorr_soynlp = preprocessing_wordfreq_to_corr(waf_year_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_soynlpadj.csv')\n",
    "# wf_yearcorr_tfidf = preprocessing_wordfreq_to_corr(wf_year_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_tfidf.csv')\n",
    "# waf_yearcorr_tfidf = preprocessing_wordfreq_to_corr(waf_year_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_tfidfadj.csv')\n",
    "# wf_yearcorr_keybert = preprocessing_wordfreq_to_corr(wf_year_keybert, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_keybert.csv')\n",
    "# waf_yearcorr_keybert = preprocessing_wordfreq_to_corr(waf_year_keybert, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_keybertadj.csv')\n",
    "\n",
    "# wf_eracorr_soynlp = preprocessing_wordfreq_to_corr(wf_era_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_soynlp.csv')\n",
    "# waf_eracorr_soynlp = preprocessing_wordfreq_to_corr(waf_era_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_soynlpadj.csv')\n",
    "# wf_eracorr_tfidf = preprocessing_wordfreq_to_corr(wf_era_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_tfidf.csv')\n",
    "# waf_eracorr_tfidf = preprocessing_wordfreq_to_corr(waf_era_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_tfidfadj.csv')\n",
    "# wf_eracorr_keybert = preprocessing_wordfreq_to_corr(wf_era_keybert.sort_values(by='score', ascending=False).iloc[:10000,:], \n",
    "#                                                     df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_keybert.csv')\n",
    "# waf_eracorr_keybert = preprocessing_wordfreq_to_corr(waf_era_keybert.sort_values(by='score', ascending=False).iloc[:10000,:], \n",
    "#                                                      df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_keybertadj.csv')\n",
    "\n",
    "# wf_senticorr_soynlp = preprocessing_wordfreq_to_corr(wf_senti_soynlp, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_soynlp.csv')\n",
    "# waf_senticorr_soynlp = preprocessing_wordfreq_to_corr(waf_senti_soynlp, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_soynlpadj.csv')\n",
    "# wf_senticorr_tfidf = preprocessing_wordfreq_to_corr(wf_senti_tfidf, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_tfidf.csv')\n",
    "# waf_senticorr_tfidf = preprocessing_wordfreq_to_corr(waf_senti_tfidf, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_tfidfadj.csv')\n",
    "# wf_senticorr_keybert = preprocessing_wordfreq_to_corr(wf_senti_keybert.sort_values(by='score', ascending=False).iloc[:10000,:],\n",
    "#                                                       df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_keybert.csv')\n",
    "# waf_senticorr_keybert = preprocessing_wordfreq_to_corr(waf_senti_keybert.sort_values(by='score', ascending=False).iloc[:10000,:], \n",
    "#                                                        df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_keybertadj.csv')\n",
    "\n",
    "# wf_agecorr_soynlp = preprocessing_wordfreq_to_corr(wf_age_soynlp, df_news, colname_target=COLNAME_MINING, colname_category='Age', num_showkeyword=100, save_name='wordcorr_age_soynlp.csv')\n",
    "# waf_agecorr_soynlp = preprocessing_wordfreq_to_corr(waf_age_soynlp, df_news, colname_target=COLNAME_MINING, colname_category='Age', num_showkeyword=100, save_name='wordcorr_age_soynlpadj.csv')\n",
    "# wf_agecorr_tfidf = preprocessing_wordfreq_to_corr(wf_age_tfidf, df_news, colname_target=COLNAME_MINING, colname_category='Age', num_showkeyword=100, save_name='wordcorr_age_tfidf.csv')\n",
    "# waf_agecorr_tfidf = preprocessing_wordfreq_to_corr(waf_age_tfidf, df_news, colname_target=COLNAME_MINING, colname_category='Age', num_showkeyword=100, save_name='wordcorr_age_tfidfadj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f98c8d29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T14:55:52.883176Z",
     "start_time": "2023-11-05T14:55:52.095040Z"
    }
   },
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "save_name_list=['wordcorr_year_soynlp.csv', 'wordcorr_year_soynlpadj.csv', \n",
    "                'wordcorr_year_tfidf.csv', 'wordcorr_year_tfidfadj.csv',\n",
    "                'wordcorr_year_keybert.csv', 'wordcorr_year_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[0])\n",
    "wf_yearcorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[1])\n",
    "waf_yearcorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[2])\n",
    "wf_yearcorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[3])\n",
    "waf_yearcorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[4])\n",
    "wf_yearcorr_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[5])\n",
    "waf_yearcorr_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordcorr_era_soynlp.csv', 'wordcorr_era_soynlpadj.csv', \n",
    "                'wordcorr_era_tfidf.csv', 'wordcorr_era_tfidfadj.csv',\n",
    "                'wordcorr_era_keybert.csv', 'wordcorr_era_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[0])\n",
    "wf_eracorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[1])\n",
    "waf_eracorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[2])\n",
    "wf_eracorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[3])\n",
    "waf_eracorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[4])\n",
    "wf_eracorr_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[5])\n",
    "waf_eracorr_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordcorr_senti_soynlp.csv', 'wordcorr_senti_soynlpadj.csv', \n",
    "                'wordcorr_senti_tfidf.csv', 'wordcorr_senti_tfidfadj.csv',\n",
    "                'wordcorr_senti_keybert.csv', 'wordcorr_senti_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[0])\n",
    "wf_senticorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[1])\n",
    "waf_senticorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[2])\n",
    "wf_senticorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[3])\n",
    "waf_senticorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[4])\n",
    "wf_senticorr_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[5])\n",
    "waf_senticorr_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordcorr_age_soynlp.csv', 'wordcorr_age_soynlpadj.csv', \n",
    "                'wordcorr_age_tfidf.csv', 'wordcorr_age_tfidfadj.csv',\n",
    "                'wordcorr_age_keybert.csv', 'wordcorr_age_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[0])\n",
    "wf_agecorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[1])\n",
    "waf_agecorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[2])\n",
    "wf_agecorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[3])\n",
    "waf_agecorr_tfidf = pd.read_csv(save_name)\n",
    "# save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[4])\n",
    "# wf_agecorr_keybert = pd.read_csv(save_name)\n",
    "# save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[5])\n",
    "# waf_agecorr_keybert = pd.read_csv(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4addd0",
   "metadata": {},
   "source": [
    "# Naver Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1d525",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_CR = ['세계', '경제', '생활/문화', '오피니언', '사회', 'IT/과학']\n",
    "### github에 업로드 되지 않도록 다른 폴더를 지정\n",
    "# 아래 예시는 내PC 바탕화면 Data 폴더를 지정\n",
    "# SAVE_LOCATION = r'C:\\Users\\user\\Desktop\\Data'    # inu\n",
    "SAVE_LOCATION = r'C:\\Users\\KK\\Desktop\\Data'    # home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968376e",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f975867d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T15:17:41.703077Z",
     "start_time": "2023-11-05T15:17:41.071896Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터로딩\n",
    "df_news = get_data_from_path(os.path.join(os.getcwd(), 'Data', 'NaverNews'), folder_name=True)\n",
    "\n",
    "# 필터링\n",
    "## 중복 처리\n",
    "df_news.drop_duplicates(subset=['Press', 'Title'], inplace=True, ignore_index=True)\n",
    "## 불필요 변수 삭제\n",
    "colname_delete = ['Content', 'URL_Origin']\n",
    "df_news = df_news[[col for col in df_news.columns if col not in colname_delete]]\n",
    "## 카테고리 필터\n",
    "category_filter = [each for each in df_news.Category.unique() if each in CATEGORY_CR]\n",
    "df_news = df_news[df_news.Category.apply(lambda x: x in category_filter)].reset_index().iloc[:,1:]\n",
    "## 언론사 중복 필터\n",
    "df_news.Press = df_news.Press.progress_apply(lambda x: str(x).split('언론사 선정')[0])\n",
    "\n",
    "# 날짜 변환\n",
    "df_news.Date = pd.to_datetime(df_news.Date)\n",
    "## 연도 반영\n",
    "df_news['Date_Year'] = pd.to_datetime(df_news.Date.astype(str)).dt.year\n",
    "## 연도+월 반영\n",
    "df_news['Date_YearMonth'] = pd.to_datetime(df_news.Date.astype(str)).dt.strftime('%Y-%m')\n",
    "## 연도그룹 반영\n",
    "df_news['Date_Period'] = df_news.Date_Year.apply(lambda x: '2013 ~ 2017' if str(x)[:4] in ['2013', '2014', '2015', '2016', '2017']\n",
    "                                                                          else '2018 ~ 2023')\n",
    "df_news = df_news[['Folder_Name', 'Date', 'Date_Year', 'Date_YearMonth', 'Date_Period', 'Press', 'Category', 'Title', 'Comment', 'URL_Naver']]\n",
    "\n",
    "# 전처리\n",
    "df_news['Title'] = df_news['Title'].progress_apply(lambda x: text_preprocessor(x, del_number=False, \n",
    "                                                                               del_bracket_content=False))\n",
    "df_news = df_news[~df_news['Title'].isnull()].reset_index().iloc[:,1:].copy()\n",
    "df_news = df_news[df_news['Title'].str.len() != 0].reset_index().iloc[:,1:]\n",
    "## 댓글 길이가 5이상 & 갯수가 5개 이상 필터\n",
    "df_news['Comment'] = df_news['Comment'].progress_apply(lambda x: [i for i in literal_eval(x) if len(i) >= 5])\n",
    "df_news = df_news[df_news['Comment'].progress_apply(lambda x: len(x) >= 5)]\n",
    "\n",
    "# 언론사 필터\n",
    "## 댓글 평균이 5이상 필터 & 발행기사수 Top100 필터\n",
    "df_news['Comment_Len'] = df_news.Comment.apply(lambda x: len(x))\n",
    "df_temp = df_news.groupby('Press')['Comment_Len'].mean()\n",
    "del_press = list(pd.DataFrame(df_temp[df_temp < 5]).index)\n",
    "del_press = del_press + list(dict(df_news.Press.value_counts()).keys())[100:]\n",
    "df_news = df_news[~df_news.Press.isin(del_press)].reset_index().iloc[:,1:]\n",
    "df_news = df_news.drop('Comment_Len', axis=1)\n",
    "\n",
    "# 댓글기준 explode\n",
    "df_news_explode = df_news.copy()\n",
    "df_news_explode = df_news_explode.explode('Comment')\n",
    "\n",
    "# 저장\n",
    "df_news.to_csv(os.path.join(SAVE_LOCATION, 'df_news_crawling.csv'), index=False, encoding='utf-8-sig')\n",
    "df_news_explode.to_csv(os.path.join(SAVE_LOCATION, 'df_news_explode_crawling.csv'), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "838b9d1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T15:18:03.695687Z",
     "start_time": "2023-11-05T15:17:48.810684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  Category\n",
      "사회       10472\n",
      "경제        5055\n",
      "생활/문화     3360\n",
      "세계        1837\n",
      "오피니언      1007\n",
      "IT/과학      659\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## 불러오기\n",
    "df_news_nv = pd.read_csv(os.path.join(SAVE_LOCATION, 'df_news_crawling.csv'))\n",
    "print('Category: ', df_news_nv.Category.value_counts())\n",
    "df_newse_nv = pd.read_csv(os.path.join(SAVE_LOCATION, 'df_news_explode_crawling.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab82a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.573px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
