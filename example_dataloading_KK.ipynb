{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f30a5c",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cfa0985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:02:10.531796Z",
     "start_time": "2023-11-04T14:02:10.526862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "# warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from preprocessing_text_KK import *\n",
    "\n",
    "def get_data_from_path(folder_location, folder_name=False, concat_axis='row'):\n",
    "    # path_folder 하위의 모든 폴더위치와 내부 file 출력\n",
    "    df = pd.DataFrame()\n",
    "    print('Getting data from', len(os.listdir(folder_location)), 'folders...')\n",
    "    for (path, dir, files) in os.walk(folder_location):\n",
    "#         print(path)\n",
    "        for file in tqdm(files):\n",
    "            path_file = os.path.join(path, file)\n",
    "\n",
    "            ## 데이터 로딩\n",
    "            if path_file[-4:] == 'xlsx':\n",
    "                df_sub = pd.read_excel(path_file)\n",
    "            elif path_file[-3:] == 'csv':\n",
    "                df_sub = pd.read_csv(path_file)\n",
    "\n",
    "            ## 키워드 태깅 여부\n",
    "            if folder_name:\n",
    "                df_sub['Folder_Name'] = os.path.basename(path)\n",
    "            \n",
    "            ## 정리\n",
    "            if concat_axis == 'col':\n",
    "                df = pd.concat([df, df_sub], axis=1)\n",
    "            elif concat_axis == 'row':\n",
    "                df = pd.concat([df, df_sub], axis=0)\n",
    "                \n",
    "    return df\n",
    "\n",
    "# 하이퍼파라미터\n",
    "DELETE_KEYWORD = ['100세', '거주환경']\n",
    "CATEGORY_BK = ['경제', '사회', '문화', '국제']\n",
    "CATEGORY_BK_Sub = ['경제>경제일반', '경제>국제경제', '경제>취업_창업',\n",
    "                   '사회>노동_복지', '사회>사건_사고', '사회>사회일반', '사회>여성', '사회>장애인', '사회>의료_건강',\n",
    "                   '문화>미술_건축', '문화>요리_여행', '문화>출판',\n",
    "                   '국제>중국', '국제>유럽_EU', '국제>일본', '국제>미국_북미', '국제>중동_아프리카',\n",
    "                   '국제>아시아', '국제>중남미', '국제>국제일반', '국제>러시아']\n",
    "CATEGORY_CR = ['세계', '경제', '생활/문화', '오피니언', '사회', 'IT/과학']\n",
    "COLNAME_CATEGORY = '일자'\n",
    "COLNAME_MINING = '제목'\n",
    "### github에 업로드 되지 않도록 다른 폴더를 지정\n",
    "# 아래 예시는 내PC 바탕화면 Data 폴더를 지정\n",
    "SAVE_LOCATION = r'C:\\Users\\user\\Desktop\\Data'    # home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86745f91",
   "metadata": {},
   "source": [
    "# BigKinds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333cf75",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3f0483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T16:23:38.004014Z",
     "start_time": "2023-11-03T16:23:38.000242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from 39 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  2.00s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:41<00:00,  3.78s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.02s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.96s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.86s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.96s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.01s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.57s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.89s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.00s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.64s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:51<00:00,  4.71s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:20<00:00,  6.76s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.59s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.54s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.57s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.52s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:24<00:00,  4.99s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 509413/509413 [04:20<00:00, 1955.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from 51 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 51/51 [00:00<00:00, 109.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터로딩\n",
    "df_news = get_data_from_path(os.path.join(os.getcwd(), 'Data', 'BigKinds'), folder_name=True)\n",
    "# 전처리\n",
    "## 중복 처리\n",
    "df_news.drop_duplicates(subset=['뉴스 식별자', '언론사', COLNAME_MINING], inplace=True, ignore_index=True)\n",
    "## 불필요 변수 삭제\n",
    "colname_delete = ['뉴스 식별자', '인물', '위치', '기관', '기고자', '통합 분류2', '통합 분류3', \n",
    "                  '사건/사고 분류1', '사건/사고 분류2', '사건/사고 분류3',\n",
    "                  '키워드', '특성추출(가중치순 상위 50개)', 'URL', '분석제외 여부']\n",
    "df_news = df_news[[col for col in df_news.columns if col not in colname_delete]]\n",
    "## 카테고리 필터\n",
    "category_filter = [each for each in df_news['통합 분류1'].unique() if each.split('>')[0] in CATEGORY_BK]\n",
    "df_news = df_news[df_news['통합 분류1'].apply(lambda x: x in category_filter)].reset_index().iloc[:,1:]\n",
    "df_news['Category'] = df_news['통합 분류1'].apply(lambda x: x.split('>')[0])\n",
    "## 전처리\n",
    "df_news[COLNAME_MINING] = df_news[COLNAME_MINING].progress_apply(lambda x: text_preprocessor(x, del_number=False, del_bracket_content=False))\n",
    "## 결측치 및 빈문자 제거\n",
    "df_news = df_news[~df_news[COLNAME_MINING].isnull()].reset_index().iloc[:,1:].copy()\n",
    "df_news = df_news[df_news[COLNAME_MINING].str.len() != 0].reset_index().iloc[:,1:]\n",
    "\n",
    "# 날짜 변환\n",
    "## 연도 반영\n",
    "df_news[COLNAME_CATEGORY+'_Year'] = pd.to_datetime(df_news[COLNAME_CATEGORY].astype(str)).dt.year\n",
    "## 연도+월 반영\n",
    "df_news[COLNAME_CATEGORY+'_YearMonth'] = pd.to_datetime(df_news[COLNAME_CATEGORY].astype(str)).dt.strftime('%Y-%m')\n",
    "## 연도그룹 반영\n",
    "df_news[COLNAME_CATEGORY+'_Era'] = df_news[COLNAME_CATEGORY].apply(lambda x: '2013 ~ 2017' if str(x)[:4] in ['2013', '2014', '2015', '2016', '2017']\n",
    "                                                                                            else '2018 ~ 2023')\n",
    "\n",
    "# 나이대 변수 추가\n",
    "df_news['Age'] = df_news['제목'].apply(lambda x: 20 if re.search(' 20대', x) != None else\n",
    "                                                 (30 if re.search(' 30대', x) != None else\n",
    "                                                 (40 if re.search(' 40대', x) != None else\n",
    "                                                 (50 if re.search(' 50대', x) != None else\n",
    "                                                 (60 if re.search(' 60대', x) != None else\n",
    "                                                 (70 if re.search(' 70대', x) != None else\n",
    "                                                 (80 if re.search(' 80대', x) != None else\n",
    "                                                 (90 if re.search(' 90대', x) != None else 0))))))))\n",
    "\n",
    "# 긍부정 라벨 추가\n",
    "df_news_sentiment = get_data_from_path(os.path.join(os.getcwd(), 'Data', 'Sentiment'), folder_name=False)\n",
    "df_news_sentiment = df_news_sentiment.sort_values(by='Unnamed: 0').reset_index().iloc[:,2:]\n",
    "df_news_sentiment.columns = ['Sentiment']\n",
    "df_news_sentiment['Sentiment'] = df_news_sentiment.Sentiment.apply(lambda x: 'Positive' if x==2 else 'Negative')\n",
    "df_news_sentiment['Positive'] = df_news_sentiment.Sentiment.apply(lambda x: 1 if x=='Positive' else 0)\n",
    "df_news_sentiment['Negative'] = df_news_sentiment.Sentiment.apply(lambda x: -1 if x=='Negative' else 0)\n",
    "df_news = pd.concat([df_news, df_news_sentiment], axis=1)\n",
    "## 최대 중복 처리\n",
    "df_news.drop_duplicates(subset=['언론사', COLNAME_MINING], inplace=True, ignore_index=True)\n",
    "df_news.drop_duplicates(subset=[COLNAME_MINING], inplace=True, ignore_index=True)\n",
    "\n",
    "# 저장\n",
    "df_news.to_csv(os.path.join(SAVE_LOCATION, 'df_news_bigkinds.csv'), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80eebe40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:02:17.997278Z",
     "start_time": "2023-11-04T14:02:14.421982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  경제    182058\n",
      "사회    175598\n",
      "문화     96737\n",
      "국제     37249\n",
      "Name: Category, dtype: int64\n",
      "Category:  사회    143555\n",
      "경제     46072\n",
      "국제     35535\n",
      "문화     21051\n",
      "Name: Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 필터링한 결과를 최종적으로 사용\n",
    "df_news = pd.read_csv(os.path.join(SAVE_LOCATION, 'df_news_bigkinds.csv'))\n",
    "print('Category: ', df_news.Category.value_counts())\n",
    "keyword_filter = [each for each in df_news['Folder_Name'].unique() if each not in DELETE_KEYWORD]\n",
    "df_news = df_news[df_news['Folder_Name'].apply(lambda x: x in keyword_filter)].reset_index().iloc[:,1:]\n",
    "category_filter = [each for each in df_news['통합 분류1'].unique() if each in CATEGORY_BK_Sub]\n",
    "df_news = df_news[df_news['통합 분류1'].apply(lambda x: x in category_filter)].reset_index().iloc[:,1:]\n",
    "print('Category: ', df_news.Category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a08332",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e25e982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>언론사</th>\n",
       "      <th>제목</th>\n",
       "      <th>통합 분류1</th>\n",
       "      <th>본문</th>\n",
       "      <th>Folder_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>일자_Year</th>\n",
       "      <th>일자_YearMonth</th>\n",
       "      <th>일자_Era</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230619</td>\n",
       "      <td>내일신문</td>\n",
       "      <td>10명중 6명이 일하는 60대  고령사회 현실로</td>\n",
       "      <td>경제&gt;취업_창업</td>\n",
       "      <td>60대 인구 10명 중 6명은 일을 하고 있는 것으로 나타났다. 60대 고령층 취업...</td>\n",
       "      <td>고령사회</td>\n",
       "      <td>경제</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06</td>\n",
       "      <td>2018 ~ 2023</td>\n",
       "      <td>60</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230614</td>\n",
       "      <td>매일경제</td>\n",
       "      <td>친구는 건물주 난 일용직 60대부터 소득차 확 커져</td>\n",
       "      <td>사회&gt;여성</td>\n",
       "      <td>올해로 12년째 서울에서 택시를 몰고 있는 장 모씨(66)는 몇 년째 친구들과 연락...</td>\n",
       "      <td>고령사회</td>\n",
       "      <td>사회</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06</td>\n",
       "      <td>2018 ~ 2023</td>\n",
       "      <td>60</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20220910</td>\n",
       "      <td>KBS</td>\n",
       "      <td>고독사한 60대 방에 가봤더니</td>\n",
       "      <td>국제&gt;아시아</td>\n",
       "      <td>[앵커]\\n\\n 모처럼 가족들이 한자리에 모인 추석인데, 삶의 마지막 순간에도 홀로...</td>\n",
       "      <td>고령사회</td>\n",
       "      <td>국제</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-09</td>\n",
       "      <td>2018 ~ 2023</td>\n",
       "      <td>60</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20220623</td>\n",
       "      <td>서울신문</td>\n",
       "      <td>산책 나온 90대 할머니 치매 알고 성폭행 사건파일</td>\n",
       "      <td>사회&gt;여성</td>\n",
       "      <td>치매를 앓던 90대 할머니가 지난 18일 산책을 위해 집 밖을 나섰다가 성폭행 피해...</td>\n",
       "      <td>고령사회</td>\n",
       "      <td>사회</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-06</td>\n",
       "      <td>2018 ~ 2023</td>\n",
       "      <td>90</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20210916</td>\n",
       "      <td>세계일보</td>\n",
       "      <td>치매 간병의 비극 80대 노인 아내 살해 후 극단 선택</td>\n",
       "      <td>사회&gt;여성</td>\n",
       "      <td>80대 노인이 치매를 앓던 70대 아내를 살해한 뒤 유서를 남기고 극단적인 선택을 ...</td>\n",
       "      <td>고령사회</td>\n",
       "      <td>사회</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021-09</td>\n",
       "      <td>2018 ~ 2023</td>\n",
       "      <td>80</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6491</th>\n",
       "      <td>20160328</td>\n",
       "      <td>국민일보</td>\n",
       "      <td>일본서 줄기세포 치료 70대 남성 귀국후 갑자기 숨져</td>\n",
       "      <td>국제&gt;일본</td>\n",
       "      <td>일본에서 줄기세포 치료제를 시술받은 우리나라 70대 남성이 귀국 후 사망한 것으로 ...</td>\n",
       "      <td>파킨슨</td>\n",
       "      <td>국제</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>2013 ~ 2017</td>\n",
       "      <td>70</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>20150309</td>\n",
       "      <td>YTN</td>\n",
       "      <td>장애인 형 돌보던 40대 형 살해 후 자살</td>\n",
       "      <td>사회&gt;장애인</td>\n",
       "      <td>정신지체 장애인인 형을 돌보던 동생이 형을 살해하고 스스로 목숨을 끊었습니다.\\n\\...</td>\n",
       "      <td>파킨슨</td>\n",
       "      <td>사회</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015-03</td>\n",
       "      <td>2013 ~ 2017</td>\n",
       "      <td>40</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>20130530</td>\n",
       "      <td>매일경제</td>\n",
       "      <td>고개 흔들지마 파킨슨병 아내 폭행치사 70대 집유</td>\n",
       "      <td>사회&gt;사건_사고</td>\n",
       "      <td>고개를 흔드는 버릇을 고치겠다며 파킨슨병을 앓는 아내를 때려 숨지게 한 70대에게 ...</td>\n",
       "      <td>파킨슨</td>\n",
       "      <td>사회</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013-05</td>\n",
       "      <td>2013 ~ 2017</td>\n",
       "      <td>70</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>20130530</td>\n",
       "      <td>경향신문</td>\n",
       "      <td>파킨슨병 앓는 아내 폭행치사 70대 집유 3년</td>\n",
       "      <td>사회&gt;사건_사고</td>\n",
       "      <td>청주지법 형사합의11부(이관용 부장판사)는 파킨슨병을 앓는 아내를 폭행해 숨지게 한...</td>\n",
       "      <td>파킨슨</td>\n",
       "      <td>사회</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013-05</td>\n",
       "      <td>2013 ~ 2017</td>\n",
       "      <td>70</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>20130411</td>\n",
       "      <td>세계일보</td>\n",
       "      <td>파킨슨병 증세인 줄 모르고 아내 살해한 70대</td>\n",
       "      <td>사회&gt;사건_사고</td>\n",
       "      <td>‘파킨슨병의 날’인 11일 이 병에 대한 이해가 부족했던 70대 남편이 부인을 폭행...</td>\n",
       "      <td>파킨슨</td>\n",
       "      <td>사회</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013-04</td>\n",
       "      <td>2013 ~ 2017</td>\n",
       "      <td>70</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6496 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            일자   언론사                              제목    통합 분류1  \\\n",
       "0     20230619  내일신문      10명중 6명이 일하는 60대  고령사회 현실로  경제>취업_창업   \n",
       "1     20230614  매일경제    친구는 건물주 난 일용직 60대부터 소득차 확 커져     사회>여성   \n",
       "2     20220910   KBS                고독사한 60대 방에 가봤더니    국제>아시아   \n",
       "3     20220623  서울신문    산책 나온 90대 할머니 치매 알고 성폭행 사건파일     사회>여성   \n",
       "4     20210916  세계일보  치매 간병의 비극 80대 노인 아내 살해 후 극단 선택     사회>여성   \n",
       "...        ...   ...                             ...       ...   \n",
       "6491  20160328  국민일보   일본서 줄기세포 치료 70대 남성 귀국후 갑자기 숨져     국제>일본   \n",
       "6492  20150309   YTN         장애인 형 돌보던 40대 형 살해 후 자살    사회>장애인   \n",
       "6493  20130530  매일경제     고개 흔들지마 파킨슨병 아내 폭행치사 70대 집유  사회>사건_사고   \n",
       "6494  20130530  경향신문       파킨슨병 앓는 아내 폭행치사 70대 집유 3년  사회>사건_사고   \n",
       "6495  20130411  세계일보       파킨슨병 증세인 줄 모르고 아내 살해한 70대  사회>사건_사고   \n",
       "\n",
       "                                                     본문 Folder_Name Category  \\\n",
       "0     60대 인구 10명 중 6명은 일을 하고 있는 것으로 나타났다. 60대 고령층 취업...        고령사회       경제   \n",
       "1     올해로 12년째 서울에서 택시를 몰고 있는 장 모씨(66)는 몇 년째 친구들과 연락...        고령사회       사회   \n",
       "2     [앵커]\\n\\n 모처럼 가족들이 한자리에 모인 추석인데, 삶의 마지막 순간에도 홀로...        고령사회       국제   \n",
       "3     치매를 앓던 90대 할머니가 지난 18일 산책을 위해 집 밖을 나섰다가 성폭행 피해...        고령사회       사회   \n",
       "4     80대 노인이 치매를 앓던 70대 아내를 살해한 뒤 유서를 남기고 극단적인 선택을 ...        고령사회       사회   \n",
       "...                                                 ...         ...      ...   \n",
       "6491  일본에서 줄기세포 치료제를 시술받은 우리나라 70대 남성이 귀국 후 사망한 것으로 ...         파킨슨       국제   \n",
       "6492  정신지체 장애인인 형을 돌보던 동생이 형을 살해하고 스스로 목숨을 끊었습니다.\\n\\...         파킨슨       사회   \n",
       "6493  고개를 흔드는 버릇을 고치겠다며 파킨슨병을 앓는 아내를 때려 숨지게 한 70대에게 ...         파킨슨       사회   \n",
       "6494  청주지법 형사합의11부(이관용 부장판사)는 파킨슨병을 앓는 아내를 폭행해 숨지게 한...         파킨슨       사회   \n",
       "6495  ‘파킨슨병의 날’인 11일 이 병에 대한 이해가 부족했던 70대 남편이 부인을 폭행...         파킨슨       사회   \n",
       "\n",
       "      일자_Year 일자_YearMonth       일자_Era  Age Sentiment  Positive  Negative  \n",
       "0        2023      2023-06  2018 ~ 2023   60  Positive         1         0  \n",
       "1        2023      2023-06  2018 ~ 2023   60  Negative         0        -1  \n",
       "2        2022      2022-09  2018 ~ 2023   60  Negative         0        -1  \n",
       "3        2022      2022-06  2018 ~ 2023   90  Negative         0        -1  \n",
       "4        2021      2021-09  2018 ~ 2023   80  Negative         0        -1  \n",
       "...       ...          ...          ...  ...       ...       ...       ...  \n",
       "6491     2016      2016-03  2013 ~ 2017   70  Negative         0        -1  \n",
       "6492     2015      2015-03  2013 ~ 2017   40  Negative         0        -1  \n",
       "6493     2013      2013-05  2013 ~ 2017   70  Negative         0        -1  \n",
       "6494     2013      2013-05  2013 ~ 2017   70  Negative         0        -1  \n",
       "6495     2013      2013-04  2013 ~ 2017   70  Negative         0        -1  \n",
       "\n",
       "[6496 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news[df_news.Age != 0].reset_index().iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7ea419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.366 Gbry 1.366 Gb\n",
      "all cohesion probabilities was computed. # words = 697\n",
      "all branching entropies was computed # words = 1898\n",
      "all accessor variety was computed # words = 1898\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 1030\n",
      "_noun_scores_ 224\n",
      "after postprocessing 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▌                                                                         | 1/8 [00:00<00:05,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training ... (0 in 623 sents) use memory 1.366 Gb\r",
      "training was done. used memory 1.366 Gb\n",
      "\r",
      " cohesion probabilities ... (1 in 669)\r",
      "all cohesion probabilities was computed. # words = 374\n",
      "\r",
      "all branching entropies was computed # words = 1221\n",
      "\r",
      "all accessor variety was computed # words = 1221\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 599\n",
      "_noun_scores_ 120\n",
      "after postprocessing 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████                                                               | 2/8 [00:01<00:02,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.365 Gby 1.365 Gb\n",
      "all cohesion probabilities was computed. # words = 587\n",
      "all branching entropies was computed # words = 1652\n",
      "all accessor variety was computed # words = 1652\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 886\n",
      "_noun_scores_ 207\n",
      "after postprocessing 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▌                                                    | 3/8 [00:01<00:02,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training ... (0 in 1039 sents) use memory 1.366 Gb\r",
      "training was done. used memory 1.366 Gb\n",
      "\r",
      " cohesion probabilities ... (1 in 981)\r",
      "all cohesion probabilities was computed. # words = 639\n",
      "\r",
      "all branching entropies was computed # words = 1801\n",
      "\r",
      "all accessor variety was computed # words = 1801\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████                                          | 4/8 [00:02<00:02,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before postprocessing 926\n",
      "_noun_scores_ 204\n",
      "after postprocessing 101\n",
      "training was done. used memory 1.366 Gby 1.366 Gb\n",
      "all cohesion probabilities was computed. # words = 489\n",
      "all branching entropies was computed # words = 1489\n",
      "all accessor variety was computed # words = 1489\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 773\n",
      "_noun_scores_ 172\n",
      "after postprocessing 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████████▌                               | 5/8 [00:03<00:01,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training ... (0 in 966 sents) use memory 1.366 Gb\r",
      "training was done. used memory 1.366 Gb\n",
      "\r",
      " cohesion probabilities ... (1 in 871)\r",
      "all cohesion probabilities was computed. # words = 532\n",
      "\r",
      "all branching entropies was computed # words = 1679\n",
      "\r",
      "all accessor variety was computed # words = 1679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████                     | 6/8 [00:03<00:01,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 841\n",
      "_noun_scores_ 191\n",
      "after postprocessing 92\n",
      "training was done. used memory 1.366 Gby 1.366 Gb\n",
      "all cohesion probabilities was computed. # words = 478\n",
      "all branching entropies was computed # words = 1488\n",
      "all accessor variety was computed # words = 1488\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 710\n",
      "_noun_scores_ 156\n",
      "after postprocessing 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training ... (0 in 174 sents) use memory 1.366 Gb\r",
      "training was done. used memory 1.366 Gb\n",
      "\r",
      " cohesion probabilities ... (1 in 212)\r",
      "all cohesion probabilities was computed. # words = 101\n",
      "\r",
      "all branching entropies was computed # words = 373\n",
      "\r",
      "all accessor variety was computed # words = 373\n",
      "C:/Users/user/anaconda3/Lib/site-packages/soynlp\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "cannot access local variable 'f' where it is not associated with a value\n",
      "before postprocessing 138\n",
      "_noun_scores_ 23\n",
      "after postprocessing 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # 연도데이터 기준 전처리\n",
    "# wf_year_soy, waf_year_soy, \\\n",
    "# wf_year_tf, waf_year_tf, \\\n",
    "# wf_year_kb, waf_year_kb = preprocessing_wordfreq(df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year',\n",
    "#                                                  num_showkeyword=10, save_local=True, save_name='wordfreq_year')\n",
    "\n",
    "# # 연도그룹데이터 기준 전처리\n",
    "# wf_era_soy, waf_era_soy, \\\n",
    "# wf_era_tf, waf_era_tf, \\\n",
    "# wf_era_kb, waf_era_kb = preprocessing_wordfreq(df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era',\n",
    "#                                                num_showkeyword=10, save_local=True, save_name='wordfreq_era')\n",
    "# ## 안되면 아래줄 실행\n",
    "# if wf_era_tf.shape[0] == 0:\n",
    "#     wf_era_tf = wf_year_tf.copy()\n",
    "#     wf_era_tf.category.apply(lambda x: '2013 ~ 2017' if x in ['2013', '2014', '2015', '2016', '2017']\n",
    "#                                                       else '2018 ~ 2023')\n",
    "#     wf_era_tf = wf_era_tf.groupby(list(wf_era_tf.columns[:2])).mean().reset_index()\n",
    "#     waf_era_tf = pd.DataFrame()\n",
    "#     for category in tqdm(sorted(df_news[COLNAME_CATEGORY+'_Era'].unique())):\n",
    "#         df_sub = df_news[df_news[COLNAME_CATEGORY+'_Era'] == category]\n",
    "#         waf_era = preprocessing_adjwordcount(wf_era_tf[['word']], df_sub[COLNAME_MINING], num_showkeyword=5)\n",
    "#         waf_era['category'] = str(category)\n",
    "#         waf_era = waf_era[['category']+list(waf_era.columns[:-1])]\n",
    "#         waf_era_tf = pd.concat([waf_era_tf, waf_era], axis=0, ignore_index=True)\n",
    "#     save_name = os.path.join(os.getcwd(), 'Data', 'word_freq_tfidf_era.csv')\n",
    "#     wf_era_tf.to_csv(save_name, index=False, encoding='utf-8-sig')\n",
    "#     save_name = os.path.join(os.getcwd(), 'Data', 'wordadj_freq_tfidf_era.csv')\n",
    "#     waf_era_tf.to_csv(save_name, index=False, encoding='utf-8-sig')\n",
    "# ######################\n",
    "\n",
    "# # 연도감성데이터 기준 전처리\n",
    "# wf_senti_soy, waf_senti_soy, \\\n",
    "# wf_senti_tf, waf_senti_tf, \\\n",
    "# wf_senti_kb, waf_senti_kb = preprocessing_wordfreq(df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', \n",
    "#                                                    num_showkeyword=10, save_local=True, save_name='wordfreq_senti')\n",
    "\n",
    "# 나이데이터 기준 전처리\n",
    "wf_age_soy, waf_age_soy, \\\n",
    "wf_age_tf, waf_age_tf, \\\n",
    "wf_age_kb, waf_age_kb = preprocessing_wordfreq(df_news[df_news.Age != 0].reset_index().iloc[:,1:], \n",
    "                                               colname_target=COLNAME_MINING, colname_category='Age',\n",
    "                                               num_showkeyword=10, save_local=True, save_name='wordfreq_age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02ac71f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:02:18.138336Z",
     "start_time": "2023-11-04T14:02:17.998286Z"
    }
   },
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "save_name_list=['wordfreq_year_soynlp.csv', 'wordfreq_year_soynlpadj.csv', \n",
    "                'wordfreq_year_tfidf.csv', 'wordfreq_year_tfidfadj.csv',\n",
    "                'wordfreq_year_keybert.csv', 'wordfreq_year_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_year_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_year_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_year_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_year_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_year_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_year_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordfreq_era_soynlp.csv', 'wordfreq_era_soynlpadj.csv', \n",
    "                'wordfreq_era_tfidf.csv', 'wordfreq_era_tfidfadj.csv',\n",
    "                'wordfreq_era_keybert.csv', 'wordfreq_era_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_era_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_era_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_era_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_era_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_era_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordfreq_senti_soynlp.csv', 'wordfreq_senti_soynlpadj.csv', \n",
    "                'wordfreq_senti_tfidf.csv', 'wordfreq_senti_tfidfadj.csv',\n",
    "                'wordfreq_senti_keybert.csv', 'wordfreq_senti_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_senti_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_senti_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_senti_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_senti_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "wf_senti_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "waf_senti_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordfreq_age_soynlp.csv', 'wordfreq_age_soynlpadj.csv', \n",
    "                'wordfreq_age_tfidf.csv', 'wordfreq_age_tfidfadj.csv',\n",
    "                'wordfreq_age_keybert.csv', 'wordfreq_age_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[0])\n",
    "wf_age_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[1])\n",
    "waf_age_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[2])\n",
    "wf_age_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[3])\n",
    "waf_age_tfidf = pd.read_csv(save_name)\n",
    "# save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[4])\n",
    "# wf_age_keybert = pd.read_csv(save_name)\n",
    "# save_name = os.path.join(os.getcwd(), 'Data', 'WordFreq', save_name_list[5])\n",
    "# waf_age_keybert = pd.read_csv(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669b8ca",
   "metadata": {},
   "source": [
    "## Word Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2791622c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>20대</td>\n",
       "      <td>1809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>여성</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>60대</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>살해</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>냉장고</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>80</td>\n",
       "      <td>신고</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>80</td>\n",
       "      <td>버스</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>80</td>\n",
       "      <td>부검</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>80</td>\n",
       "      <td>낙상</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>90</td>\n",
       "      <td>치매</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     category word  score\n",
       "0          20  20대   1809\n",
       "1          20   여성    167\n",
       "2          20  60대    107\n",
       "3          20   살해     95\n",
       "4          20  냉장고     84\n",
       "..        ...  ...    ...\n",
       "116        80   신고     23\n",
       "117        80   버스     15\n",
       "118        80   부검     13\n",
       "119        80   낙상      9\n",
       "120        90   치매     80\n",
       "\n",
       "[121 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_age_soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c671f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████████████████████████▌          | 7/8 [00:00<00:00, 25.34it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wf_agecorr_soynlp \u001b[38;5;241m=\u001b[39m preprocessing_wordfreq_to_corr(wf_age_soynlp, df_news, colname_target\u001b[38;5;241m=\u001b[39mCOLNAME_MINING, colname_category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, num_showkeyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordcorr_age_soynlp.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m waf_agecorr_soynlp \u001b[38;5;241m=\u001b[39m preprocessing_wordfreq_to_corr(waf_age_soynlp, df_news, colname_target\u001b[38;5;241m=\u001b[39mCOLNAME_MINING, colname_category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, num_showkeyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordcorr_age_soynlpadj.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m wf_agecorr_tfidf \u001b[38;5;241m=\u001b[39m preprocessing_wordfreq_to_corr(wf_age_tfidf, df_news, colname_target\u001b[38;5;241m=\u001b[39mCOLNAME_MINING, colname_category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, num_showkeyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordcorr_age_tfidf.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\DataScience\\Lecture\\DEBA-Study\\preprocessing_text_KK.py:473\u001b[0m, in \u001b[0;36mpreprocessing_wordfreq_to_corr\u001b[1;34m(df_wordfreq, df, colname_target, colname_category, num_showkeyword, save_local, save_name)\u001b[0m\n\u001b[0;32m    470\u001b[0m df_sub \u001b[38;5;241m=\u001b[39m df[df[colname_category] \u001b[38;5;241m==\u001b[39m category]\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# 단어 벡터화 및 상관관계\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m _, _, word_corrpair \u001b[38;5;241m=\u001b[39m freq2vectorcorr_preprocessor(wf_sub\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:], \n\u001b[0;32m    474\u001b[0m                                                    df_sub[colname_target], num_showkeyword\u001b[38;5;241m=\u001b[39mnum_showkeyword)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m## 카테고리 추가\u001b[39;00m\n\u001b[0;32m    477\u001b[0m word_corrpair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(category)\n",
      "File \u001b[1;32mC:\\DataScience\\Lecture\\DEBA-Study\\preprocessing_text_KK.py:448\u001b[0m, in \u001b[0;36mfreq2vectorcorr_preprocessor\u001b[1;34m(df_wordfreq, df_series, num_showkeyword)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;66;03m#     ## vector가 0인 word 제거\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;66;03m#     colnames = df_wordvec.columns[df_wordvec.sum(axis=0) != 0]\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;66;03m#     df_wordvec = df_wordvec[colnames].copy()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# word correlation\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     wordcorr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcorrcoef(df_wordvec\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m    447\u001b[0m     df_wordcorrpair \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([(colnames[i], colnames[j], wordcorr[i,j]) \n\u001b[1;32m--> 448\u001b[0m                                      \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(wordcorr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(wordcorr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j])\n\u001b[0;32m    449\u001b[0m     df_wordcorrpair\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrelation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    450\u001b[0m     df_wordcorr \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(wordcorr, index\u001b[38;5;241m=\u001b[39mcolnames, columns\u001b[38;5;241m=\u001b[39mcolnames)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "wf_agecorr_soynlp = preprocessing_wordfreq_to_corr(wf_age_soynlp, df_news, colname_target=COLNAME_MINING, colname_category='Age', num_showkeyword=100, save_name='wordcorr_age_soynlp.csv')\n",
    "waf_agecorr_soynlp = preprocessing_wordfreq_to_corr(waf_age_soynlp, df_news, colname_target=COLNAME_MINING, colname_category='Age', num_showkeyword=100, save_name='wordcorr_age_soynlpadj.csv')\n",
    "wf_agecorr_tfidf = preprocessing_wordfreq_to_corr(wf_age_tfidf, df_news, colname_target=COLNAME_MINING, colname_category='Age', num_showkeyword=100, save_name='wordcorr_age_tfidf.csv')\n",
    "waf_agecorr_tfidf = preprocessing_wordfreq_to_corr(waf_age_tfidf, df_news, colname_target=COLNAME_MINING, colname_category='Age', num_showkeyword=100, save_name='wordcorr_age_tfidfadj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0080934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:10:30.756437Z",
     "start_time": "2023-11-04T15:08:21.765552Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [02:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.43 GiB for an array with shape (109294, 4213) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # 관련성 전처리\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# wf_yearcorr_soynlp = preprocessing_wordfreq_to_corr(wf_year_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_soynlp.csv')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# waf_yearcorr_soynlp = preprocessing_wordfreq_to_corr(waf_year_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_soynlpadj.csv')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# wf_eracorr_tfidf = preprocessing_wordfreq_to_corr(wf_era_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_tfidf.csv')\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# waf_eracorr_tfidf = preprocessing_wordfreq_to_corr(waf_era_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_tfidf.csv')\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m wf_eracorr_keybert \u001b[38;5;241m=\u001b[39m preprocessing_wordfreq_to_corr(wf_era_keybert\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m10000\u001b[39m,:], \n\u001b[0;32m     14\u001b[0m                                                     df_news, colname_target\u001b[38;5;241m=\u001b[39mCOLNAME_MINING, colname_category\u001b[38;5;241m=\u001b[39mCOLNAME_CATEGORY\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Era\u001b[39m\u001b[38;5;124m'\u001b[39m, num_showkeyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordcorr_era_keybert.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m waf_eracorr_keybert \u001b[38;5;241m=\u001b[39m preprocessing_wordfreq_to_corr(waf_era_keybert\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m10000\u001b[39m,:], \n\u001b[0;32m     16\u001b[0m                                                      df_news, colname_target\u001b[38;5;241m=\u001b[39mCOLNAME_MINING, colname_category\u001b[38;5;241m=\u001b[39mCOLNAME_CATEGORY\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Era\u001b[39m\u001b[38;5;124m'\u001b[39m, num_showkeyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordcorr_era_keybert.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m wf_senticorr_soynlp \u001b[38;5;241m=\u001b[39m preprocessing_wordfreq_to_corr(wf_senti_soynlp, df_news, colname_target\u001b[38;5;241m=\u001b[39mCOLNAME_MINING, colname_category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, num_showkeyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordcorr_senti_soynlp.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\DataScience\\Lecture\\DEBA-Study\\preprocessing_text_KK.py:473\u001b[0m, in \u001b[0;36mpreprocessing_wordfreq_to_corr\u001b[1;34m(df_wordfreq, df, colname_target, colname_category, num_showkeyword, save_local, save_name)\u001b[0m\n\u001b[0;32m    470\u001b[0m df_sub \u001b[38;5;241m=\u001b[39m df[df[colname_category] \u001b[38;5;241m==\u001b[39m category]\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# 단어 벡터화 및 상관관계\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m _, _, word_corrpair \u001b[38;5;241m=\u001b[39m freq2vectorcorr_preprocessor(wf_sub\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:], \n\u001b[0;32m    474\u001b[0m                                                    df_sub[colname_target], num_showkeyword\u001b[38;5;241m=\u001b[39mnum_showkeyword)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m## 카테고리 추가\u001b[39;00m\n\u001b[0;32m    477\u001b[0m word_corrpair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(category)\n",
      "File \u001b[1;32mC:\\DataScience\\Lecture\\DEBA-Study\\preprocessing_text_KK.py:433\u001b[0m, in \u001b[0;36mfreq2vectorcorr_preprocessor\u001b[1;34m(df_wordfreq, df_series, num_showkeyword)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# series to dataframe vector\u001b[39;00m\n\u001b[0;32m    432\u001b[0m df_wordvec \u001b[38;5;241m=\u001b[39m df_series\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: word2vec_preprocessor(dict_wordfreq, x))\n\u001b[1;32m--> 433\u001b[0m df_wordvec \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df_wordvec\u001b[38;5;241m.\u001b[39mvalues], columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(dict_wordfreq\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m## vector가 0의 비율이 많은 word 제거\u001b[39;00m\n\u001b[0;32m    435\u001b[0m colnames_topweight \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries((df_wordvec \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m df_wordvec\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:806\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    805\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 806\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m    809\u001b[0m         data,\n\u001b[0;32m    810\u001b[0m         columns,\n\u001b[0;32m    811\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    812\u001b[0m         dtype,\n\u001b[0;32m    813\u001b[0m     )\n\u001b[0;32m    814\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    815\u001b[0m         arrays,\n\u001b[0;32m    816\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    820\u001b[0m     )\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:835\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays, columns\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 835\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[0;32m    837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_dict_to_arrays(data, columns)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:856\u001b[0m, in \u001b[0;36m_list_to_arrays\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    853\u001b[0m     content \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mto_object_array_tuples(data)\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# list of lists\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     content \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mto_object_array(data)\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "File \u001b[1;32mlib.pyx:2966\u001b[0m, in \u001b[0;36mpandas._libs.lib.to_object_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.43 GiB for an array with shape (109294, 4213) and data type object"
     ]
    }
   ],
   "source": [
    "# # 관련성 전처리\n",
    "# wf_yearcorr_soynlp = preprocessing_wordfreq_to_corr(wf_year_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_soynlp.csv')\n",
    "# waf_yearcorr_soynlp = preprocessing_wordfreq_to_corr(waf_year_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_soynlpadj.csv')\n",
    "# wf_yearcorr_tfidf = preprocessing_wordfreq_to_corr(wf_year_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_tfidf.csv')\n",
    "# waf_yearcorr_tfidf = preprocessing_wordfreq_to_corr(waf_year_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_tfidfadj.csv')\n",
    "# wf_yearcorr_keybert = preprocessing_wordfreq_to_corr(wf_year_keybert, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_keybert.csv')\n",
    "# waf_yearcorr_keybert = preprocessing_wordfreq_to_corr(waf_year_keybert, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Year', num_showkeyword=100, save_name='wordcorr_year_keybertadj.csv')\n",
    "\n",
    "# wf_eracorr_soynlp = preprocessing_wordfreq_to_corr(wf_era_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_soynlp.csv')\n",
    "# waf_eracorr_soynlp = preprocessing_wordfreq_to_corr(waf_era_soynlp, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_soynlp.csv')\n",
    "# wf_eracorr_tfidf = preprocessing_wordfreq_to_corr(wf_era_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_tfidf.csv')\n",
    "# waf_eracorr_tfidf = preprocessing_wordfreq_to_corr(waf_era_tfidf, df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_tfidf.csv')\n",
    "# wf_eracorr_keybert = preprocessing_wordfreq_to_corr(wf_era_keybert.sort_values(by='score', ascending=False).iloc[:10000,:], \n",
    "#                                                     df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_keybert.csv')\n",
    "# waf_eracorr_keybert = preprocessing_wordfreq_to_corr(waf_era_keybert.sort_values(by='score', ascending=False).iloc[:10000,:], \n",
    "#                                                      df_news, colname_target=COLNAME_MINING, colname_category=COLNAME_CATEGORY+'_Era', num_showkeyword=100, save_name='wordcorr_era_keybert.csv')\n",
    "\n",
    "# wf_senticorr_soynlp = preprocessing_wordfreq_to_corr(wf_senti_soynlp, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_soynlp.csv')\n",
    "# waf_senticorr_soynlp = preprocessing_wordfreq_to_corr(waf_senti_soynlp, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_soynlpadj.csv')\n",
    "# wf_senticorr_tfidf = preprocessing_wordfreq_to_corr(wf_senti_tfidf, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_tfidf.csv')\n",
    "# waf_senticorr_tfidf = preprocessing_wordfreq_to_corr(waf_senti_tfidf, df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_tfidfadj.csv')\n",
    "# wf_senticorr_keybert = preprocessing_wordfreq_to_corr(wf_senti_keybert.sort_values(by='score', ascending=False).iloc[:10000,:],\n",
    "#                                                       df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_keybert.csv')\n",
    "# waf_senticorr_keybert = preprocessing_wordfreq_to_corr(waf_senti_keybert.sort_values(by='score', ascending=False).iloc[:10000,:], \n",
    "#                                                        df_news, colname_target=COLNAME_MINING, colname_category='Sentiment', num_showkeyword=100, save_name='wordcorr_senti_keybertadj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "save_name_list=['wordcorr_year_soynlp.csv', 'wordcorr_year_soynlpadj.csv', \n",
    "                'wordcorr_year_tfidf.csv', 'wordcorr_year_tfidfadj.csv',\n",
    "                'wordcorr_year_keybert.csv', 'wordcorr_year_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[0])\n",
    "wf_yearcorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[1])\n",
    "waf_yearcorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[2])\n",
    "wf_yearcorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[3])\n",
    "waf_yearcorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[4])\n",
    "wf_yearcorr_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[5])\n",
    "waf_yearcorr_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordcorr_era_soynlp.csv', 'wordcorr_era_soynlpadj.csv', \n",
    "                'wordcorr_era_tfidf.csv', 'wordcorr_era_tfidfadj.csv',\n",
    "                'wordcorr_era_keybert.csv', 'wordcorr_era_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[0])\n",
    "wf_eracorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[1])\n",
    "waf_eracorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[2])\n",
    "wf_eracorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[3])\n",
    "waf_eracorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[4])\n",
    "wf_eracorr_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[5])\n",
    "waf_eracorr_keybert = pd.read_csv(save_name)\n",
    "\n",
    "# 불러오기\n",
    "save_name_list=['wordcorr_senti_soynlp.csv', 'wordcorr_senti_soynlpadj.csv', \n",
    "                'wordcorr_senti_tfidf.csv', 'wordcorr_senti_tfidfadj.csv',\n",
    "                'wordcorr_senti_keybert.csv', 'wordcorr_senti_keybertadj.csv']\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[0])\n",
    "wf_senticorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[1])\n",
    "waf_senticorr_soynlp = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[2])\n",
    "wf_senticorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[3])\n",
    "waf_senticorr_tfidf = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[4])\n",
    "wf_senticorr_keybert = pd.read_csv(save_name)\n",
    "save_name = os.path.join(os.getcwd(), 'Data', 'WordCorr', save_name_list[5])\n",
    "waf_senticorr_keybert = pd.read_csv(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4addd0",
   "metadata": {},
   "source": [
    "# Naver Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968376e",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로딩\n",
    "df_news = get_data_from_path(os.path.join(os.getcwd(), 'Data', 'NaverNews'), folder_name=True)\n",
    "\n",
    "# 필터링\n",
    "## 중복 처리\n",
    "df_news.drop_duplicates(subset=['Press', 'Title'], inplace=True, ignore_index=True)\n",
    "## 불필요 변수 삭제\n",
    "colname_delete = ['Content', 'URL_Origin']\n",
    "df_news = df_news[[col for col in df_news.columns if col not in colname_delete]]\n",
    "## 카테고리 필터\n",
    "category_filter = [each for each in df_news.Category.unique() if each in CATEGORY_CR]\n",
    "df_news = df_news[df_news.Category.apply(lambda x: x in category_filter)].reset_index().iloc[:,1:]\n",
    "## 언론사 중복 필터\n",
    "df_news.Press = df_news.Press.progress_apply(lambda x: str(x).split('언론사 선정')[0])\n",
    "\n",
    "# 날짜 변환\n",
    "df_news.Date = pd.to_datetime(df_news.Date)\n",
    "## 연도 반영\n",
    "df_news['Date_Year'] = pd.to_datetime(df_news.Date.astype(str)).dt.year\n",
    "## 연도+월 반영\n",
    "df_news['Date_YearMonth'] = pd.to_datetime(df_news.Date.astype(str)).dt.strftime('%Y-%m')\n",
    "## 연도그룹 반영\n",
    "df_news['Date_Period'] = df_news.Date_Year.apply(lambda x: '2013 ~ 2017' if str(x)[:4] in ['2013', '2014', '2015', '2016', '2017']\n",
    "                                                                          else '2018 ~ 2023')\n",
    "df_news = df_news[['Folder_Name', 'Date', 'Date_Year', 'Date_YearMonth', 'Date_Period', 'Press', 'Category', 'Title', 'Comment', 'URL_Naver']]\n",
    "\n",
    "# 전처리\n",
    "df_news['Title'] = df_news['Title'].progress_apply(lambda x: text_preprocessor(x, del_number=False, \n",
    "                                                                               del_bracket_content=False))\n",
    "df_news = df_news[~df_news['Title'].isnull()].reset_index().iloc[:,1:].copy()\n",
    "df_news = df_news[df_news['Title'].str.len() != 0].reset_index().iloc[:,1:]\n",
    "## 댓글 길이가 5이상 & 갯수가 5개 이상 필터\n",
    "df_news['Comment'] = df_news['Comment'].progress_apply(lambda x: [i for i in literal_eval(x) if len(i) >= 5])\n",
    "df_news = df_news[df_news['Comment'].progress_apply(lambda x: len(x) >= 5)]\n",
    "\n",
    "# 언론사 필터\n",
    "## 댓글 평균이 5이상 필터 & 발행기사수 Top100 필터\n",
    "df_news['Comment_Len'] = df_news.Comment.apply(lambda x: len(x))\n",
    "df_temp = df_news.groupby('Press')['Comment_Len'].mean()\n",
    "del_press = list(pd.DataFrame(df_temp[df_temp < 5]).index)\n",
    "del_press = del_press + list(dict(df_news.Press.value_counts()).keys())[100:]\n",
    "df_news = df_news[~df_news.Press.isin(del_press)].reset_index().iloc[:,1:]\n",
    "df_news = df_news.drop('Comment_Len', axis=1)\n",
    "\n",
    "# 댓글기준 explode\n",
    "df_news_explode = df_news.copy()\n",
    "df_news_explode = df_news_explode.explode('Comment')\n",
    "\n",
    "# 저장\n",
    "df_news.to_csv(os.path.join(SAVE_LOCATION, 'df_news_crawling.csv'), index=False, encoding='utf-8-sig')\n",
    "df_news_explode.to_csv(os.path.join(SAVE_LOCATION, 'df_news_explode_crawling.csv'), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 불러오기\n",
    "df_news_nv = pd.read_csv(os.path.join(SAVE_LOCATION, 'df_news_crawling.csv'))\n",
    "print('Category: ', df_news_nv.Category.value_counts())\n",
    "df_newse_nv = pd.read_csv(os.path.join(SAVE_LOCATION, 'df_news_explode_crawling.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8f9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6f076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab82a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.591px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
