{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1EHHprypyYWS89UYB02V9zzc1JEk2kuSk","timestamp":1697442102306}],"gpuType":"T4","authorship_tag":"ABX9TyNUokDkiLj78r+11IEaaTVs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPAfVrsQkWZw","executionInfo":{"status":"ok","timestamp":1697442341827,"user_tz":-540,"elapsed":23918,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"9873ea89-84cd-469e-9268-5e5ca1694ff3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#구글드라이브 연동\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install mxnet\n","!pip install gluonnlp==0.8.0\n","!pip install tqdm pandas\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install torch>=1.8.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZSZuWILkgAQ","executionInfo":{"status":"ok","timestamp":1697442383997,"user_tz":-540,"elapsed":42174,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"897baaad-1d33-4931-d4f5-ae1eb58f89d7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.5)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n","Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2023.7.22)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.1\n","    Uninstalling graphviz-0.20.1:\n","      Successfully uninstalled graphviz-0.20.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Collecting gluonnlp==0.8.0\n","  Downloading gluonnlp-0.8.0.tar.gz (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0) (1.23.5)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.8.0-py3-none-any.whl size=292696 sha256=8cf9c1b59da9376b916b8deae32583d4136cb1431a857567be4b57ff9472ee97\n","  Stored in directory: /root/.cache/pip/wheels/2d/cc/dc/7ec84dced25f738b8be400101abb67e4b50c905090a51017e4\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.8.0\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Collecting transformers\n","  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"]}]},{"cell_type":"code","source":["!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ueepMm5kgFZ","executionInfo":{"status":"ok","timestamp":1697442391032,"user_tz":-540,"elapsed":7040,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"252ed0a1-6e78-4c9f-d44e-f1505563b538"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-ibvjws82/kobert-tokenizer_631bef7856f14cd2a2c0b62d3543b651\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-ibvjws82/kobert-tokenizer_631bef7856f14cd2a2c0b62d3543b651\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: kobert_tokenizer\n","  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4633 sha256=16a71a389ba03ddd6e8c1038367819e3bcd7e95a2022c559668b0bdcccba0fdf\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-6847o3g3/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n","Successfully built kobert_tokenizer\n","Installing collected packages: kobert_tokenizer\n","Successfully installed kobert_tokenizer-0.1\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Je-Hng6FkgIG","executionInfo":{"status":"ok","timestamp":1697442400466,"user_tz":-540,"elapsed":9438,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"8493e494-ec71-46ef-9f8d-ddc2fe884dac"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/mxnet/optimizer/optimizer.py:163: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n","  warnings.warn('WARNING: New optimizer %s.%s is overriding '\n"]}]},{"cell_type":"code","source":["from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"],"metadata":{"id":"_U-juEdZkgKs","executionInfo":{"status":"ok","timestamp":1697442402329,"user_tz":-540,"elapsed":1866,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["## GPU\n","device = torch.device(\"cuda:0\")"],"metadata":{"id":"ZSpDyuDVkgM8","executionInfo":{"status":"ok","timestamp":1697443500800,"user_tz":-540,"elapsed":553,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ul2DgaXmg_pF","executionInfo":{"status":"ok","timestamp":1697443551025,"user_tz":-540,"elapsed":1987,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"5b5670db-f401-4894-852e-caef5a2873c5"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n","The class this function is called from is 'KoBERTTokenizer'.\n"]}]},{"cell_type":"markdown","source":["# 데이터 셋팅"],"metadata":{"id":"5vpo9fwkkvHz"}},{"cell_type":"code","source":["!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n","!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TdSaYiBUkq05","executionInfo":{"status":"ok","timestamp":1697443506866,"user_tz":-540,"elapsed":4288,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"774e0fe5-fc3b-4511-b2d0-e36e8e184dcb"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-16 08:05:00--  https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/dl/374ftkec978br3d/ratings_train.txt [following]\n","--2023-10-16 08:05:00--  https://www.dropbox.com/s/dl/374ftkec978br3d/ratings_train.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc7c8bddb7c41519b80f4ad43dc2.dl.dropboxusercontent.com/cd/0/get/CFscq6-l4xtypAmDhv_12s4OMYIScRMUoqqnY1A3NguHhLPPF0LhpMqSB3vyglSgtEuVbzNe0OLNAw_4zx2RV0jly0p3kGUurqcrxFcoRhtfpQED-rPgNYcOhpJiytKyXgdTfbC-e9gENGkaBIBbypzf/file?dl=1# [following]\n","--2023-10-16 08:05:01--  https://uc7c8bddb7c41519b80f4ad43dc2.dl.dropboxusercontent.com/cd/0/get/CFscq6-l4xtypAmDhv_12s4OMYIScRMUoqqnY1A3NguHhLPPF0LhpMqSB3vyglSgtEuVbzNe0OLNAw_4zx2RV0jly0p3kGUurqcrxFcoRhtfpQED-rPgNYcOhpJiytKyXgdTfbC-e9gENGkaBIBbypzf/file?dl=1\n","Resolving uc7c8bddb7c41519b80f4ad43dc2.dl.dropboxusercontent.com (uc7c8bddb7c41519b80f4ad43dc2.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n","Connecting to uc7c8bddb7c41519b80f4ad43dc2.dl.dropboxusercontent.com (uc7c8bddb7c41519b80f4ad43dc2.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14628807 (14M) [application/binary]\n","Saving to: ‘ratings_train.txt?dl=1.2’\n","\n","ratings_train.txt?d 100%[===================>]  13.95M  26.2MB/s    in 0.5s    \n","\n","2023-10-16 08:05:02 (26.2 MB/s) - ‘ratings_train.txt?dl=1.2’ saved [14628807/14628807]\n","\n","--2023-10-16 08:05:02--  https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/dl/977gbwh542gdy94/ratings_test.txt [following]\n","--2023-10-16 08:05:03--  https://www.dropbox.com/s/dl/977gbwh542gdy94/ratings_test.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc8718c8aca0daf2cf69d0b432ef.dl.dropboxusercontent.com/cd/0/get/CFtG9x_-1qOWEX1OSKHKiY1BoibaKnewKfJ7OQAj2Kx6r0A4pC2K17uqNy7JsXcuslAAww0tFjw59BprvVLyY9RzDwJ3ClFlYGhtc5UBsC0Ra30MJSSU6ZlQNS2_7Ez8D1r3RaPYMXa8mn_Y88z8He5m/file?dl=1# [following]\n","--2023-10-16 08:05:03--  https://uc8718c8aca0daf2cf69d0b432ef.dl.dropboxusercontent.com/cd/0/get/CFtG9x_-1qOWEX1OSKHKiY1BoibaKnewKfJ7OQAj2Kx6r0A4pC2K17uqNy7JsXcuslAAww0tFjw59BprvVLyY9RzDwJ3ClFlYGhtc5UBsC0Ra30MJSSU6ZlQNS2_7Ez8D1r3RaPYMXa8mn_Y88z8He5m/file?dl=1\n","Resolving uc8718c8aca0daf2cf69d0b432ef.dl.dropboxusercontent.com (uc8718c8aca0daf2cf69d0b432ef.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6022:15::a27d:420f\n","Connecting to uc8718c8aca0daf2cf69d0b432ef.dl.dropboxusercontent.com (uc8718c8aca0daf2cf69d0b432ef.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4893335 (4.7M) [application/binary]\n","Saving to: ‘ratings_test.txt?dl=1.2’\n","\n","ratings_test.txt?dl 100%[===================>]   4.67M  18.5MB/s    in 0.3s    \n","\n","2023-10-16 08:05:04 (18.5 MB/s) - ‘ratings_test.txt?dl=1.2’ saved [4893335/4893335]\n","\n"]}]},{"cell_type":"code","source":["dataset_train = nlp.data.TSVDataset(\"ratings_train.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)\n","dataset_test = nlp.data.TSVDataset(\"ratings_test.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)"],"metadata":{"id":"9eap3be9kq3d","executionInfo":{"status":"ok","timestamp":1697443507989,"user_tz":-540,"elapsed":1137,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["dataset_train[0:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dHoAU2Lkq5x","executionInfo":{"status":"ok","timestamp":1697443507990,"user_tz":-540,"elapsed":8,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"9d737caa-9aad-401e-8196-712bb4d082c1"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['아 더빙.. 진짜 짜증나네요 목소리', '0'],\n"," ['흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '1'],\n"," ['너무재밓었다그래서보는것을추천한다', '0']]"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["# 75:25. 7:3~8:2 비율\n","print(len(dataset_train), len(dataset_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7iegddC0kq73","executionInfo":{"status":"ok","timestamp":1697443507990,"user_tz":-540,"elapsed":7,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"2c0eb767-65ec-4f11-d2bc-33ac1ba19fe6"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["150000 50000\n"]}]},{"cell_type":"code","source":["# 데이터 다운로드\n","!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n","!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\n","\n","# 데이터 읽어오기\n","dataset_train = nlp.data.TSVDataset(\"ratings_train.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)\n","dataset_test = nlp.data.TSVDataset(\"ratings_test.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)\n","\n","# 새로운 파일로 저장\n","with open(\"small_ratings_train.txt\", \"w\") as f:\n","    for sample in dataset_train[:10001]:  # 예시로 10000개의 샘플만 사용\n","        f.write(\"\\t\".join(sample) + \"\\n\")\n","\n","with open(\"small_ratings_test.txt\", \"w\") as f:\n","    for sample in dataset_test[:2001]:  # 예시로 2000개의 샘플만 사용\n","        f.write(\"\\t\".join(sample) + \"\\n\")\n","\n","# 저장된 파일을 다시 읽어오기\n","small_dataset_train = nlp.data.TSVDataset(\"small_ratings_train.txt\", field_indices=[0, 1], num_discard_samples=1)\n","small_dataset_test = nlp.data.TSVDataset(\"small_ratings_test.txt\", field_indices=[0, 1], num_discard_samples=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LKkVGYe3kq-p","executionInfo":{"status":"ok","timestamp":1697443511819,"user_tz":-540,"elapsed":3834,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"4a9fee0d-3f8c-4be7-e091-61184c41d165"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-16 08:05:05--  https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/dl/374ftkec978br3d/ratings_train.txt [following]\n","--2023-10-16 08:05:06--  https://www.dropbox.com/s/dl/374ftkec978br3d/ratings_train.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc64d3c5ceeccaf6c31b8e28ac34.dl.dropboxusercontent.com/cd/0/get/CFt84NAn1PmZPqQat0germ_fNfB3AB70h3Sr_1VMDXlR2xOaJa745TC7P1jJf4admlvRdiOFUGtJ6FSH9yqxChf_8h27v0T7cqpRekb46hvVt28S2n3QVQcjnFTA53_1pKiiW-_5S5xVZHuBTo6AizMl/file?dl=1# [following]\n","--2023-10-16 08:05:06--  https://uc64d3c5ceeccaf6c31b8e28ac34.dl.dropboxusercontent.com/cd/0/get/CFt84NAn1PmZPqQat0germ_fNfB3AB70h3Sr_1VMDXlR2xOaJa745TC7P1jJf4admlvRdiOFUGtJ6FSH9yqxChf_8h27v0T7cqpRekb46hvVt28S2n3QVQcjnFTA53_1pKiiW-_5S5xVZHuBTo6AizMl/file?dl=1\n","Resolving uc64d3c5ceeccaf6c31b8e28ac34.dl.dropboxusercontent.com (uc64d3c5ceeccaf6c31b8e28ac34.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n","Connecting to uc64d3c5ceeccaf6c31b8e28ac34.dl.dropboxusercontent.com (uc64d3c5ceeccaf6c31b8e28ac34.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14628807 (14M) [application/binary]\n","Saving to: ‘ratings_train.txt?dl=1.3’\n","\n","ratings_train.txt?d 100%[===================>]  13.95M  25.6MB/s    in 0.5s    \n","\n","2023-10-16 08:05:07 (25.6 MB/s) - ‘ratings_train.txt?dl=1.3’ saved [14628807/14628807]\n","\n","--2023-10-16 08:05:07--  https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/dl/977gbwh542gdy94/ratings_test.txt [following]\n","--2023-10-16 08:05:07--  https://www.dropbox.com/s/dl/977gbwh542gdy94/ratings_test.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc4e3ea7ee770e69bfd10652fd8d.dl.dropboxusercontent.com/cd/0/get/CFss7z1OjxaEyMf1JKK8MToHk4vfafiZOZ_7oEeYHC9pL4aeTQKXfysXG-Vmnv-ypg3YYIAtuXtr5hhKWht5m5RIWULTDeIUz7QMEqqezAmvaUBYDXpEJ_qUwxnQqPkgopGsrUR_Rz9Cs4uZ39R6gftg/file?dl=1# [following]\n","--2023-10-16 08:05:08--  https://uc4e3ea7ee770e69bfd10652fd8d.dl.dropboxusercontent.com/cd/0/get/CFss7z1OjxaEyMf1JKK8MToHk4vfafiZOZ_7oEeYHC9pL4aeTQKXfysXG-Vmnv-ypg3YYIAtuXtr5hhKWht5m5RIWULTDeIUz7QMEqqezAmvaUBYDXpEJ_qUwxnQqPkgopGsrUR_Rz9Cs4uZ39R6gftg/file?dl=1\n","Resolving uc4e3ea7ee770e69bfd10652fd8d.dl.dropboxusercontent.com (uc4e3ea7ee770e69bfd10652fd8d.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n","Connecting to uc4e3ea7ee770e69bfd10652fd8d.dl.dropboxusercontent.com (uc4e3ea7ee770e69bfd10652fd8d.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4893335 (4.7M) [application/binary]\n","Saving to: ‘ratings_test.txt?dl=1.3’\n","\n","ratings_test.txt?dl 100%[===================>]   4.67M  13.1MB/s    in 0.4s    \n","\n","2023-10-16 08:05:09 (13.1 MB/s) - ‘ratings_test.txt?dl=1.3’ saved [4893335/4893335]\n","\n"]}]},{"cell_type":"code","source":["print(len(small_dataset_train), len(small_dataset_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UmW4RuNkkrA-","executionInfo":{"status":"ok","timestamp":1697443511819,"user_tz":-540,"elapsed":6,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"e90de96e-f878-4fec-e7c6-098dc859b6e5"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["10000 2000\n"]}]},{"cell_type":"code","source":["class BERTSentenceTransform:\n","    r\"\"\"BERT style data transformation.\n","\n","    Parameters\n","    ----------\n","    tokenizer : BERTTokenizer.\n","        Tokenizer for the sentences.\n","    max_seq_length : int.\n","        Maximum sequence length of the sentences.\n","    pad : bool, default True\n","        Whether to pad the sentences to maximum length.\n","    pair : bool, default True\n","        Whether to transform sentences or sentence pairs.\n","    \"\"\"\n","\n","    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n","        self._tokenizer = tokenizer\n","        self._max_seq_length = max_seq_length\n","        self._pad = pad\n","        self._pair = pair\n","        self._vocab = vocab\n","\n","    def __call__(self, line):\n","        \"\"\"Perform transformation for sequence pairs or single sequences.\n","\n","        The transformation is processed in the following steps:\n","        - tokenize the input sequences\n","        - insert [CLS], [SEP] as necessary\n","        - generate type ids to indicate whether a token belongs to the first\n","        sequence or the second sequence.\n","        - generate valid length\n","\n","        For sequence pairs, the input is a tuple of 2 strings:\n","        text_a, text_b.\n","\n","        Inputs:\n","            text_a: 'is this jacksonville ?'\n","            text_b: 'no it is not'\n","        Tokenization:\n","            text_a: 'is this jack ##son ##ville ?'\n","            text_b: 'no it is not .'\n","        Processed:\n","            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n","            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n","            valid_length: 14\n","\n","        For single sequences, the input is a tuple of single string:\n","        text_a.\n","\n","        Inputs:\n","            text_a: 'the dog is hairy .'\n","        Tokenization:\n","            text_a: 'the dog is hairy .'\n","        Processed:\n","            text_a: '[CLS] the dog is hairy . [SEP]'\n","            type_ids: 0     0   0   0  0     0 0\n","            valid_length: 7\n","\n","        Parameters\n","        ----------\n","        line: tuple of str\n","            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n","            (text_a, text_b). For single sequences, the input is a tuple of single\n","            string: (text_a,).\n","\n","        Returns\n","        -------\n","        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n","        np.array: valid length in 'int32', shape (batch_size,)\n","        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n","\n","        \"\"\"\n","\n","        # convert to unicode\n","        text_a = line[0]\n","        if self._pair:\n","            assert len(line) == 2\n","            text_b = line[1]\n","\n","        tokens_a = self._tokenizer.tokenize(text_a)\n","        tokens_b = None\n","\n","        if self._pair:\n","            tokens_b = self._tokenizer(text_b)\n","\n","        if tokens_b:\n","            # Modifies `tokens_a` and `tokens_b` in place so that the total\n","            # length is less than the specified length.\n","            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","            self._truncate_seq_pair(tokens_a, tokens_b,\n","                                    self._max_seq_length - 3)\n","        else:\n","            # Account for [CLS] and [SEP] with \"- 2\"\n","            if len(tokens_a) > self._max_seq_length - 2:\n","                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n","\n","        # The embedding vectors for `type=0` and `type=1` were learned during\n","        # pre-training and are added to the wordpiece embedding vector\n","        # (and position vector). This is not *strictly* necessary since\n","        # the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        #vocab = self._tokenizer.vocab\n","        vocab = self._vocab\n","        tokens = []\n","        tokens.append(vocab.cls_token)\n","        tokens.extend(tokens_a)\n","        tokens.append(vocab.sep_token)\n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens.extend(tokens_b)\n","            tokens.append(vocab.sep_token)\n","            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n","\n","        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The valid length of sentences. Only real  tokens are attended to.\n","        valid_length = len(input_ids)\n","\n","        if self._pad:\n","            # Zero-pad up to the sequence length.\n","            padding_length = self._max_seq_length - valid_length\n","            # use padding tokens for the rest\n","            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n","            segment_ids.extend([0] * padding_length)\n","\n","        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n","            np.array(segment_ids, dtype='int32')\n","# [출처] No module named 'kobert' 에러 해결|작성자 yeon"],"metadata":{"id":"kdv9V6SE2k90","executionInfo":{"status":"ok","timestamp":1697443558941,"user_tz":-540,"elapsed":4,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n","                 pad, pair):\n","        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkeOUNtQgwbw","executionInfo":{"status":"ok","timestamp":1697443699696,"user_tz":-540,"elapsed":4544,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"b4713b60-7be6-4e51-be2b-70288a585484"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n","The class this function is called from is 'KoBERTTokenizer'.\n"]}]},{"cell_type":"code","source":["# # 토큰화, 정수 인코딩, 패딩\n","# # 위의 BERTSentenceTransform 모듈을 사용\n","# class BERTDataset(Dataset):\n","#     def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n","#                  pad, pair):\n","#         transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n","#         #transform = nlp.data.BERTSentenceTransform(\n","#         #    tokenizer, max_seq_length=max_len, pad=pad, pair=pair) #<-이 코드를 쓰면 vocab을 못찾는 에러 발생\n","#         self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","#         self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","#     def __getitem__(self, i):\n","#         return (self.sentences[i] + (self.labels[i], ))\n","\n","#     def __len__(self):\n","#         return (len(self.labels))\n","\n","# tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","# bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","# vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"],"metadata":{"id":"HtqufniRkrFq","executionInfo":{"status":"ok","timestamp":1697443512711,"user_tz":-540,"elapsed":895,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# 파라미터 설정\n","# 후에 좋은 성능을 내는 값 찾아야 함\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"],"metadata":{"id":"gJ9-vjlbmArt","executionInfo":{"status":"ok","timestamp":1697443703120,"user_tz":-540,"elapsed":372,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# tok = tokenizer.tokenize\n","data_train = BERTDataset(small_dataset_train, 0, 1, tokenizer, vocab, max_len, True, False)\n","data_test = BERTDataset(small_dataset_test, 0, 1, tokenizer, vocab, max_len, True, False)"],"metadata":{"id":"O92PIIkumAuD","executionInfo":{"status":"ok","timestamp":1697443794496,"user_tz":-540,"elapsed":1413,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# torch 형식의 dataset\n","train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"],"metadata":{"id":"NojEFiTwmAwP","executionInfo":{"status":"ok","timestamp":1697443743931,"user_tz":-540,"elapsed":397,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# KoBERT 학습모델\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=2, # 두가지로 분류하겠다는 의미->긍/부정(다중분류일 경우 이를 변경)\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","\n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","\n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"metadata":{"id":"hLkjWQ0wmAzN","executionInfo":{"status":"ok","timestamp":1697443746399,"user_tz":-540,"elapsed":4,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# print(torch.cuda.is_available())"],"metadata":{"id":"4fuOpe8hoy4Y","executionInfo":{"status":"aborted","timestamp":1697443512712,"user_tz":-540,"elapsed":9,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"],"metadata":{"id":"fjuvcisXmA1h","executionInfo":{"status":"ok","timestamp":1697443750961,"user_tz":-540,"elapsed":397,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["# Prepare optimizer and schedule (linear warmup and decay)\n","# 옵티마이저와 스케줄 설정\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"],"metadata":{"id":"dvhsKTO2mA4D","executionInfo":{"status":"aborted","timestamp":1697443512712,"user_tz":-540,"elapsed":9,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"],"metadata":{"id":"ZkOt_uUOmA6U","executionInfo":{"status":"aborted","timestamp":1697443512712,"user_tz":-540,"elapsed":9,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 정확도 측정 함수\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"metadata":{"id":"1FsCwPGNmA8i","executionInfo":{"status":"aborted","timestamp":1697443512712,"user_tz":-540,"elapsed":9,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델 학습"],"metadata":{"id":"3hy61WmAmSBl"}},{"cell_type":"code","source":["train_history = []\n","test_history = []\n","loss_history = []\n","\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","\n","        # print(label.shape, out.shape)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","            train_history.append(train_acc / (batch_id+1))\n","            loss_history.append(loss.data.cpu().numpy())\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    # train_history.append(train_acc / (batch_id+1))\n","    model.eval() # 모델을 평가 모드로 설정\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length = valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids) # 모델에 입력 데이터 전달하여 출력 얻기\n","        test_acc += calc_accuracy(out, label) # 정확도 계산하여 누적\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n","    # 테스트 정확도의 추이를 기록하고 후에 시각화하거나 분석하는 데 사용\n","    test_history.append(test_acc / (batch_id+1))\n","    # 위에서 파라미터 설정할 때 epoch의 수를 5개로 지정해둠(오버피팅이 발생하지 않는 적절한 갯수를 찾는 것? -> early stoping 생각해두기)"],"metadata":{"id":"wg8S89hAmBBG","executionInfo":{"status":"aborted","timestamp":1697443512712,"user_tz":-540,"elapsed":9,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# loss\n","plt.plot(loss_history, label='Training Loss')\n","plt.title('Training Loss Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# Training accuracy\n","plt.plot(train_history, label='Training Accuracy')\n","plt.title('Training Accuracy Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n","# Test accuracy\n","plt.plot(test_history, label='Test Accuracy')\n","plt.title('Test Accuracy Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"3B62fZaYMlxI","executionInfo":{"status":"aborted","timestamp":1697443512712,"user_tz":-540,"elapsed":8,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","epochs = range(1, num_epochs + 1)\n","\n","# training and test accuracy\n","plt.figure(figsize=(12, 6))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, train_history, 'bo-', label='Training Accuracy')\n","plt.plot(epochs, test_history, 'ro-', label='Test Accuracy')\n","plt.title('Training and Test Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","# training loss\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, loss_history, 'go-')\n","plt.title('Loss-Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","\n","plt.tight_layout()\n","plt.show()\n","# epoch 3에서 극소점이 나타나는 모습"],"metadata":{"id":"0ZpTIRm5Ml0H","executionInfo":{"status":"aborted","timestamp":1697443512713,"user_tz":-540,"elapsed":9,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델 저장"],"metadata":{"id":"i5nIahe_6fus"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","# 드라이브 마운트\n","drive.mount('/content/drive')\n","\n","# 모델을 드라이브 경로에 저장\n","PATH = '/content/drive/My Drive/'\n","model_name = 'small_model2.pt'  # 모델 파일 이름\n","torch.save(model.state_dict(), PATH + model_name)"],"metadata":{"id":"MOGV-J6fVGTs","executionInfo":{"status":"aborted","timestamp":1697443512713,"user_tz":-540,"elapsed":9,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference\n"],"metadata":{"id":"gRJnFqt132aQ"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","\n","\n","# parameter\n","max_len = 64\n","batch_size = 64\n","\n","# device\n","device = torch.device(\"cuda:0\")\n","\n","#BERT 모델, Vocabulary 불러오기\n","bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n","\n","\n","## 학습 모델 불러오기\n","PATH = './drive/My Drive/small_model2.pt'\n","model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n","model.load_state_dict(torch.load(PATH, map_location=device))\n","model.eval()\n","\n","# # 토큰화\n","# tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","# tok = tokenizer.tokenize #<-에러 발생 AttributeError: 'function' object has no attribute 'tokenize'\n","\n","def predict(predict_sentence):\n","\n","    data = [predict_sentence, 0]\n","    dataset_another = [data]\n","\n","    another_test = BERTDataset(dataset_another, 0, 1, tokenizer, vocab, max_len, True, False)\n","    test_loader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=0)\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_loader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","\n","        out = model(token_ids, valid_length, segment_ids)\n","\n","        test_eval=[]\n","        for i in out:\n","            logits=i\n","            logits = logits.detach().cpu().numpy()\n","\n","            if np.argmax(logits) == 0:\n","                test_eval.append(\"부정적\")\n","            else:\n","                test_eval.append(\"긍정적\")\n","\n","        print(\">> 해당 리뷰는 \" + test_eval[0] + \" 리뷰 입니다.\")\n","\n","print(\"\\n0을 입력하면 리뷰 감성분석 프로그램이 중단됩니다.\\n\")\n","\n","while True:\n","    sentence = input(\"긍부정을 판단할 리뷰를 입력해주세요 : \")\n","    if sentence == \"0\":\n","      print(\">> 긍부정 판단을 종료합니다!\\n\")\n","      break\n","    predict(sentence)\n","    print(\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"MD1B5c7xAQNn","executionInfo":{"status":"error","timestamp":1697444184171,"user_tz":-540,"elapsed":6171,"user":{"displayName":"임보민 (Bomin Im)","userId":"17927590123847360621"}},"outputId":"b103297e-e3d5-476d-c82b-904bbff72efe"},"execution_count":59,"outputs":[{"name":"stdout","output_type":"stream","text":["\n","0을 입력하면 리뷰 감성분석 프로그램이 중단됩니다.\n","\n","긍부정을 판단할 리뷰를 입력해주세요 : 별로다.\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-457804b249ab>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">> 긍부정 판단을 종료합니다!\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-59-457804b249ab>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(predict_sentence)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdataset_another\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0manother_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_another\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manother_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-51-2c43c026bbdd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len, pad, pair)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTSentenceTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-51-2c43c026bbdd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTSentenceTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-d15443c16da0>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtext_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mtokens_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mtokens_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'tokenize'"]}]}]}