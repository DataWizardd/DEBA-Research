{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d407af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤링시 필요한 라이브러리 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "900bd540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests, csv, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#정치, 경제, 사회, 생활/문화, 세계, IT/과학 뉴스 링크\n",
    "link_list = ['https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=100', 'https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101', 'https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=102', 'https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=103', 'https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=104', 'https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=105']\n",
    "\n",
    "naver_news = \"https://news.naver.com\"\n",
    "\n",
    "cnt = 1\n",
    "url1 = \"\"\n",
    "\n",
    "def requests_get(url):#해당 url의 웹소스를 추출하는 함수\n",
    "    global url1\n",
    "    url1 = url\n",
    "    html1 = requests.get(url, headers={'User-Agent':'Mozilla/5.0'})#봇 차단 회피\n",
    "    soup = BeautifulSoup(html1.content, \"html.parser\")\n",
    "    if url in link_list:\n",
    "        more_news(soup)#정치, 경제, 사회, 생활/문화, 세계, IT/과학 뉴스 링크라면 관련뉴스 링크 추출하는 함수로\n",
    "    else:\n",
    "        more_news1(soup)#관련뉴스에서 뉴스들의 링크를 수집하는 함수로\n",
    "\n",
    "def more_news(soup):\n",
    "\n",
    "    global naver_news, link_list, url1\n",
    "\n",
    "    if url1 == link_list[0]:#정치뉴스 사이트만 관련뉴스 링크를 수집할 수 있는 태그가 달라서 조건문 사용 \n",
    "        link = soup.find_all(\"div\", {\"class\":\"cluster_foot_inner\"})\n",
    "        print(link)\n",
    "        for i in link:\n",
    "            more_link = i.find(\"a\").attrs[\"href\"]\n",
    "            real_link = naver_news + more_link #\"https://news.naver.com\"를 + 해 줘야 완벽한 링크가 된다.\n",
    "            requests_get(real_link)#관련뉴스 사이트의 링크를 수집해 다시 웹소스를 추출하는 함수로 보낸다.\n",
    "    else:#정치뉴스 외의 뉴스들은 여기서 관련뉴스 링크 수집\n",
    "        link1 = soup.find_all(\"div\", {\"class\":\"cluster_head_topic_wrap\"})\n",
    "        for i1 in link1:\n",
    "            more_link1 = i1.find(\"a\").attrs[\"href\"]\n",
    "            real_link1 = naver_news + more_link1\n",
    "            requests_get(real_link1)#관련뉴스 사이트의 링크를 수집해 다시 웹소스를 추출하는 함수로 보낸다.\n",
    "\n",
    "def more_news1(soup):#관련뉴스의 링크를 받아 뉴스들의 링크를 수집한다.\n",
    "    link = soup.select(\"#main_content > div > ul > li > dl > dt > a\")#모든 뉴스 링크 수집\n",
    "    plink = soup.select(\"#main_content > div > ul > li > dl > dt.photo > a\")#뉴스 링크의 사진링크 수집\n",
    "    print(link, plink)\n",
    "    for pl in plink:\n",
    "        link.remove(pl)#뉴스 링크들 중에서 사진 링크 제거\n",
    "\n",
    "    for l in link:\n",
    "        more_link = l[\"href\"]#뉴스링크\n",
    "        title = l.get_text()#기사제목\n",
    "        con_craw(more_link, title)#댓글 수집하는 함수로 전송\n",
    "\n",
    "\n",
    "def con_craw(url, title):\n",
    "    global cnt\n",
    "    print(cnt,\" \"+url)#댓글 수집한 url숫자 출력\n",
    "    cnt += 1\n",
    "    \n",
    "    oid = url.split(\"/\")[5] #여기서부터는 네이버 댓글 api 사용\n",
    "    aid = url.split(\"/\")[6].split(\"?\")[0]\n",
    "\n",
    "    page = 1\n",
    "    header = {\"User-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\", \"referer\": url,}\n",
    "\n",
    "    while True:\n",
    "\n",
    "        c_url = \"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&templateId=default_society&pool=cbox5&_callback=jQuery1707138182064460843_1523512042464&lang=ko&country=&objectId=news\" + oid + \"%2C\" + aid + \"&categoryId=&pageSize=20&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=\" + str(page) + \"&refresh=false&sort=FAVORITE\"\n",
    "        r = requests.get(c_url, headers=header)\n",
    "\n",
    "        html = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "        total_comm = str(html).split('comment\":')[1].split(\",\")[0]\n",
    "\n",
    "        time = re.findall('\"modTime\":([^\\*]*),\"modTimeGmt\"', str(html))#댓글작성시간\n",
    "        name = re.findall('\"userName\":([^\\*]*)[*]{4}\"', str(html))#아이디(마스킹되어있음 asdf****)\n",
    "        cont = re.findall('\"contents\":([^\\*]*),\"userIdNo\"', str(html))#댓글내용\n",
    "        symp =re.findall('\"sympathyCount\":([^\\*]*),\"antipathyCount\"', str(html))#추천수\n",
    "        anti = re.findall('\"antipathyCount\":([^\\*]*),\"hideReplyButton\"', str(html))#비추천수\n",
    "\n",
    "        for a, b, c, d, e in zip(time, name, cont, symp, anti):#zip함수를 사용해 한꺼번에 csv에 쓰기\n",
    "            c = re.sub('[^\\w\\s]', '', c)#댓글에서 특수문자 제거\n",
    "            c = re.sub('([ㄱ-ㅎㅏ-ㅣ]+)', '', c)#댓글에서 한글 하나의 자음모음 제거\n",
    "            c = re.sub('ㆍ','',c)\n",
    "            c = re.sub('n', '',c)\n",
    "            c = re.sub('ᆢ', '',c)\n",
    "            with open(\"/home/song/Desktop/workplace/python/data/test1.csv\", \"a\", newline=\"\")as f:\n",
    "                wr = csv.writer(f)\n",
    "                wr.writerow([title.replace('\"',''),\n",
    "                    url,\n",
    "                    a.replace(\"+0900\", \"\").replace(\"T\",\" \").replace('\"',''),\n",
    "                    b.replace('\"', ''),\n",
    "                    c.replace('\"',''),\n",
    "                    d,\n",
    "                    e])\n",
    "\n",
    "        if int(total_comm) <= ((page) * 20):#댓글 페이지 수 끝까지\n",
    "            break\n",
    "        else:\n",
    "            page += 1\n",
    "\n",
    "\n",
    "def wcsv():\n",
    "    with open(\"C:\\\\DataScience\\\\Module\\\\test1.csv\",\"w\")as f:#댓글 넣을csv파일 생성\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Title\", \"URL\", \"Time\", \"ID\", \"Comments\", \"Sympathy\", \"Antipathy\"])\n",
    "\n",
    "    for i in link_list:#정치, 경제, 사회, 생활/문화, 세계, IT/과학 뉴스 링크를 웹 소스 추출하는 함수로 보내면서 시작\n",
    "        requests_get(i)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    wcsv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea84dc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ace82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3775a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2ea2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
